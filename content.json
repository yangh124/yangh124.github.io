{"meta":{"title":"yangh's Blog","subtitle":"","description":"你还有好多未完成的梦，你有什么道理停下脚步。","author":"yangh","url":"https://yangh124.github.io","root":"/"},"pages":[{"title":"404 Not Found：该页无法显示","date":"2022-11-05T10:19:09.883Z","updated":"2021-10-11T07:59:41.000Z","comments":false,"path":"/404.html","permalink":"https://yangh124.github.io/404.html","excerpt":"","text":""},{"title":"关于","date":"2022-11-05T10:19:09.998Z","updated":"2021-11-02T03:18:01.000Z","comments":false,"path":"about/index.html","permalink":"https://yangh124.github.io/about/index.html","excerpt":"","text":"这是我的个人博客，主要存放了自己写的一些学习笔记。"},{"title":"书单","date":"2022-11-05T10:19:09.999Z","updated":"2021-10-11T07:59:41.000Z","comments":false,"path":"books/index.html","permalink":"https://yangh124.github.io/books/index.html","excerpt":"","text":""},{"title":"分类","date":"2024-10-05T13:42:38.646Z","updated":"2024-10-05T13:42:38.646Z","comments":false,"path":"categories/index.html","permalink":"https://yangh124.github.io/categories/index.html","excerpt":"","text":""},{"title":"友情链接","date":"2022-11-05T10:19:10.002Z","updated":"2021-10-11T07:59:41.000Z","comments":true,"path":"links/index.html","permalink":"https://yangh124.github.io/links/index.html","excerpt":"","text":""},{"title":"Repositories","date":"2022-11-05T10:19:10.004Z","updated":"2021-10-11T07:59:41.000Z","comments":false,"path":"repository/index.html","permalink":"https://yangh124.github.io/repository/index.html","excerpt":"","text":""},{"title":"标签","date":"2024-10-05T13:42:08.831Z","updated":"2024-10-05T13:42:08.831Z","comments":false,"path":"tags/index.html","permalink":"https://yangh124.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"Docker安装ImmotalWrt教程","slug":"Dock安装ImmotalWrt","date":"2024-10-01T16:00:00.000Z","updated":"2024-10-05T15:47:33.611Z","comments":true,"path":"2024/10/02/Dock安装ImmotalWrt/","permalink":"https://yangh124.github.io/2024/10/02/Dock%E5%AE%89%E8%A3%85ImmotalWrt/","excerpt":"","text":"安装环境 服务器：Ubuntu 22.04.5 LTS (GNU&#x2F;Linux 6.8.0-45-generic x86_64) Docker版本：24.0.5（snap安装：sudo snap install docker） 下载ImmortalWrt下载地址：ImmtoalWrt Downloads 选择最新稳定版本。点击对应版本链接，依次进入目录x86 -&gt; 64（其他架构请自行选择），下载rootfs.tar.gz。我当前版本的地址是：rootfs.tar.gz。至此，ImmtoalWrt下载完成。 制作Docker镜像准备工作 创建工作目录在当前登录用户下创建，注意修改为你当前用户名1mkdir /home/yh/immotalWrt 将下载的ImmtoalWrt上传至该目录，并重命名为openwrt.tar.gz 创建Dockerfile12345678910111213FROM scratch# author info LABEL author=yhLABEL email=yh.124@qq.comADD openwrt.tar.gz /EXPOSE 80USER rootCMD [&quot;/sbin/init&quot;] 文件结构123456yh@yh-ubuntu:~/immotalWrt$ lltotal 11092drwxrwxr-x 2 yh yh 4096 10月 2 08:37 ./drwxr-x--- 17 yh yh 4096 10月 2 08:37 ../-rw-rw-r-- 1 yh yh 135 10月 2 08:36 Dockerfile-rw-r--r-- 1 yh yh 11344000 10月 2 08:23 openwrt.tar.gz 构建镜像1234567# sudo docker build $&#123;安装文件所在目录&#125; -t $&#123;镜像名称&#125;sudo docker build /home/yh/immotalWrt -t openwrt# 查看镜像yh@yh-ubuntu:~/immotalWrt$ sudo docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEopenwrt latest 7d75d3e47dc8 10 hours ago 31.1MB 至此，immotalwrt镜像构建完成 设置网络 打开网卡混杂模式12# 请自行替换自己的网卡名称（修改enp4s0）sudo ip link set enp4s0 promisc on 创建Docker network12# 我的路由器地址为192.168.3.1。请自行修改相关参数（子网地址、网关地址、网卡）sudo docker network create -d macvlan --subnet=192.168.3.0/24 --gateway=192.168.3.1 -o parent=enp4s0 openwrt 至此，网络配置完成 启动openwrt启动容器123456# 自行替换相应参数（容器名称、网络名称、镜像名称）sudo docker run --restart always --name openwrt -d --network openwrt --privileged openwrt# 查看容器yh@yh-ubuntu:~/immotalWrt$ sudo docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESb08428b5b80d openwrt &quot;/sbin/init&quot; 8 seconds ago Up 7 seconds openwrt 修改容器内网络 进入容器12# 进入容器（自行替换容器id）sudo docker exec -it b08428b5b80d /bin/sh 修改网络，执行vi /etc/config/network123456789# 修改lan配置（自行参照修改，注意ipaddr要使用未被占用的）,修改完成后保存config interface &#x27;lan&#x27; option device &#x27;br-lan&#x27; option proto &#x27;static&#x27; option ipaddr &#x27;192.168.3.109&#x27; option netmask &#x27;255.255.255.0&#x27; option ip6assign &#x27;60&#x27; option gateway &#x27;192.168.3.1&#x27; option dns &#x27;192.168.3.1&#x27; 重启容器内网络，/etc/init.d/network restart,或者重启容器sudo docker restart b08428b5b80d至此容器内网络配置完成 初始化设置访问192.168.3.109 第一次登录无需密码，直接点击登录，进入系统后自行初始化密码。 软件包初始化依次点击进入系统-&gt; 软件包更新软件包会报错，提示未安装wget-ssl，需要手动安装wget-ssl及其依赖。下载仓库地址：pkgs.org。 选择x86版本 找到下载地址，复制地址下载至本地 上传软件包并安装如安装报错请自行按照提示信息安装相关其他依赖wget-ssl 及其依赖 4. 点击更新列表没有报错，代表更新完成 至此，软件包初始化完成 安装theme-argon 软件包安装完成刷新浏览器页面至此，immotalwrt安装完成✌️","categories":[{"name":"其他","slug":"其他","permalink":"https://yangh124.github.io/categories/%E5%85%B6%E4%BB%96/"}],"tags":[]},{"title":"Camunda","slug":"Camunda","date":"2024-03-26T16:00:00.000Z","updated":"2024-10-05T15:47:33.609Z","comments":true,"path":"2024/03/27/Camunda/","permalink":"https://yangh124.github.io/2024/03/27/Camunda/","excerpt":"","text":"介绍Camunda是一个基于java的框架，支持用于工作流和过程自动化的BPMN、用于案例管理的CMMN和用于商业决策管理的DMN。 BPMN ： Business Process Management And Notation 业务流程管理和符号 CMMN ： Case Management Mode And Notation 案例管理模型和符号 DMN ： Decision Management And Notation 决策管理和符号 Camunda 组件 ) Process Engine &amp; Infrastructure 流程引擎 流程引擎是负责执行BPMN 2.0流程、CMMN 1.1案例和DMN 1.3决策的Java库。它有一个轻量级的POJO核心，并使用关系数据库来持久性。 orm映射由mybatis映射框架提供。 Spring 框架集成 CDI&#x2F;Java EE 集成 Runtime 容器集成 (与应用服务器基础架构集成。) Modeler Camunda Modeler: BPMN 2.0和CMMN 1.1图表以及DMN 1.3决策表的建模工具。 bpmn.io: 用于建模框架和工具包的开源项目 Web Applications REST API：REST API允许您从远程应用程序或JavaScript应用程序中使用流程引擎。 Camunda Tasklist：一个用于人工管理工作流和用户任务的web应用程序，它允许流程参与者检查他们的工作流任务并导航到任务表单，以便处理任务并提供数据输入。 Camunda Cockpit：是一个用于流程监视和操作的web应用程序，它允许您搜索流程实例、检查它们的状态并修复损坏的实例。 Camunda Admin：一个web应用程序，允许您管理用户、组和授权。 流程引擎集成通过提供的 Spring Boot starters，Camunda引擎可以在Spring Boot应用程序中使用。 文档地址：Spring Boot Integration | docs.camunda.org 流程引擎概念Process Definitions流程定义定义了流程的结构。你可以说流程定义就是流程。Camunda平台使用BPMN 2.0作为建模流程定义的主要建模语言。 Process Instances流程实例是流程定义的单个执行。流程实例与流程定义的关系与面向对象编程中对象与类的关系相同（在这个类比中，流程实例扮演对象的角色，流程定义扮演类的角色）。 Executions如果你的流程实例包含多个执行路径（例如并行网关），你必须能够区分流程实例内当前的活动路径。在下面的例子中，两个用户任务“receive payment”和“ship order”可以同时执行。 在内部实现中，流程引擎在流程实例遇到并行网关时会创建两个并发的执行，每个并发的执行都有一个执行路径。 或者“作用域”（scopes）也会创建新的执行，例如，如果流程引擎到达一个嵌入式子流程，就会创建执行。 或者多实例也会创建新的执行。 执行是分层次的，一个流程实例内的所有执行组成一棵树，流程实例是树上的根节点。注意：流程实例本身就是一个执行。执行属于变量范围, 意味着动态数据可以与之关联。 Activity Instances活动实例的概念与执行的概念相似，但采取了不同的视角。执行可以被想象成在流程中移动的 “token”，而活动实例则代表一个活动（任务、子流程…）的单个实例。因此，活动实例的概念更加倾向“面向状态（state-oriented）”。 活动实例也构成一棵树，遵循 BPMN 2.0 所提供的范围标准。处于 “同一层次的子流程”（即，同一范围的一部分，包含在同一子流程中）的活动，其活动实例会处于树的同一层次。 Jobs and Job DefinitionsCamunda流程引擎包括一个名为“Job执行器（Job Executor）”的组件。Job执行器是一个调度组件，负责执行异步的后台Job。 当一个流程被部署时，流程引擎为流程中的每个活动创建一个Job定义，该定义将在运行时创建Job。这允许你在流程中查询关于计时器和异步延续的信息。 Process Variables流程变量可用于向流程运行时状态或更具体地说，变量作用域（scopes）添加数据。一般来说，变量由名称和值组成。例如，如果一个活动设置了一个名为var的变量，后续活动可以使用这个名称来访问它。变量的值是Java对象。 流程引擎 API RepositoryService这个服务提供了管理和操纵部署和流程定义的操作，包括： 查询引擎已知的部署和流程定义。 暂停（Suspend）和激活（activate）流程定义。暂停意味着不能对它们做进一步的操作，而激活则是相反的操作。 检索各种资源，如部署中包含的文件或由引擎自动生成的流程图 123456789# 部署流程Deployment deployment = repositoryService.createDeployment() .addModelInstance(name + &quot;.bpmn&quot;, modelInstance) .name(name) .tenantId(tenantId) .deploy(); # 查询流程repositoryService.createProcessDefinitionQuery().tenantIdIn(tenantId).deploymentId(deploymentId).singleResult(); RuntimeService 通过流程定义创建的流程实例 用于检索和存储流程变量的服务 对流程实例和执行进行查询123456789# 创建流程实例（启动一个流程）runtimeService.startProcessInstanceById(processDefinitionId, businessKey, processVariables); # 设置变量runtimeService.setVariables(executionId, processVariables); # 删除流程实例runtimeService.deleteProcessInstance(processInstanceId, deleteReason); TaskService 查询分配给用户或组的任务。 创建新的独立任务。这些任务与流程实例无关。 操作任务分配给哪个用户，或者哪些用户以某种方式参与了任务。 声明（claim）并完成（complete）一个任务。声明意味着有人决定成为该任务的受让人（assignee），这意味着该用户将完成该任务。完成意味着“完成任务的工作”。通常这是填写某种表格。12345678# 设置变量taskService.setVariableLocal(taskId, variableName, value); # 查询任务taskService.createTaskQuery().processInstanceId(processInstanceId).list(); # 完成任务taskService.complete(taskId); IdentityService对组和用户进行管理（创建、更新、删除、查询…） HistoryService用于查询引擎收集的所有历史数据。当执行流程时，引擎可以保留很多数据（这是可配置的），如流程实例的开始时间、谁做了哪些任务、完成任务花了多长时间、每个流程实例遵循的路径等。该服务主要暴露了访问这些数据的查询功能。 1234567# 分页查询某人发起的流程historyService.createHistoricProcessInstanceQuery() .tenantIdIn(tenantId) .startedBy(startUserId) .orderByProcessInstanceStartTime() .desc() .listPage(firstResult, maxResults); ManagementService它允许检索关于数据库表和表元数据的信息。此外，它暴露了查询功能和Job的管理操作。Job在引擎中被用于各种事情，如定时器、异步延续、延迟暂停&#x2F;激活等。 OtherFormService、FilterService、ExternalTaskService、CaseService、DecisionService 流程变量流程变量可用于向流程运行时状态或更具体地说，变量作用域添加数据。一般来说，变量由名称和值组成。例如，如果一个活动设置了一个名为var的变量，后续活动可以使用这个名称来访问它。变量的值是Java对象。 变量作用域和变量可访问性所有可以拥有变量的实体称为变量作用域(variable scopes)。包括执行(executions（包括流程实例在内）)和任务（tasks）。参看下面的流程模型，其中红点标志着活动任务。 有一个流程实例有两个子执行，每个子执行都创建了一个任务。所有这五个实体都是变量作用域，箭头标志着父-子关系。在父作用域上定义的变量可以在每个子作用域中被访问，除非子作用域定义了同名的变量。反过来说，子变量不能从父作用域访问。直接附属于有关作用域的变量被称为 local 变量（Local Variable）。 设置和查询变量使用相关API查询和设置变量 支持的变量类型 委托代码委托代码（Delegation Code）允许你在流程执行期间发生某些事件时执行外部Java代码、脚本或表达式。 有四种不同类型委托代码： Java 代理类（JAVA Delegates）：可以附加到 BPMN 服务任务（Service Task）。 变量映射（Delegate Variable Mapping）：可以附加到发起活动。 执行监听器（Execution Listeners）：可以附加到令牌流动的任何事件，例如，启动一个流程实例或进入一个活动。 任务监听器（Task Listeners）：可以附加到用户任务生命周期内的任何事件，例如，用户任务的创建或完成。 Java Delegate实现可以在流程执行期间调用的类，这个类需要实现org.camunda.bpm.engine.delegate.JavaDelegate接口，并在execute方法中提供所需的逻辑。当流程执行到达这个特定的步骤时，它将执行该方法中定义的逻辑，并以默认的BPMN 2.0方式完成活动。 Delegate Variable Mapping略 Execution Listener执行监听器允许你在流程执行过程中发生某些事件时执行外部Java代码或计算一个表达式。可以捕获的事件有： 启动或结束一个流程实例。 进行一个过渡（Transition）。 启动或结束一个活动。 启动或结束一个网关。 启动或结束一个中间事件。 结束一个 start event 或启动一个 end event. Task Listener任务监听器被用来在某个任务相关事件发生时执行自定义的Java逻辑或表达式。它只能作为用户任务的一个子元素添加到流程定义中。请注意，这也必须作为BPMN 2.0 extensionElements的一个子元素，并在Camunda命名空间中定义，因为任务监听器是Camunda引擎专用的。 BPMN 2.0BPMN(Business Process Model and Notation)，即业务流程模型和符号，是业务流程模型的一种标准图形注解。 实现范围标注为 橙色 的部分被实现了 符号参与者 子流程 任务 网关 数据 注释 事件在BPMN中有开始事件、中间事件和结束事件。这三种事件类型都可以捕获事件或抛出事件。中间事件可以用作任务的边界事件，在这种情况下，它们可以是中断事件，也可以是非中断事件。这为您在流程中使用事件提供了很大的灵活性。 任务Service Task服务任务用于调用服务。在Camunda中，这是通过调用Java代码来完成的，或者为外部工作者提供的工作项来完成以异步或直接调用Web服务的形式实现逻辑。 Send Task发送任务用于发送消息。在Camunda，这是通过调用Java代码来完成的。 发送任务具有与服务任务相同的行为。 User Task用户任务用于对需要由人类参与者完成的工作进行建模。当流程执行到达这样的用户任务时，在分配给该任务的用户或组的任务列表中创建一个新任务。 Business Rule Task一个业务规则任务可以同步执行一个或多个规则，可以调用Java代码或外部工作者（external worker）来实现，或者以异步方式调用以Web服务形式实现的逻辑。 Script Task脚本任务是自动活动。当流程执行到达脚本任务时，执行相应的脚本。 Receive Task一个接收任务是一个简单的任务，它等待某个消息的到来。当流程的执行到达一个接收任务时，流程的状态被提交到持久性存储中。这意味着流程将一直处于这种等待状态，直到引擎收到一个特定的消息，从而触发流程继续执行。 Manual Task手动任务定义了一个对BPM引擎来说是外部的任务。它被用来模拟由引擎不需要知道的人所做的工作，而且没有已知的系统或UI接口。对于引擎来说，手工任务被当作一个传递节点来处理，在流程执行到达的那一刻将自动继续流程。 Task Markers除了各种类型的任务外，我们还可以将任务标记为循环（loops）、多实例（Multiple Instance）或补偿（compensations）。标记可以与任务类型相结合使用。 Multiple Instance多实例活动是为业务流程中的某个步骤定义重复的一种方式。在编程概念中，多实例与 for each 结构相似：它允许对给定集合中的每个项目按顺序或并行地执行某个步骤甚至一个完整的子流程。 多实例是一个有额外属性（所谓的 “多实例特性”）的常规活动，它将导致该活动在运行时被多次执行。以下活动可以成为多实例活动。 Service Task 服务任务 Send Task 发送任务 User Task 用户任务 Business Rule Task 业务规则任务 Script Task 脚本任务 Receive Task 接收任务 Manual Task 手动任务 (Embedded) Sub-Process （嵌入）子流程 Call Activity 发起活动 Transaction Subprocess 事务子流程 网关或事件不能成为多实例。 如果一个活动是多实例的，这将由活动底部的三条短线表示。三条垂直线表示实例将以并行方式执行，而三条水平线表示顺序执行。 根据规范的要求，每个创建的执行实例的父执行实例都将具有以下变量： nrOfInstances: 实例的总数量 nrOfActiveInstances: 当前活动的，即尚未完成的实例的数量。对于一个连续的多实例，这将永远是1。 nrOfCompletedInstances: 已经完成的实例的数量。 这些值可以通过调用 “execution.getVariable(x) “方法检索。 此外，每个创建的执行将有一个执行本地变量（即对其他执行不可见，也不存储在流程实例级别）。 loopCounter: 表示该特定实例的for each循环中的索引 compensations（补偿）如果一个活动被用来补偿另一个活动的影响，它可以被声明为补偿处理程序。补偿处理程序不包含在常规流程中，并且只在抛出补偿事件时被执行。 注意在 “cancel hotel reservation” 服务任务的底部中心区域的补偿处理程序图标。 补偿处理程序可能没有传入或传出的序列流。 补偿处理程序必须使用定向关联与补偿边界事件相关联。 为了声明一个活动是一个补偿处理程序，我们需要将属性isForCompensation设置为真。 网关Data-based Exclusive Gateway (XOR)基于数据的排他网关(XOR) Parallel Gateway并行网关 Inclusive Gateway包容网关 Event-based Gateway基于事件的网关 事件BPMN定义了不同的事件类型。Camunda支持以下内容。 Start Events：开始事件定义了流程或子流程的起始位置。 None Events：没有指定触发器和行为的事件。 Message Events：捕获&#x2F;抛出消息的事件。 Timer Events：等待计时器条件的事件。 Error Events：捕获&#x2F;抛出错误的事件。 Escalation Events：捕获&#x2F;抛出升级的事件。 Signal Events：捕获&#x2F;抛出信号的事件。 Cancel and Compensation Events：抛出&#x2F;捕获补偿和取消事务事件。 Conditional Events：捕获条件事件。 Link Events：连接非常长的序列流。 Terminate Events：结束一个作用域。 子流程在Camunda中，子流程允许基于可重用性和分组进行建模。以下是Camunda支持的不同类型的子流程： 嵌入式子流程（Embedded Subprocess）：将一组流程节点组合成一个范围。 调用活动（Call Activity）：从一个流程调用另一个流程。 事件子流程（Event Subprocess）：基于事件驱动的子流程。 事务子流程（Transaction Subprocess）：建模业务事务。 Camunda扩展Camunda延伸BPMN定义的扩展元素和属性。 Model API模型api是用于解析、创建、编辑和编写XML文件的简单轻量级库。模型API基于通用XML模型API，该API对于通用XML处理非常有用。 BPMN Model APIBPMN模型API可以轻松地从现有流程定义中提取信息，编辑现有流程定义或创建一个全新的流程定义，而无需手动进行XML解析。 注意:目前BPMN模型API不支持整个BPMN 2.0规范。已经支持的BPMN 2.0元素列表可以在源代码包org. camunda.bpmm.model .bpmn.instance中找到。 Read a Model123456789// read a model from a fileFile file = new File(&quot;PATH/TO/MODEL.bpmn&quot;);BpmnModelInstance modelInstance = Bpmn.readModelFromFile(file);// read a model from a streamInputStream stream = [...]BpmnModelInstance modelInstance = Bpmn.readModelFromStream(stream);Create a ModelBpmnModelInstance modelInstance = Bpmn.createEmptyModel(); Fluent Builder API为了构建简洁高效的BPMN流程模型，我们提供了一款流式调用的构建器API。使用此API，您可以在几行代码中轻松创建基本流程。在下面示例中，我们演示了如何在不到50行代码内创建具有5个任务和2个网关的相当复杂的流程。 123456789101112131415161718192021222324252627282930313233343536373839404142434445@Testpublic void testBpmn()&#123; BpmnModelInstance modelInstance = Bpmn.createExecutableProcess(&quot;invoice&quot;) .camundaHistoryTimeToLive(180) .name(&quot;BPMN API Invoice Process&quot;) .startEvent() .name(&quot;Invoice received&quot;) .userTask() .name(&quot;Assign Approver&quot;) .camundaAssignee(&quot;demo&quot;) .userTask() .id(&quot;approveInvoice&quot;) .name(&quot;Approve Invoice&quot;) .exclusiveGateway() .name(&quot;Invoice approved?&quot;) .gatewayDirection(GatewayDirection.Diverging) .condition(&quot;yes&quot;, &quot;$&#123;approved&#125;&quot;) .userTask() .name(&quot;Prepare Bank Transfer&quot;) .camundaFormKey(&quot;embedded:app:forms/prepare-bank-transfer.html&quot;) .camundaCandidateGroups(&quot;accounting&quot;) .serviceTask() .name(&quot;Archive Invoice&quot;) .camundaClass(&quot;org.camunda.bpm.example.invoice.service.ArchiveInvoiceService&quot;) .endEvent() .name(&quot;Invoice processed&quot;) .moveToLastGateway() .condition(&quot;no&quot;, &quot;$&#123;!approved&#125;&quot;) .userTask() .name(&quot;Review Invoice&quot;) .camundaAssignee(&quot;demo&quot;) .exclusiveGateway() .name(&quot;Review successful?&quot;) .gatewayDirection(GatewayDirection.Diverging) .condition(&quot;no&quot;, &quot;$&#123;!clarified&#125;&quot;) .endEvent() .name(&quot;Invoice not processed&quot;) .moveToLastGateway() .condition(&quot;yes&quot;, &quot;$&#123;clarified&#125;&quot;) .connectTo(&quot;approveInvoice&quot;) .done(); // show the BPMN 2.0 process model XML on the console log Bpmn.writeModelToStream(System.out, modelInstance);&#125; fluent builder API提供了以下基本元素: process start event exclusive gateway parallel gateway script task service task user task signal event definition end event subprocess History历史事件流提供关于已执行的流程实例的审批信息。 历史和审批事件流程引擎维护数据库内运行的流程实例的状态。这包括在流程实例达到等待状态时将其写(1)到数据库中，并在流程继续执行时读(2)该状态。我们称这个数据库为 运行时数据库（runtime database） 。除了维护运行时状态外，流程引擎还创建了一个审批日志，提供关于已执行流程实例的审批信息。我们称这个事件流为 历史事件流（history event stream） （3）。构成这个事件流的各个事件被称为 历史事件（History Events） ，包含关于已执行的流程实例、活动实例、改变的流程变量等的数据。 在默认配置中，流程引擎将简单地把这个事件流写入（4.）历史数据库。历史服务（HistoryService）的API允许查询（5）这个数据库。历史数据库和历史服务是可选的组件；如果历史事件流没有被记录到历史数据库中，或者如果用户选择将事件记录到不同的数据库，流程引擎仍然能工作，它仍然能够填充历史事件流。这是可能的，因为BPMN 2.0核心引擎组件不从历史数据库读取状态。也可以配置流程引擎的 “historyLevel” 配置项，来配置记录的数据量。 由于流程引擎不依赖历史数据库的存在来生成历史事件流，因此可以提供不同的后端实现来存储历史事件流。默认的后端实现是 “DbHistoryEventHandler” ，它将事件流记录到历史数据库。可以替换后端实现，为历史事件日志提供一个自定义的存储机制。 历史记录级别 NONE: 不会记录历史事件。 ACTIVITY: 以下事件将被记录： 流程实例 START, UPDATE, END, MIGRATE: 当流程实例被 started, updated, ended 和 migrated 时触发。 案例实例 CREATE, UPDATE, CLOSE: 当案例实例被 created, updated 和 closed 时触发。 活动实例 START, UPDATE, END, MIGRATE: 当活动实例被 started, updated, ended 和 migrated 时触发。 案例活动实例 CREATE, UPDATE, END: 当案例活动实例被 created, updated 和 ended 时触发。 任务实例 CREATE, UPDATE, COMPLETE, DELETE, MIGRATE: 当任务实例被 created, updated (i.e., re-assigned, delegated etc.), completed, deleted 和 migrated 时触发。 AUDIT: 除了ACTIVITY级别提供的事件，还记录了以下事件： 流程变量 CREATE, UPDATE, DELETE, MIGRATE: 当流程变量被 created, updated, deleted 和 migrated时触发，默认历史记录后端（DbHistoryEventHandler）将变量实例事件写入历史变量实例数据表中。因为在更新流程变量时，此表中的行会更新，所以只有流程变量的最后一个值将可用。 FULL: 除了AUDIT级别提供的事件，还记录了以下事件： 表单属性 UPDATE：当表单属性被 created 或 updated 时被触发。 默认的历史后端（DbHistoryEventHandler）将历史变量的更新情况写入数据库。这使得使用历史服务查看一个流程变量的中间值成为可能。 用户操作日志 UPDATE: 当用户执行操作时触发，例如承接（claim）用户任务、委派（delegating）用户任务等。 事件 CREATE, DELETE, RESOLVE, MIGRATE: 当事件被 created, deleted, resolved 和 migrated 时记录。 历史Job日志 CREATE, FAILED, SUCCESSFUL, DELETED: 当Job被 created, execution, successful 或 deleted 时记录。 决策实例评估：当DMN引擎进行决策评估时记录。 批处理 START, END: 当批处理被 started 或 ended 时触发。 身份 links ADD, DELETE: 当 身份link 被 added, deleted 以及设置或改变用户任务的受让人、拥有人时被记录。 历史外部任务日志 CREATED, DELETED, FAILED, SUCCESSFUL: 作为外部任务被 created, deleted 或外部任务执行者报告了成功或失败时会记录。 AUTO: 如果你打算在同一个数据库中运行多个引擎，那么 AUTO 级别是很有用的。在这种情况下，所有的引擎都必须使用相同的历史级别。与其手动保持配置同步，不如使用 auto 级别，引擎会自动确定数据库中已经配置的级别。如果没有找到，则使用默认值 audit 。请记住。如果你打算使用自定义历史级别，你必须为每个配置注册自定义级别，否则会出现异常。 如果你需要自定义历史事件的记录量，你可以提供一个自定义的 HistoryEventProducer 实现并将其应用到流程引擎配置中。 设置历史记录级别历史记录级别可以通过流程引擎配置中的一个属性设置。默认为FULL（当前版本下7.19） History levels and CockpitCamunda Platform Cockpit 应用程序在历史级别设置为 “FULL” 时工作得最好。较低的历史级别将禁用某些与历史有关的功能。 历史实体有以下历史实体，与运行时数据相反，它们在流程和案例实例完成后，将留在数据库中。 HistoricProcessInstances 包含有关当前和过去的流程实例的信息。 HistoricVariableInstances 包含有关在流程实例中保持的最新状态的信息。 HistoricCaseInstances 包含有关当前和过去案例实例的信息。 HistoricActivityInstances 包含有关一项执行活动的信息。 HistoricCaseActivityInstances 包含关于单个执行案例活动的信息。 HistoricTaskInstances 包含有关当前和过去（已完成和删除）任务实例的信息。 HistoricDetails 包含与历史流程实例，活动实例或任务实例相关的各种信息。 HistoricIncidents 包含有关当前和过去的信息（即，删除或解决的）事件。 UserOperationLogEntry 日志条目包含有关用户执行操作的信息。例如创建新任务，完成任务等操作。 HistoricJobLog 包含有关Job执行的信息。日志记录有关Job生命周期的详细信息。 HistoricDecisionInstance 包含关于决策的单个评估的信息，包括输入和输出值。 HistoricBatch 包含有关当前和过去批处理的信息。 HistoricIdentityLinkLog 包含有关当前和过去的identity links变更信息(added, deleted, 受让人、拥有人的设置或改变)。 HistoricExternalTaskLog 包含有关外部任务执行信息。提供了有关外部任务的生命周期的详细信息。 历史流程实例对于每个流程实例，流程引擎将在历史数据库中创建单个记录，并在流程执行期间继续更新此记录。每个历史流程实例记录都可以获得分配的以下状态： ACTIVE - 运行中的流程实例 SUSPENDED - 暂停的流程实例 COMPLETED - 通过正常结束事件完成的 EXTERNALLY_TERMINATED - 外部终止，例如通过REST API INTERNALLY_TERMINATED - 内部终止，例如通过终止边界事件 以下状态可以被例如 REST API 或 Cockpit 的外部行为触发：ACTIVE, SUSPENDED, EXTERNALLY_TERMINATED。 历史清理使用流程引擎进行密集操作时，可能会产生大量的历史数据。历史数据清理是一项功能，根据可配置的存活时间设置来删除这些数据。 它会删除： 历史流程实例以及所有相关的历史数据（例如历史变量实例、历史任务实例、历史实例权限、与它们相关的所有评论和附件等）。 历史决策实例以及所有相关的历史数据（即历史决策输入和输出实例）。 历史案例实例以及所有相关的历史数据（例如历史变量实例、历史任务实例等）。 历史批处理以及所有相关的历史数据（历史事务和作业日志）。 历史数据清理可以手动触发或定期安排。只有 Camunda 管理员有权限手动执行历史数据清理。 清理策略 Removal-Time-based Strategy End-Time-based Strategy 数据库数据库表结构流程引擎的数据库由多个表组成。 表的名称都以ACT开头的。后面的是说明表的用途的两个字符标识。这个用途标识也将与服务API大致匹配。 ACT_RE_*: RE代表资源库（Repository）。带有这个前缀的表包含“静态”信息，如流程定义和流程资源（图像、规则等）。 ACT_RU_*: RU代表运行时（Runtime）。包含流程实例、用户任务、变量、Job等的运行时数据。引擎只在流程实例执行期间存储运行时数据，并在流程实例结束时删除记录。这使运行时表保持小而快。 ACT_ID_*: ID代表身份信息（Identity）。这些表包含身份信息有用户、组等。 ACT_HI_*: HI代表历史（History）。这些是包含历史数据的表，如过去的流程实例、变量、任务等。 ACT_GE_*: GE代表一般数据（General），在各种使用情况下使用。 流程引擎的主要表有流程定义、执行、任务、变量和事件订阅。它们的关系显示在下面的UML模型中： 流程定义 (ACT_RE_PROCDEF)包含所有已部署的流程定义。它包括有版本详细信息，资源名称或暂停状态等信息。 执行 (ACT_RU_EXECUTION)包含所有当前运行中的执行。它包括有流程定义，父执行，业务主键，当前活动和关于执行状态的不同元数据的信息。 任务(ACT_RU_TASK)包含所有正在运行的流程实例的所有执行中的任务。它包括有相应的流程实例，执行以及元数据等信息，例如创建时间，受让人或截止日期。 变量 (ACT_RU_VARIABLE)包含所有当前设置的流程或任务变量。它包括变量的名称、类型和值以及相应流程实例或任务的信息。 事件订阅 (ACT_RU_EVENT_SUBSCR)包含所有当前现有的事件订阅。它包括预期事件的类型、名称和配置以及相应的流程实例和执行的信息。 表结构日志 (ACT_GE_SCHEMA_LOG)记录数据库表结构版本的历史记录。在进行表结构修改时都会写入记录。在数据库创建时会写入一条初始信息，以后每次修改都会记录id 、版本、数据库更新日期以及时间戳。 运行时指标日志 (ACT_RU_METER_LOG)表包含一组运行时指标，可以帮助得出有关Camunda平台使用情况、负载和性能的结论。 任务指标日志 (ACT_RU_TASK_METER_LOG)包含一组与任务相关的指标，可以帮助得出关于BPM平台的使用、负载和性能的结论。 实体关系图下面的实体关系图将数据库表和它们的显式外键约束可视化，按照BPMN引擎、DMN引擎、CMMN引擎、历史引擎和身份引擎进行分组。请注意，这些图并没有将表之间的隐性联系可视化。 BPMN引擎 DMN引擎 CMMN引擎 历史 身份 数据库配置为 date&#x2F;time 类型禁用发送毫秒精度引擎的MySQL数据库模式不支持列类型 TIMESTAMP 和 DATETIME 的毫秒精度。 也就是会四舍五入到下一秒或上一秒，例如，2021-01-01 15:00:46.731被四舍五入到2021-01-01 15:00:47 可以通过设置sendFractionalSeconds&#x3D;false来避免向MySQL服务器发送毫秒。 在你的JDBC连接URL中设置sendFractionalSeconds&#x3D;false。 隔离级别配置大多数数据库管理系统提供了四种不同的隔离级别可供设置。例如，由ANSI&#x2F;USO SQL定义的级别是（从低到高的隔离）有： 读未提交（READ UNCOMMITTED ） 不可重复读（READ COMMITTED） 可重复读（REPEATABLE READS） 串行化（SERIALIZABLE） 运行Camunda所需的隔离级别是 不可重复读（READ COMMITTED） ，根据你的数据库系统不同，它可能有不同的名称。将隔离级别设置为 REPEATABLE READS 会导致死锁，所以在改变隔离级别时需要谨慎。","categories":[{"name":"后端","slug":"后端","permalink":"https://yangh124.github.io/categories/%E5%90%8E%E7%AB%AF/"}],"tags":[{"name":"Camunda","slug":"Camunda","permalink":"https://yangh124.github.io/tags/Camunda/"}]},{"title":"k8s","slug":"k8s","date":"2024-03-08T16:00:00.000Z","updated":"2024-10-05T15:47:33.619Z","comments":true,"path":"2024/03/09/k8s/","permalink":"https://yangh124.github.io/2024/03/09/k8s/","excerpt":"","text":"概述Kubernetes 是一个可移植、可扩展的开源平台，用于管理容器化的工作负载和服务，可促进声明式配置和自动化。 Kubernetes 拥有一个庞大且快速增长的生态，其服务、支持和工具的使用范围相当广泛。 Kubernetes 这个名字源于希腊语，意为“舵手”或“飞行员”。k8s 这个缩写是因为 k 和 s 之间有八个字符的关系。 Google 在 2014 年开源了 Kubernetes 项目。 Kubernetes 建立在 Google 大规模运行生产工作负载十几年经验的基础上， 结合了社区中最优秀的想法和实践。 Kubernetes提供： 服务发现和负载均衡Kubernetes 可以使用 DNS 名称或自己的 IP 地址来暴露容器。 如果进入容器的流量很大，Kubernetes 可以负载均衡并分配网络流量，从而使部署稳定。 存储编排Kubernetes 允许你自动挂载你选择的存储系统，例如本地存储、公共云提供商等。 自动部署和回滚你可以使用 Kubernetes 描述已部署容器的所需状态， 它可以以受控的速率将实际状态更改为期望状态。 例如，你可以自动化 Kubernetes 来为你的部署创建新容器， 删除现有容器并将它们的所有资源用于新容器。 自动完成装箱计算你为 Kubernetes 提供许多节点组成的集群，在这个集群上运行容器化的任务。 你告诉 Kubernetes 每个容器需要多少 CPU 和内存 (RAM)。 Kubernetes 可以将这些容器按实际情况调度到你的节点上，以最佳方式利用你的资源。 自我修复Kubernetes 将重新启动失败的容器、替换容器、杀死不响应用户定义的运行状况检查的容器， 并且在准备好服务之前不将其通告给客户端。 密钥与配置管理Kubernetes 允许你存储和管理敏感信息，例如密码、OAuth 令牌和 SSH 密钥。 你可以在不重建容器镜像的情况下部署和更新密钥和应用程序配置，也无需在堆栈配置中暴露密钥。 批处理执行除了服务外，Kubernetes 还可以管理你的批处理和 CI（持续集成）工作负载，如有需要，可以替换失败的容器。 水平扩缩使用简单的命令、用户界面或根据 CPU 使用率自动对你的应用进行扩缩。 IPv4&#x2F;IPv6 双栈为 Pod（容器组）和 Service（服务）分配 IPv4 和 IPv6 地址。 为可扩展性设计在不改变上游源代码的情况下为你的 Kubernetes 集群添加功能。 Kubernetes 组件当你部署完 Kubernetes，便拥有了一个完整的集群。 一组工作机器，称为节点， 会运行容器化应用程序。每个集群至少有一个工作节点。 工作节点会托管 Pod，而 Pod 就是作为应用负载的组件。 控制平面管理集群中的工作节点和 Pod。 在生产环境中，控制平面通常跨多台计算机运行， 一个集群通常运行多个节点，提供容错性和高可用性。 控制平面组件（Control Plane Components）控制平面组件会为集群做出全局决策，比如资源的调度。 以及检测和响应集群事件，例如当不满足部署的 replicas 字段时，要启动新的 Pod）。 控制平面组件可以在集群中的任何节点上运行。 然而，为了简单起见，设置脚本通常会在同一个计算机上启动所有控制平面组件， 并且不会在此计算机上运行用户容器。 请参阅使用 kubeadm 构建高可用性集群 中关于跨多机器控制平面设置的示例。","categories":[{"name":"运维","slug":"运维","permalink":"https://yangh124.github.io/categories/%E8%BF%90%E7%BB%B4/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"https://yangh124.github.io/tags/k8s/"}]},{"title":"k3s实战","slug":"k3s实战","date":"2023-02-27T16:00:00.000Z","updated":"2024-10-05T15:47:33.618Z","comments":true,"path":"2023/02/28/k3s实战/","permalink":"https://yangh124.github.io/2023/02/28/k3s%E5%AE%9E%E6%88%98/","excerpt":"","text":"k3s简介K3s 是一个轻量级的 Kubernetes 发行版，它针对边缘计算、物联网等场景进行了高度优化。K3s 有以下增强功能： 打包为单个二进制文件。 使用基于 sqlite3 的轻量级存储后端作为默认存储机制。同时支持使用 etcd3、MySQL 和 PostgreSQL 作为存储机制。 封装在简单的启动程序中，通过该启动程序处理很多复杂的 TLS 和选项。 默认情况下是安全的，对轻量级环境有合理的默认值。 添加了简单但功能强大的batteries-included功能，例如：本地存储提供程序，服务负载均衡器，Helm controller 和 Traefik Ingress controller。 所有 Kubernetes control-plane 组件的操作都封装在单个二进制文件和进程中，使 K3s 具有自动化和管理包括证书分发在内的复杂集群操作的能力。 最大程度减轻了外部依赖性，K3s 仅需要 kernel 和 cgroup 挂载。 K3s 软件包需要的依赖项包括： containerd Flannel CoreDNS CNI 主机实用程序（iptables、socat 等） Ingress controller（Traefik） 嵌入式服务负载均衡器（service load balancer） 嵌入式网络策略控制器（network policy controller） 为什么叫k3s我们希望安装的 Kubernetes 在内存占用方面只是一半的大小。Kubernetes 是一个 10 个字母的单词，简写为 K8s。所以，有 Kubernetes 一半大的东西就是一个 5 个字母的单词，简写为 K3s。K3s 没有全称，也没有官方的发音。 安装k3s1curl -sfL https://rancher-mirror.rancher.cn/k3s/k3s-install.sh | INSTALL_K3S_MIRROR=cn sh - 运行此安装后： * K3s 服务将被配置为在节点重启后或进程崩溃或被杀死时自动重启。 * 将安装其他实用程序，包括kubectl、crictl、ctr、k3s-killall.sh 和 k3s-uninstall.sh。 * 将kubeconfig文件写入到&#x2F;etc&#x2F;rancher&#x2F;k3s&#x2F;k3s.yaml，由 K3s 安装的 kubectl 将自动使用该文件 如果要部署集群，使用以下脚本部署： 1234curl -sfL https://rancher-mirror.rancher.cn/k3s/k3s-install.sh | INSTALL_K3S_MIRROR=cn K3S_URL=https://myserver:6443 K3S_TOKEN=mynodetoken sh -myserver -&gt; master节点的ipmynodetoken -&gt; cat /var/lib/rancher/k3s/server/node-token 查看版本12345678910111213141516171819202122root@YH-UBUNTU:~# kubectl version --output=yamlclientVersion: buildDate: &quot;2023-01-26T00:47:47Z&quot; compiler: gc gitCommit: 9176e03c5788e467420376d10a1da2b6de6ff31f gitTreeState: clean gitVersion: v1.25.6+k3s1 goVersion: go1.19.5 major: &quot;1&quot; minor: &quot;25&quot; platform: linux/amd64kustomizeVersion: v4.5.7serverVersion: buildDate: &quot;2023-01-26T00:47:47Z&quot; compiler: gc gitCommit: 9176e03c5788e467420376d10a1da2b6de6ff31f gitTreeState: clean gitVersion: v1.25.6+k3s1 goVersion: go1.19.5 major: &quot;1&quot; minor: &quot;25&quot; platform: linux/amd64 禁用默认traefik删除traefik12kubectl -n kube-system delete helmcharts.helm.cattle.io traefikkubectl -n kube-system delete helmcharts.helm.cattle.io traefik-crd 关闭开机自动部署traefik 修改服务文件sudo vim /etc/systemd/system/k3s.service,将以下内容追加至ExecStart：1--disable traefik --disable traefik-crd 清除部署文件中的traefiksudo rm /var/lib/rancher/k3s/server/manifests/traefik.yaml 重启服务12sudo systemctl daemon-reloadsudo service k3s restart 配置私有镜像仓库可以配置 Containerd 连接到私有镜像仓库，并使用它们在节点上拉取私有镜像。 启动时，K3s 会检查&#x2F;etc&#x2F;rancher&#x2F;k3s&#x2F;中是否存在registries.yaml文件，并指示 containerd 使用文件中定义的镜像仓库。如果你想使用一个私有的镜像仓库，那么你需要在每个使用镜像仓库的节点上以 root 身份创建这个文件。 配置registries.yaml（这里使用的是阿里云的镜像服务） 1234567891011121314mirrors: docker.io: endpoint: - &quot;registry.cn-hangzhou.aliyuncs.com&quot;configs: &quot;registry.cn-hangzhou.aliyuncs.com&quot;: auth: username: # 这是私有镜像仓库的用户名 password: # 这是私有镜像仓库的密码 # 使用tls时需要配置 tls: cert_file: # 镜像仓库中使用的cert文件的路径。 key_file: # 镜像仓库中使用的key文件的路径。 ca_file: # 镜像仓库中使用的ca文件的路径。 客户端和服务器证书K3s 客户端和服务器证书自颁发日起 365 天内有效。每次启动 K3s 时，已过期或 90 天内过期的证书都会自动更新。要手动轮换客户端和服务器证书，请使用 k3s certificate rotate 子命令： 12345678# 停止 K3ssystemctl stop k3s# 轮换证书k3s certificate rotate# 启动 K3ssystemctl start k3s 你可以通过指定证书名称来轮换单个或多个证书： 1k3s certificate rotate --service &lt;SERVICE&gt;,&lt;SERVICE&gt; 使用 kubectl 从外部访问集群（云效）将 &#x2F;etc&#x2F;rancher&#x2F;k3s&#x2F;k3s.yaml 复制到位于集群外部的主机上的 ~&#x2F;.kube&#x2F;config。然后，将 server 字段的值替换为你 K3s Server 的 IP 或名称。现在，你可以使用 kubectl 来管理 K3s 集群。 HelmHelm安装 下载所需版本【当前3.11.1】，地址：Helm下载 安装123456# 解压安装包tar -zxf helm-v3.11.1-linux-amd64.tar.gz# 移至bin目录mv linux-amd64/helm /usr/local/bin/helm# 安装完成，查看版本helm version 配置仓库123# 用于安装mysql,redishelm repo add bitnami https://charts.bitnami.com/bitnamihelm repo update 搜索chart1helm search repo xxx Helm使用 生成chart部署 12345678910111213141516171819[root@VM-16-2-centos ~/mydata/cicd]# helm create test-chartCreating test-chart[root@VM-16-2-centos ~/mydata/cicd]# tree test-chart/test-chart/├── charts├── Chart.yaml├── templates│ ├── deployment.yaml│ ├── _helpers.tpl│ ├── hpa.yaml│ ├── ingress.yaml│ ├── NOTES.txt│ ├── serviceaccount.yaml│ ├── service.yaml│ └── tests│ └── test-connection.yaml└── values.yaml3 directories, 10 files 拉取仓库chart部署1helm pull bitnami/mysql 部署命令12# 升级或安装helm upgrade --install [helm-name] [chart-name] 项目部署本文介绍使用helm部署，所有部署文件已上传至GitHub，地址：k3s-cicd 安装Mysql创建StorageClass创建mysql-StorageClass.yaml 12345678kind: StorageClassapiVersion: storage.k8s.io/v1metadata: name: local-storage-mysql namespace: db-systemprovisioner: kubernetes.io/no-provisionervolumeBindingMode: WaitForFirstConsumerreclaimPolicy: Retain kubectl apply -f mysql-StorageClass.yaml 创建PV创建mysql-pv.yaml 123456789101112131415161718192021222324apiVersion: v1kind: PersistentVolumemetadata: name: mysql-pv namespace: db-system labels: type: localspec: capacity: storage: 2Gi accessModes: - ReadWriteOnce persistentVolumeReclaimPolicy: Retain storageClassName: local-storage-mysql local: path: &quot;/opt/mysql&quot; # 确保机器上有此目录 nodeAffinity: required: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/arch operator: In values: - amd64 kubectl apply -f mysql-pv.yaml 创建PVC创建mysql-pvc.yaml 1234567891011121314apiVersion: v1kind: PersistentVolumeClaimmetadata: name: mysql-pvc namespace: db-system labels: app: mysql-pvcspec: accessModes: - ReadWriteOnce #此处需要和pv对应才能匹配 resources: requests: storage: 2Gi storageClassName: local-storage-mysql #此处需要和StorageClass.yaml匹配 kubectl apply -f mysql-pvc.yaml 拉取mysql charthelp helm pull bitnami/mysql 修改配置12345678910111213141516171819# 设置root账户密码auth.rootPassword=&quot;root&quot; # 关闭默认创建databaseauth.createDatabase=false# 配置pvcprimary.persistence.existingClaim=&quot;mysql-pvc&quot;# 配置storageClassprimary.persistence.storageClass=&quot;local-storage-mysql&quot;# 修改服务类型为NodePortprimary.service.type=NodePort # 配置暴露端口primary.service.nodePorts.mysql=&quot;30306&quot; # 关闭健康检查primary.startupProbe.enabled=false primary.readinessProbe.enabled=false primary.livenessProbe.enabled=false # 关闭主从secondary.replicaCount=0 secondary.persistence.enable=false 启动Mysqlhelm install mysql8 /root/mydata/cicd/charts/mysql --namespace db-system 安装Redis创建StorageClass创建redis-StorageClass.yaml 12345678kind: StorageClassapiVersion: storage.k8s.io/v1metadata: name: local-storage-redis namespace: db-systemprovisioner: kubernetes.io/no-provisionervolumeBindingMode: WaitForFirstConsumerreclaimPolicy: Retain kubectl apply -f redis-StorageClass.yaml 创建PV创建redis-pv.yaml 123456789101112131415161718192021222324apiVersion: v1kind: PersistentVolumemetadata: name: redis-pv namespace: db-system labels: type: localspec: capacity: storage: 1Gi accessModes: - ReadWriteOnce persistentVolumeReclaimPolicy: Retain storageClassName: local-storage-redis local: path: &quot;/opt/redis&quot; # 确保目录存在 nodeAffinity: required: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/arch operator: In values: - amd64 kubectl apply -f redis-pv.yaml 创建PVC创建redis-pvc.yaml 1234567891011121314apiVersion: v1kind: PersistentVolumeClaimmetadata: name: redis-pvc namespace: db-system labels: app: redis-pvcspec: accessModes: - ReadWriteOnce #此处需要和pv对应才能匹配 resources: requests: storage: 1Gi storageClassName: local-storage-redis #此处需要和StorageClass.yaml匹配 kubectl apply -f redis-pvc.yaml 拉取mysql charthelp helm pull bitnami/redis 修改配置1234567891011121314# 单机模式architecture=standalone # NodoPort#--set master.service.type=NodePort # 配置暴露端口#--set master.service.nodePorts.redis=30919 # 配置密码#--set global.redis.password=root# 配置storageClass#--set primary.persistence.storageClass=&quot;local-storage-mysql&quot;# 配置内存#--set primary.persistence.size=1Gi# 配置pvc#--set primary.persistence.existingClaim=&quot;mysql-pvc&quot; 启动Redishelm install redis /root/mydata/cicd/charts/redis --namespace db-system 查看12345678910111213141516171819202122root@YH-UBUNTU:~/mydata/cicd# kubectl -n db-system get pv,pvcNAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGEpersistentvolume/mysql-pv 2Gi RWO Retain Bound db-system/mysql-pvc local-storage-mysql 7dpersistentvolume/redis-pv 1Gi RWO Retain Bound db-system/redis-pvc local-storage-redis 7dNAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGEpersistentvolumeclaim/mysql-pvc Bound mysql-pv 2Gi RWO local-storage-mysql 7dpersistentvolumeclaim/redis-pvc Bound redis-pv 1Gi RWO local-storage-redis 7droot@YH-UBUNTU:~/mydata/cicd# kubectl -n db-system get allNAME READY STATUS RESTARTS AGEpod/mysql8-0 1/1 Running 4 (43m ago) 7dpod/redis-master-0 1/1 Running 4 (43m ago) 6d23hNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEservice/mysql8-headless ClusterIP None &lt;none&gt; 3306/TCP 7dservice/mysql8 NodePort 10.43.201.139 &lt;none&gt; 3306:30306/TCP 7dservice/redis-headless ClusterIP None &lt;none&gt; 6379/TCP 6d23hservice/redis-master NodePort 10.43.111.113 &lt;none&gt; 6379:30919/TCP 6d23hNAME READY AGEstatefulset.apps/mysql8 1/1 7dstatefulset.apps/redis-master 1/1 6d23h 部署项目准备配置用于访问阿里云镜像仓库的Secret 1# kubectl create secret docker-registry aliyun-docker --docker-server=registry.cn-hangzhou.aliyuncs.com --docker-username=xxxx --docker-password=xxxx 后端部署项目地址：https://github.com/yangh124/weather-push 创建charthelm create weather-push 123456789101112# 删除不需要的文件，最终剩下以下文件root@YH-UBUNTU:~/mydata/cicd/charts# tree weather-pushweather-push├── Chart.yaml├── templates│ ├── deployment.yaml│ ├── _helpers.tpl│ ├── NOTES.txt│ └── service.yaml└── values.yaml1 directory, 6 files 修改value.yaml1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253# Default values for k8s-cicd.# This is a YAML-formatted file.# Declare variables to be passed into your templates.replicaCount: 1image: repository: nginx pullPolicy: IfNotPresent # Overrides the image tag whose default is the chart appVersion. tag: &quot;&quot;# 配置secret用于拉取镜像imagePullSecrets: - name: aliyun-dockernameOverride: &quot;&quot;fullnameOverride: &quot;&quot;podAnnotations: &#123;&#125;podSecurityContext: &#123;&#125; # fsGroup: 2000securityContext: &#123;&#125; # capabilities: # drop: # - ALL # readOnlyRootFilesystem: true # runAsNonRoot: true # runAsUser: 1000service: type: ClusterIP port: 8080resources: &#123;&#125; # We usually recommend not to specify default resources and to leave this as a conscious # choice for the user. This also increases chances charts run on environments with little # resources, such as Minikube. If you do want to specify resources, uncomment the following # lines, adjust them as necessary, and remove the curly braces after &#x27;resources:&#x27;. # limits: # cpu: 100m # memory: 128Mi # requests: # cpu: 100m # memory: 128MinodeSelector: &#123;&#125;tolerations: []affinity: &#123;&#125; 启动后端项目1helm upgrade --install weather-push /root/mydata/cicd/charts/weather-push --set image.repository=xxxx --set image.tag=xxxx 前端部署项目地址：https://github.com/yangh124/weather-push-admin 创建charthelm create weather-push-admin 123456789101112# 删除不需要的文件，最终剩下以下文件root@YH-UBUNTU:~/mydata/cicd/charts# tree weather-push-adminweather-push-admin├── Chart.yaml├── templates│ ├── deployment.yaml│ ├── _helpers.tpl│ ├── NOTES.txt│ └── service.yaml└── values.yaml1 directory, 6 files 修改value.yaml1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253# Default values for k8s-cicd.# This is a YAML-formatted file.# Declare variables to be passed into your templates.replicaCount: 1image: repository: nginx pullPolicy: IfNotPresent # Overrides the image tag whose default is the chart appVersion. tag: &quot;&quot;# 配置secret用于拉取镜像imagePullSecrets: - name: aliyun-dockernameOverride: &quot;&quot;fullnameOverride: &quot;&quot;podAnnotations: &#123;&#125;podSecurityContext: &#123;&#125; # fsGroup: 2000securityContext: &#123;&#125; # capabilities: # drop: # - ALL # readOnlyRootFilesystem: true # runAsNonRoot: true # runAsUser: 1000service: type: ClusterIP port: 80resources: &#123;&#125; # We usually recommend not to specify default resources and to leave this as a conscious # choice for the user. This also increases chances charts run on environments with little # resources, such as Minikube. If you do want to specify resources, uncomment the following # lines, adjust them as necessary, and remove the curly braces after &#x27;resources:&#x27;. # limits: # cpu: 100m # memory: 128Mi # requests: # cpu: 100m # memory: 128MinodeSelector: &#123;&#125;tolerations: []affinity: &#123;&#125; 启动前端项目1helm upgrade --install weather-push-admin /root/mydata/cicd/charts/weather-push-admin --set image.repository=xxxx --set image.tag=xxxx 部署完成1234567891011121314151617root@YH-UBUNTU:~/mydata/cicd/helm-script# kubectl get allNAME READY STATUS RESTARTS AGEpod/weather-push-admin-75997bd55d-hpn7m 1/1 Running 3 (65m ago) 6dpod/weather-push-975677b54-q5c5s 1/1 Running 11 (64m ago) 7dNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEservice/kubernetes ClusterIP 10.43.0.1 &lt;none&gt; 443/TCP 25dservice/weather-push ClusterIP 10.43.152.37 &lt;none&gt; 8080/TCP 7dservice/weather-push-admin ClusterIP 10.43.188.180 &lt;none&gt; 80/TCP 6dNAME READY UP-TO-DATE AVAILABLE AGEdeployment.apps/weather-push-admin 1/1 1 1 6ddeployment.apps/weather-push 1/1 1 1 7dNAME DESIRED CURRENT READY AGEreplicaset.apps/weather-push-admin-75997bd55d 1 1 1 6dreplicaset.apps/weather-push-975677b54 1 1 1 7d 安装ingress-nginx添加仓库helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx 拉取charthelm pull ingress-nginx/ingress-nginx 修改配置value.yaml1234controller.hostNetwork=truecontroller.dnsPolicy=ClusterFirstWithHostNetcontroller.nodeSelector.ingress=&quot;true&quot;controller.kind=DaemonSet 最后修改配置文件中的镜像仓库地址，默认是 k8s.gcr.io 国内无法访问修改后的配置：https://github.com/yangh124/cicd/blob/master/charts/ingress-nginx/values.yaml 启动ingress-nginx1234567helm upgrade --install ingress-nginx /root/mydata/cicd/charts/ingress-nginx --namespace ingress-nginx# 给当前节点打上标签 ingress=tureroot@YH-UBUNTU:~/mydata/cicd/charts# kubectl get nodeNAME STATUS ROLES AGE VERSIONyh-ubuntu Ready control-plane,master 27d v1.25.6+k3s1root@YH-UBUNTU:~/mydata/cicd/charts# kubectl label node yh-ubuntu ingress=true node/yh-ubuntu labeled 查看12345678910root@YH-UBUNTU:~/mydata/cicd/charts# kubectl -n ingress-nginx get allNAME READY STATUS RESTARTS AGEpod/ingress-nginx-controller-7w5ff 1/1 Running 4 (19m ago) 6d23hNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEservice/ingress-nginx-controller-admission ClusterIP 10.43.48.154 &lt;none&gt; 443/TCP 6d23hservice/ingress-nginx-controller NodePort 10.43.127.201 &lt;none&gt; 80:32080/TCP,443:32443/TCP 6d23hNAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGEdaemonset.apps/ingress-nginx-controller 1 1 1 1 1 ingress=true,kubernetes.io/os=linux 6d23h 配置ingress配置tls证书kubectl create secret tls admin-tls --key=/root/ssl/8798465_admin.yh124.space.key --cert=/root/ssl/8798465_admin.yh124.space.pem 前端ingress配置ingress-nginx-admin.yaml 1234567891011121314151617181920212223apiVersion: networking.k8s.io/v1kind: Ingressmetadata: name: admin-ingress annotations: nginx.ingress.kubernetes.io/rewrite-target: /spec: tls: - hosts: - admin.yh124.space secretName: admin-tls rules: - host: admin.yh124.space http: paths: - path: / pathType: Prefix backend: service: name: weather-push-admin port: number: 80 后端ingress配置ingress-nginx-api.yaml 1234567891011121314151617181920212223apiVersion: networking.k8s.io/v1kind: Ingressmetadata: name: api-ingress annotations: nginx.ingress.kubernetes.io/rewrite-target: /$2spec: tls: - hosts: - admin.yh124.space secretName: admin-tls rules: - host: admin.yh124.space http: paths: - path: /api(/|$)(.*) pathType: Prefix backend: service: name: weather-push port: number: 8080 启用配置12kubelctl apply -f ingress-nginx-admin.yamlkubelctl apply -f ingress-nginx-api.yaml 查看ingress1234[root@VM-16-2-centos ~]# kubectl get ingNAME CLASS HOSTS ADDRESS PORTS AGEapi-ingress &lt;none&gt; admin.yh124.space 10.43.198.167 80, 443 6d22hadmin-ingress &lt;none&gt; admin.yh124.space 10.43.198.167 80, 443 6d22h 部署完成1234567891011121314151617181920212223242526272829303132333435363738394041424344[root@VM-16-2-centos ~]# kubectl get all --all-namespacesNAMESPACE NAME READY STATUS RESTARTS AGEkube-system pod/local-path-provisioner-6c79684f77-55wlm 1/1 Running 2 (17d ago) 17dkube-system pod/coredns-d76bd69b-ctlw9 1/1 Running 2 (17d ago) 17dkube-system pod/metrics-server-7cd5fcb6b7-j62zh 1/1 Running 2 (17d ago) 17ddb-system pod/mysql8-0 1/1 Running 0 8ddb-system pod/redis-master-0 1/1 Running 0 8ddefault pod/weather-push-7489dd6bb-zfk76 1/1 Running 1 (8d ago) 8ddefault pod/weather-push-admin-5f48464c7f-ppn7v 1/1 Running 0 6d23hingress-nginx pod/ingress-nginx-controller-pd44l 1/1 Running 0 6d22hNAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEdefault service/kubernetes ClusterIP 10.43.0.1 &lt;none&gt; 443/TCP 17dkube-system service/kube-dns ClusterIP 10.43.0.10 &lt;none&gt; 53/UDP,53/TCP,9153/TCP 17dkube-system service/metrics-server ClusterIP 10.43.128.65 &lt;none&gt; 443/TCP 17ddb-system service/mysql8-headless ClusterIP None &lt;none&gt; 3306/TCP 8ddb-system service/mysql8 NodePort 10.43.159.214 &lt;none&gt; 3306:30306/TCP 8ddb-system service/redis-headless ClusterIP None &lt;none&gt; 6379/TCP 8ddb-system service/redis-master NodePort 10.43.173.144 &lt;none&gt; 6379:30919/TCP 8ddefault service/weather-push ClusterIP 10.43.253.44 &lt;none&gt; 8080/TCP 8dingress-nginx service/ingress-nginx-controller-admission ClusterIP 10.43.234.91 &lt;none&gt; 443/TCP 6d23hingress-nginx service/ingress-nginx-controller NodePort 10.43.198.167 &lt;none&gt; 80:32080/TCP,443:32443/TCP 6d23hdefault service/weather-push-admin NodePort 10.43.64.109 &lt;none&gt; 80:30015/TCP 6d23hNAMESPACE NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGEingress-nginx daemonset.apps/ingress-nginx-controller 1 1 1 1 1 ingress=true,kubernetes.io/os=linux 6d23hNAMESPACE NAME READY UP-TO-DATE AVAILABLE AGEkube-system deployment.apps/coredns 1/1 1 1 17dkube-system deployment.apps/local-path-provisioner 1/1 1 1 17dkube-system deployment.apps/metrics-server 1/1 1 1 17ddefault deployment.apps/weather-push 1/1 1 1 8ddefault deployment.apps/weather-push-admin 1/1 1 1 6d23hNAMESPACE NAME DESIRED CURRENT READY AGEkube-system replicaset.apps/coredns-d76bd69b 1 1 1 17dkube-system replicaset.apps/local-path-provisioner-6c79684f77 1 1 1 17dkube-system replicaset.apps/metrics-server-7cd5fcb6b7 1 1 1 17ddefault replicaset.apps/weather-push-7489dd6bb 1 1 1 8ddefault replicaset.apps/weather-push-admin-5f48464c7f 1 1 1 6d23hNAMESPACE NAME READY AGEdb-system statefulset.apps/mysql8 1/1 8ddb-system statefulset.apps/redis-master 1/1 8d","categories":[{"name":"运维","slug":"运维","permalink":"https://yangh124.github.io/categories/%E8%BF%90%E7%BB%B4/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"https://yangh124.github.io/tags/k8s/"}]},{"title":"MySQL性能分析工具的使用","slug":"MySQL性能分析工具的使用","date":"2022-06-05T16:00:00.000Z","updated":"2024-10-05T15:47:33.615Z","comments":true,"path":"2022/06/06/MySQL性能分析工具的使用/","permalink":"https://yangh124.github.io/2022/06/06/MySQL%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7%E7%9A%84%E4%BD%BF%E7%94%A8/","excerpt":"","text":"在数据库调优中，我们的目标就是响应时间更快，吞吐量更大。利用宏观的监控工具和微观的日志分析可以帮助我们快速的找到调优的思路和方式。 数据库服务优化步骤小结： 查看系统性能参数在MySQL中，可以使用show status语句查询一些MySQL数据库服务器的性能参数、执行频率。 1SHOW [GLOBAL | SESSION] STATUS LIKE &#x27;参数&#x27; 一些常用的性能参数如下： Connections：连接MySQL服务器的次数 Uptime：MySQL服务器的上线时间 Slow_queries：慢查询的次数 Innodb_rows_read：select查询返回的行数 Innodb_rows_inserted：insert操作插入的行数 Innodb_rows_updated：update操作更新的行数 Innodb_rows_deleted：delete操作删除的行数 Com_select：查询操作的次数 Com_insert：插入操作的次数，对于批量插入的insert操作，只累加一次。 Com_update：更新操作的次数 Com_delete：删除操作的次数 统计SQL的查询成本：last_query_cost如果想要查看某条SQL语句的查询成本，可以在执行完这条SQL语句之后，通过查询当前会话的last_query_cost变量值来得到当前查询的成本。它通常也是我们评价一个查询的执行效率的一个常用指标。这个查询的成本对应的是SQL语句所需要读取的页的数量。 定位执行慢的SQL：慢查询日志默认情况下，MySQL数据库没有开启慢查询日志，需要手动设置这个参数。如果不是调优的需要的话，一般不建议启动该参数，因为开启慢查询日志或多或少会带来一些性能影响。 开启慢查询日志参数开启slow_query_log1234# 查看是否开启SHOW VARIABLES LIKE &#x27;%slow_query%&#x27;;# 开启慢查询日志SET GLOBAL slow_query_log = on; 设置long_query_time12345# 查看long_query_timeSHOW VARIABLES LIKE &#x27;%long_query_time%&#x27;;# 修改为1秒SET SESSION long_query_time = 1;SET GLOBAL long_query_time = 1; 永久设置配置my.conf 慢查询日志分析工具：mysqldumpslow1234567891011# 取出使用最多的10条慢查询mysqldumpslow -s c -t 10 /data/mysql/xxx-slow.log # 取出查询时间最慢的3条慢查询 mysqldumpslow -s t -t 3 /data/mysql/xxx-slow.log # 得到按照时间排序的前10条里面含有左连接的查询语句 mysqldumpslow -s t -t 10 -g “left join” /data/mysql/xxx-slow.log # 按照扫描行数最多的mysqldumpslow -s r -t 10 -g &#x27;left join&#x27; /data/mysql/xxx-slow.log 查看SQL执行成本：SHOW PROFILESHOW PROFILE是MySQL提供的可以用来分析当前会话中的SQL都做了什么，执行的资源消耗情况，可用于sql调优的测量。默认情况下是关闭的。并保存最近15次的运行结果。我们可以在会话级别开启这个功能 12SHOW VARIABLES LIKE &#x27;profiling&#x27;;SET profiling = ON; 查看最近的查询 1SHOW PROFILES; 查看某一个查询的详细信息 1SHOW PROFILE ALL FOR QUERY [QUERY_ID]; 分析查询语句：EXPLAIN1EXPLAIN SELECT|DELETE|UPDATE|INSERT xxxx; EXPLAIN语句输出的各个列的作用如下： EXPLAIN的进一步使用EXPLAIN四种输出格式传统格式传统格式输出一个表格形式，概要说明查询计划。 1EXPLAIN xxxxxx JSON格式第一种格式中介绍的EXPLAIN语句输出中缺少了一个衡量执行计划好坏的重要属性 – 成本。而JSON格式是四种格式中输出信息最详尽的格式，里面包含了执行的成本信息。 1EXPLAIN FORMAT=JSON xxxxx EXPLAIN的Column与JSON的对应关系：（来源于Mysql5.7文档） 123456789101112131415161718192021222324252627282930313233343536&#123; &quot;query_block&quot;: &#123; &quot;select_id&quot;: 1, &quot;cost_info&quot;: &#123; &quot;query_cost&quot;: &quot;3.83&quot; &#125;, &quot;table&quot;: &#123; &quot;table_name&quot;: &quot;sys_user&quot;, &quot;access_type&quot;: &quot;range&quot;, &quot;possible_keys&quot;: [ &quot;PRIMARY&quot; ], &quot;key&quot;: &quot;PRIMARY&quot;, &quot;used_key_parts&quot;: [ &quot;id&quot; ], &quot;key_length&quot;: &quot;4&quot;, &quot;rows_examined_per_scan&quot;: 7, &quot;rows_produced_per_join&quot;: 7, &quot;filtered&quot;: &quot;100.00&quot;, &quot;cost_info&quot;: &#123; &quot;read_cost&quot;: &quot;2.43&quot;, &quot;eval_cost&quot;: &quot;1.40&quot;, &quot;prefix_cost&quot;: &quot;3.83&quot;, &quot;data_read_per_join&quot;: &quot;80K&quot; &#125;, &quot;used_columns&quot;: [ &quot;id&quot;, &quot;login_name&quot;, &quot;password&quot;, ... ], &quot;attached_condition&quot;: &quot;(`sys_user`.`id` &gt; 1)&quot; &#125; &#125;&#125; TREE格式tree格式是8.0.16版本之后引入的新格式，主要根据查询的各个部分之间的关系和各部分的执行顺序来描述如何查询。 可视化输出SHOW WARNINGS的使用在使用了EXPLAIN语句查看了某个查询的执行计划后，紧跟着还可以使用SHOW WARNINGS语句查看与这个查询的执行计划有关的一些扩展信息。 分析优化器执行计划：traceOPTIMIZED_TRACE是MySQL5.6引入的一项跟踪功能，他可以跟踪优化器做出的各种决策（比如访问表的方法、各种开销计算、各种转换等），并将跟踪结果记录到INFORMATION_SCHEMA.OPTIMIZER_TACE表中。此功能默认是关闭的。开启trace，并设置格式为JSON，同时设置trace最大能够使用的内存的大小，避免解析过程中因为默认内存大小而不能够完整展示。 12SET optimizer_trace=&quot;enable=on&quot;,end_markers_in_json=on;SET optimizer_trace_max_mem_size=1000000; 123select xxxxxx;SELECT * FROM information_schema.OPTIMIZER_TRACE; MySQL监控分析视图：sys schema索引情况123456# 1.查询冗余的索引SELECT * FROM sys.schema_redundant_indexes;# 2.查询未使用过的索引SELECT * FROM sys.schema_unused_indexes;# 3.查询索引的使用情况SELECT * FROM sys.schema_index_statistics WHERE table_schema=&#x27;dbname&#x27;; 表相关123456# 4.查询表的访问量SELECT table_schema,table_name,SUM(io_read_requests+io_write_requests) AS io FROM sys.schema_table_statistics GROUP BY table_schema,table_name ORDER BY io desc;# 5.查询占用buffer pool较多的表SELECT object_schema,object_name,allocated,data FROM sys.innodb_buffer_stats_by_table ORDER BY allocated LIMIT 10;# 6.查看表的全表扫描情况SELECT * FROM sys.statements_with_full_table_scans WHERE db=&#x27;dbname&#x27;; 语句相关123456# 7.监控SQL执行的频率SELECT db,exec_count,query FROM sys.statement_analysis ORDER BY exec_count desc;# 8.监控使用了排序的SQLSELECT db,exec_count,first_seen,last_seen,query FROM sys.statements_with_sorting LIMIT 1;# 9.监控使用了临时表或者磁盘临时表的SQLSELECT db,exec_count,tmp_tables,tmp_disk_tables,query FROM sys.statement_analysis WHERE tmp_tables&gt;0 OR tmp_disk_tables &gt; 0 ORDER BY (tmp_tables+tmp_disk_tables) DESC; IO相关12# 10.查看消耗的磁盘IO文件SELECT file,avg_read,avg_write,avg_read+avg_write as avg_io FROM sys.io_global_by_file_by_bytes ORDER BY avg_read limit 10; innodb相关1SELECT * FROM sys.innodb_lock_waits; 风险提示：通过sys库去查询时，MySQL会消耗大量资源去收集相关信息，严重的可能会导致业务请求被阻塞，从而引起故障。建议生产环境不要频繁去查询sys或performance_schema、information_schema来完成监控、巡检等工作。","categories":[{"name":"后端","slug":"后端","permalink":"https://yangh124.github.io/categories/%E5%90%8E%E7%AB%AF/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://yangh124.github.io/tags/MySQL/"}]},{"title":"Redisson分布式锁（使用注解方式）","slug":"Redisson分布式锁（使用注解方式）","date":"2022-05-29T16:00:00.000Z","updated":"2024-10-05T15:47:33.617Z","comments":true,"path":"2022/05/30/Redisson分布式锁（使用注解方式）/","permalink":"https://yangh124.github.io/2022/05/30/Redisson%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%EF%BC%88%E4%BD%BF%E7%94%A8%E6%B3%A8%E8%A7%A3%E6%96%B9%E5%BC%8F%EF%BC%89/","excerpt":"","text":"引入pom依赖12345&lt;dependency&gt; &lt;groupId&gt;org.redisson&lt;/groupId&gt; &lt;artifactId&gt;redisson-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;3.13.1&lt;/version&gt;&lt;/dependency&gt; 配置RedissonClient1234567891011121314151617181920212223242526272829303132333435import org.redisson.Redisson;import org.redisson.api.RedissonClient;import org.redisson.config.Config;import org.redisson.config.SingleServerConfig;import org.redisson.config.TransportMode;import org.springframework.beans.factory.annotation.Value;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;/** * @author : yh * @date : 2021/11/29 14:03 */@Configurationpublic class RedissonConfig &#123; @Value(&quot;$&#123;spring.redis.host&#125;&quot;) private String host; @Value(&quot;$&#123;spring.redis.port&#125;&quot;) private int port; @Value(&quot;$&#123;spring.redis.password&#125;&quot;) private String password; @Bean public RedissonClient redissonClient() &#123; Config config = new Config(); // 这里使用最简单的单机模式 SingleServerConfig singleServerConfig = config.useSingleServer(); singleServerConfig.setAddress(&quot;redis://&quot; + host + &quot;:&quot; + port); singleServerConfig.setPassword(password); return Redisson.create(config); &#125;&#125; 自定义RedissonLock注解123456789101112131415161718192021222324252627282930313233343536import java.lang.annotation.*;import java.util.concurrent.TimeUnit;/** * @author : yh * @date : 2022/5/27 16:05 */@Target(&#123;ElementType.METHOD&#125;)@Retention(RetentionPolicy.RUNTIME)@Documented@Inheritedpublic @interface RedissonLock &#123; /** * 锁的key */ String value(); /** * 锁的key SpEL 表达式 * */ String key() default &quot;&quot;; /** * 加锁时间 * */ long time() default -1; /** * 时间单位 * */ TimeUnit timeUnit() default TimeUnit.SECONDS;&#125; 定义Aspect123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102import java.util.Objects;import java.util.concurrent.TimeUnit;import org.aspectj.lang.ProceedingJoinPoint;import org.aspectj.lang.annotation.AfterThrowing;import org.aspectj.lang.annotation.Around;import org.aspectj.lang.annotation.Aspect;import org.aspectj.lang.annotation.Pointcut;import org.aspectj.lang.reflect.MethodSignature;import org.redisson.api.RLock;import org.redisson.api.RedissonClient;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.beans.factory.annotation.Value;import org.springframework.core.DefaultParameterNameDiscoverer;import org.springframework.expression.EvaluationContext;import org.springframework.expression.spel.standard.SpelExpressionParser;import org.springframework.expression.spel.support.StandardEvaluationContext;import org.springframework.stereotype.Component;/** * @author : yh * @date : 2022/5/27 16:07 */@Component@Aspectpublic class RedissonLockAspect &#123; @Autowired private RedissonClient redissonClient; // redis key全局前缀 @Value(&quot;$&#123;spring.redis.prefix&#125;&quot;) private String redisPrefix; /** * 用于SpEL表达式解析 */ private final SpelExpressionParser spelExpressionParser = new SpelExpressionParser(); /** * 用于获取方法参数定义名字 */ private final DefaultParameterNameDiscoverer defaultParameterNameDiscoverer = new DefaultParameterNameDiscoverer(); @Pointcut(&quot;@annotation(com.xxx.annotation.RedissonLock)&quot;) public void LockAspect() &#123;&#125; @Around(&quot;LockAspect()&quot;) public Object around(ProceedingJoinPoint proceedingJoinPoint) throws Throwable &#123; Object object; RLock lock = null; try &#123; // 获取注解实体信息 RedissonLock lockEntity = (((MethodSignature)proceedingJoinPoint.getSignature()).getMethod()).getAnnotation(RedissonLock.class); String key = lockEntity.key(); String cacheName = lockEntity.value(); long time = lockEntity.time(); TimeUnit timeUnit = lockEntity.timeUnit(); // 根据名字获取锁实例 lock = redissonClient.getLock(getKey(cacheName, key, proceedingJoinPoint)); // 这里加锁失败会阻塞等待；也可以使用tryLock，不阻塞，加锁失败返回flase，自行处理后续逻辑（比如抛异常） lock.lock(time, timeUnit); object = proceedingJoinPoint.proceed(); &#125; finally &#123; if (Objects.nonNull(lock) &amp;&amp; lock.isHeldByCurrentThread()) &#123; lock.unlock(); &#125; &#125; return object; &#125; @AfterThrowing(value = &quot;LockAspect()&quot;, throwing = &quot;ex&quot;) public void afterThrowing(Throwable ex) &#123; throw new RuntimeException(ex); &#125; /** * 获取缓存的key * * key 定义在注解上，支持SPEL表达式 cacheName 为缓存的名称 * * * @return 缓存的key -&gt; redisPrefix:cacheName:SpELVal */ public String getKey(String cacheName, String spel, ProceedingJoinPoint proceedingJoinPoint) &#123; MethodSignature methodSignature = (MethodSignature)proceedingJoinPoint.getSignature(); String[] paramNames = defaultParameterNameDiscoverer.getParameterNames(methodSignature.getMethod()); if (!&quot;&quot;.equals(spel) &amp;&amp; null != paramNames &amp;&amp; paramNames.length &gt; 0) &#123; EvaluationContext context = new StandardEvaluationContext(); Object[] args = proceedingJoinPoint.getArgs(); for (int i = 0; i &lt; args.length; i++) &#123; context.setVariable(paramNames[i], args[i]); &#125; Object value = spelExpressionParser.parseExpression(spel).getValue(context); if (null != value) &#123; return redisPrefix + &quot;:&quot; + cacheName + &quot;:&quot; + value; &#125; &#125; return redisPrefix + &quot;:&quot; + cacheName; &#125;&#125; 使用12345// 也可以指定加锁时间 time设值@RedissonLock(value = &quot;test:lock&quot;, key = &quot;#id&quot;)public void testLock(Long id) &#123; // ...&#125; 查看redis中的锁","categories":[{"name":"后端","slug":"后端","permalink":"https://yangh124.github.io/categories/%E5%90%8E%E7%AB%AF/"}],"tags":[{"name":"分布式","slug":"分布式","permalink":"https://yangh124.github.io/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"}]},{"title":"Docker实战","slug":"Docker实战","date":"2022-05-19T16:00:00.000Z","updated":"2024-10-05T15:47:33.609Z","comments":true,"path":"2022/05/20/Docker实战/","permalink":"https://yangh124.github.io/2022/05/20/Docker%E5%AE%9E%E6%88%98/","excerpt":"","text":"Docker 部署springboot项目使用docker-maven-plugin maven插件 服务器安装docker略 Docker开启远程访问 查看docker服务状态（查看docker.service位置）1234567pi@raspberrypi:~ $ service docker status● docker.service - Docker Application Container Engine Loaded: loaded (/lib/systemd/system/docker.service; enabled; vendor preset: enabled) Active: active (running) since Sun 2022-04-17 17:28:04 CST; 1 months 2 days ago......... 编辑docker.service1vim /lib/systemd/system/docker.service 修改[Service]下的ExecStart，添加-H tcp:&#x2F;&#x2F;0.0.0.0:2375；对外开放docker服务（建议不要使用默认端口2375）1ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2375 -H fd:// --containerd=/run/containerd/containerd.sock 重新加载Docker配置生效 12systemctl daemon-reload systemctl restart docker 服务器防火墙开放端口 浏览器访问http://ip:2375/version，测试是否成功 使用TLS(HTTPS)保证Docker服务安全1. 服务器端生成CA共钥和私钥12345678910111213141516171819202122232425262728293031# 创建目录存放CA证书pi@raspberrypi:~ $ mkdir ca# 进入目录pi@raspberrypi:~ $ cd ca# 生成私钥（会提示输入密码）pi@raspberrypi:~/ca $ openssl genrsa -aes256 -out ca-key.pem 4096Generating RSA private key, 4096 bit long modulus (2 primes)..........................++++.......++++e is 65537 (0x010001)Enter pass phrase for ca-key.pem:# 生成证书信息（需要输入上面的密码以及一些其他信息【注意Common Name填写服务器外网IP】）pi@raspberrypi:~/ca $ openssl req -new -x509 -days 3650 -key ca-key.pem -sha256 -out ca.pemEnter pass phrase for ca-key.pem:You are about to be asked to enter information that will be incorporatedinto your certificate request.What you are about to enter is what is called a Distinguished Name or a DN.There are quite a few fields but you can leave some blankFor some fields there will be a default value,If you enter &#x27;.&#x27;, the field will be left blank.-----Country Name (2 letter code) [AU]:CNState or Province Name (full name) [Some-State]:ZheJiangLocality Name (eg, city) []:HangZhouOrganization Name (eg, company) [Internet Widgits Pty Ltd]:yanghao Organizational Unit Name (eg, section) []:yanghaoCommon Name (e.g. server FQDN or YOUR name) []:192.168.3.3Email Address []:yh.124@qq.com 有了CA之后，就可以创建服务器密钥和证书签名请求(CSR)了。确保“Common Name”与你连接Docker时使用的主机名匹配 2. 创建服务器密钥和证书签名请求123456789# 创建服务器密钥pi@raspberrypi:~/ca $ openssl genrsa -out server-key.pem 4096Generating RSA private key, 4096 bit long modulus (2 primes)................................................................++++.........++++e is 65537 (0x010001)# 证书签名请求（注意修改为你的服务器IP）openssl req -subj &quot;/CN=192.168.3.3&quot; -sha256 -new -key server-key.pem -out server.csr 3. 配置白名单配置白名单，允许连接的ip（通过证书）;IP可以配置多个 1echo subjectAltName = IP:192.168.3.3,IP::192.168.3.5 &gt;&gt; extfile.cnf 4. 设置Docker守护进程密钥的扩展使用属性，仅用于服务器身份验证1echo extendedKeyUsage = serverAuth &gt;&gt; extfile.cnf 5. 生成签名证书（需要输入密码）12openssl x509 -req -days 3650 -sha256 -in server.csr -CA ca.pem -CAkey ca-key.pem \\ -CAcreateserial -out server-cert.pem -extfile extfile.cnf 6. 对于客户端身份验证，创建一个客户端密钥和证书签名请求12345678910111213141516# 生成 key.pemopenssl genrsa -out key.pem 4096# 生成 csropenssl req -subj &#x27;/CN=client&#x27; -new -key key.pem -out client.csr# 创建配置文件echo extendedKeyUsage = clientAuth &gt; extfile-client.cnf# 生成签名证书openssl x509 -req -days 3650 -sha256 -in client.csr -CA ca.pem -CAkey ca-key.pem \\ -CAcreateserial -out cert.pem -extfile extfile-client.cnf # 将服务器端的ca.pem cert.pem key.pem复制到本地scp ca.pem cert.pem key.pem yh@192.168.3.5:/Users/yh/ca 7. 删除不需要的文件12# 服务器（服务端）rm -rf server.csr extfile.cnf client.csr extfile-client.cnf 8. 修改文件权限，防止误删123456# 服务端chmod -v 0400 ca-key.pem server-key.pemchmod -v 0444 ca.pem server-cert.pem# 客户端chmod -v 0400 key.pemchmod -v 0444 ca.pem cert.pem 9. 再次修改服务器docker.service123vim /lib/systemd/system/docker.service# 修改ExecStart（添加证书的路径）ExecStart=/usr/bin/dockerd --tlsverify --tlscacert=/home/pi/ca/ca.pem --tlscert=/home/pi/ca/server-cert.pem --tlskey=/home/pi/ca/server-key.pem -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock 10. 重新加载daemon并重启docker12systemctl daemon-reload &amp;&amp; systemctl restart docker 11. 配置本地环境变量1234567mkdir -pv ~/.docker# 将证书copy到.docker下cp -v &#123;ca,cert,key&#125;.pem ~/.dockerchmod -v 0400 key.pemchmod -v 0444 ca.pem cert.pem# 配置环境变量export DOCKER_HOST=tcp://0.0.0.0:2376 DOCKER_TLS_VERIFY=1 12. 客户端测试1234# 配置了环境变量后就可以访问服务器docker服务docker ps# 没有配置环境变量docker --tlsverify --tlscacert=/Users/yh/.docker/ca.pem --tlscert=/Users/yh/.docker/cert.pem --tlskey=/Users/yh/.docker/key.pem -H=tcp://192.168.3.3:2376 version 项目配置配置docker-file-maven-plugin插件12345678910111213141516171819202122232425262728293031323334&lt;build&gt; &lt;plugins&gt; &lt;!-- dockerfile maven --&gt; &lt;plugin&gt; &lt;groupId&gt;com.spotify&lt;/groupId&gt; &lt;artifactId&gt;dockerfile-maven-plugin&lt;/artifactId&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;default&lt;/id&gt; &lt;goals&gt; &lt;goal&gt;build&lt;/goal&gt; &lt;goal&gt;push&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;!--docker镜像相关的配置信息--&gt; &lt;configuration&gt; &lt;contextDirectory&gt;$&#123;project.basedir&#125;&lt;/contextDirectory&gt; &lt;dockerfile&gt;$&#123;project.basedir&#125;/Dockerfile&lt;/dockerfile&gt; &lt;!--使用username和password标签，也可以使用useMavenSettingsForAuth，在settings.xml文件中的配置 servers--&gt; &lt;useMavenSettingsForAuth&gt;false&lt;/useMavenSettingsForAuth&gt; &lt;username&gt;xxx&lt;/username&gt; &lt;password&gt;xxx&lt;/password&gt; &lt;!--远程仓库地址--&gt; &lt;repository&gt;$&#123;docker.repository.registry&#125;/$&#123;docker.repository.namespace&#125;/$&#123;project.artifactId&#125;&lt;/repository&gt; &lt;tag&gt;$&#123;project.version&#125;&lt;/tag&gt; &lt;buildArgs&gt; &lt;JAR_FILE&gt;$&#123;project.build.finalName&#125;.jar&lt;/JAR_FILE&gt; &lt;/buildArgs&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; Dockerfile12345678910111213141516171819202122232425FROM houwm/jdk8:arm64MAINTAINER yanghao&lt;yh.124@qq.com&gt;# 参数ARG JAR_FILE# 环境变量ENV TZ=Asia/Shanghai LANG=C.UTF-8ENV PARAMS=&quot;--server.port=8080 --spring.profiles.active=prod -Xms512m -Xmx512m&quot;# 设置时区RUN ln -snf /usr/share/zoneinfo/$TZ /etc/localtime &amp;&amp; echo $TZ &gt; /etc/timezone# 复制jar包COPY target/$&#123;JAR_FILE&#125; /app.jar# 暴露端口EXPOSE 8080# 工作目录WORKDIR /# 执行命令ENTRYPOINT [&quot;/bin/sh&quot;,&quot;-c&quot;,&quot;java -jar /app.jar $&#123;PARAMS&#125;&quot;] 部署项目123456789101112131415# 会自动在远程服务器构建镜像mvn package# 就可以省略这个步骤mvn dockerfile:build# 会自动推送镜像到远程仓库mvn deploy# 就可以省略这个步骤mvn dockerfile:push# 如果要临时跳过所有的Dockerfile相关的目标，执行如下Maven命令mvn clean install -Ddockerfile.skip# 想跳过某一个goalmvn clean package -Ddockerfile.build.skipmvn clean package -Ddockerfile.tag.skipmvn clean deploy -Ddockerfile.push.skip","categories":[{"name":"运维","slug":"运维","permalink":"https://yangh124.github.io/categories/%E8%BF%90%E7%BB%B4/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"https://yangh124.github.io/tags/Docker/"}]},{"title":"InnoDB数据存储结构","slug":"InnoDB数据存储结构","date":"2022-01-14T16:00:00.000Z","updated":"2024-10-05T15:47:33.614Z","comments":true,"path":"2022/01/15/InnoDB数据存储结构/","permalink":"https://yangh124.github.io/2022/01/15/InnoDB%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8%E7%BB%93%E6%9E%84/","excerpt":"","text":"数据库的存储结构：页MySQL服务器上的存储引擎负责对表中的数据的读取和写入工作，不同存储引擎中存放的格式一般不同。此处只剖析InnoDB存储引擎的数据存储结构。 磁盘与内存交互的基本单位：页InnoDB将数据划分为若干个页，InnoDB中页的大小默认为16KB。 在数据库中，不论读一行，还是读多行，都是将这些行所在的页进行加载，也就是说，数据库管理存储空间的基本单位是页（Page），数据库I&#x2F;O操作的最小单位为页。 页结构概述页a、页b、页c…页n，这些页可以不在物理结构上相连，只要通过双向链表相关联即可。每个数据页中的记录会按照主键值从小到大的顺序组成一个单向链表，每个数据页都会为存储在它里面的记录生成一个页目录，在通过主键查找时就可以在页目录中使用二分法快速定位到对应的槽，然后再遍历该槽对应分组中的记录即可快速找到指定记录。 页的上层结构 区（Extent）是比页大一级的存储结构，在InnoDB存储引擎中，一个区会分配64个连续的页。因为InnoDB中页的默认大小是16KB，所以一个区的大小是 64 * 16KB &#x3D; 1M。 段（Segment）由一个或多个区组成，区在文件系统是一个连续分配的空间（在InnoDB中是连续的64个页），不过在段中不要求区与区之间是相邻的。段是数据库中的分配单位，不同类型的数据库对象以不同的段形式存在。当我们创建数据表、索引的时候，就会相应创建对应的段，比如创建一张表时会创建一个表段，创建一个索引时会创建一个索引段。 表空间（TableSpace）是一个逻辑容器，表空间存储的对象是段，在一个表空间中可以有一个或多个段，但是一个段只能属于一个表空间。数据库由一个或多个表空间组成。表空间从管理上可以分为系统表空间、用户表空间、撤销表空间、临时表空间等。 页的内部结构页如果按类来划分的话，常见的有数据页（保存B+树节点）、系统页、Undo页和事务数据页等。数据页是我们最常使用的页。 InnoDB行格式（或记录格式）COMPACT行格式 变长字段长度列表MySQL支持一些变长的数据类型，比如VARCHAR(M)、VARBINARY(M)、TEXT类型，BOLB类型，这些数据类型修饰列称为变长字段。注意： 这里面存储的变长长度和字段顺序是反过来的。比如在表中字段是a(10),b(15)，那么在变长字段列表中存储的长度顺序就是15，10。 NULL值列表 二进制位的值为1时，代表该列的值为NULL。 二进制位的值为0时，代表该列的值不为NULL。 NOT NULL的字段没有。 记录头信息（5字节）记录真实数据 Dynamic和Compressed行格式一个页的大小一般是16KB，也就是16384字节，而VARCHAR(M)最多可储存65533个字节。这样就出现了一个页存放不了一条记录，这种现象称为行溢出。Dynamic和Compressed行格式和Compact行格式挺像，只不过在处理行溢出数据时有分歧： Dynamic和Compressed两种行格式对于存放在BLOB中的数据采用了完全的行溢出方式，如图，在数据页中只存放20个字节的指针（溢出页的地址），实际的数据都存放在Off Page（溢出页）中。 Compact和Redundant两种格式会在记录的真实数据处存储一部分数据（存放768个前缀字节）。Compressed行记录格式的另一个功能就是，存储在其中的行数据会以zlib的算法进行非常压缩。因此对于BLOL，TEXT，VARCHAR这类大长度类型的数据能够进行非常有效的存储。 Redundant行格式Redundant行格式是MySQL5.0版本及之前InnoDB的行记录存储方式。 区、段与碎片区为什么要有区？B+树的每一层中的页都会形成一个双向链表如果以页为单位来分配存储空间的话，双向链表相邻的两个页之间的物理位置可能离的非常远。 扫描页就会是随机IO。随机IO是非常慢的，所以我们应该尽量的让链表中页的物理位置也相邻，这样进行范围查询的时候时候才可以使用所谓的顺序IO。一个区就是在物理位置上连续的64个页。因为InnoDB中的页大小。默认是16kb，所以一个区的大小是64*16&#x3D;1M。在表中数据量大的时候，为某个索引分配空间的时候就不再按照页为单位分配了，而是按照区为单位分配，甚至在表中数据特别多的时候，可以一次性分配多个连续的区。虽然可能造成一点点空间的浪费（数据不足以填满整个区），但是从性能角度看，可以消除很多随机IO，利大于弊。 为什么要有段？对于范围查询，其实是对B+树叶子节点的记录进行顺序扫描，而如果不区分叶子节点和非叶子节点，统统把节点代表的页面放到申请到的区中的话，进行范围扫描的效果就可能会比较差。所以InnoDB对B+树的叶子节点和非叶子节点进行区别对待，也就是说叶子节点有自己独有的区，非叶子节点也有自己独特的区。存放叶子节点的区的集合就是一个段（segment），存放非叶子节点的区的集合也是一个段。也就是说一个索引会生成2个段，一个叶子节点段，一个非叶子节点段。 除了索引的叶子节点段和非叶子节点段之外，InnoDB中还有为储存一些特殊的数据定义的段，比如回滚段。所以常见的段有数据段，索引段，回滚段。数据段即为B+树的叶子节点，索引段即为非叶子节点段。 段其实不对应表空间中某一个连续的物理区域，而是一个逻辑上的概念，由若干个零散的页面以及一些完整的区组成。 为什么要有碎片区？一个区默认占用1M（64*16K）存储空间，所以默认情况下一个只存了几条记录的小表也需要2M存储空间吗？ 为了考虑以完整的区为单位分配给某个段，对于数量较小的表太浪费存储空间的这种情况，InnoDB提出了一个碎片区（fragment） 的概念。在一个碎片区中，并不是所有的页都是为了存储同一个段的数据而存在的，而是碎片区中的页可以用于不同的目的，比如有些页用于段A，有些页用于段B，有些页用于段C，有些页甚至哪个段都不属于。碎片区直属于表空间，并不属于任何一个段。 所以此后为某个段分配存储空间的策略是这样的： 在刚开始向表中插入数据的时候，段是从某个碎片区以单个页面为单位来分配存储空间的。 当某个段已经占用了32个碎片区页面之后，就会申请以完整的区为单位来分配存储空间的。 区的分类 空闲的区（free）：现在还没有用到这个区中的任何页面。 有剩余空间的碎片区（free_frag）：表示碎片区中还有可用页面。 没有剩余空间的碎片区（full_frag）：表示碎片区的所有页面都被使用，没有空闲的页面。 附属于某个段的区（fseg）：每一个索引都可以分为叶子节点段和非叶子节点段。处于free、free_frag、full_frag都是碎片区，fseg为属于某个段的区。 表空间表空间可以看作是InnoDB存储引擎逻辑结构的最高层，所有的数据都存放在表空间中。 表空间是一个逻辑容器，表空间存储的对象是段，在一个表空间中可以有一个或多个段，但是一个段只能属于一个表空间。数据库由一个或多个表空间组成，表空间从管理上可以划分为系统表空间（system Tablespace），独立表空间（File-per-table Tablespace）、撤销表空间（Undo-tablesapce）和临时表空间（Temporary Tablespace）等。 独立表空间独立表空间结构独立表空间由段、区、页组成。 表空间对应的文件一个新建的表对应的 .idb 文件文件只占用了96k，才6个页面的大小（MySQL5.7），这是因为一开始表中没有数据。随着表中的数据增加，表空间对应的文件也逐渐增大。 系统表空间系统表空间和独立表空间基本类似，只不过MySQL只有一个系统表空间，在系统表空间中会额外记录一些有关整个系统信息的页面，这部分是独立表空间没有的。","categories":[{"name":"后端","slug":"后端","permalink":"https://yangh124.github.io/categories/%E5%90%8E%E7%AB%AF/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://yangh124.github.io/tags/MySQL/"}]},{"title":"索引的创建与设计原则","slug":"索引的创建与设计原则","date":"2022-01-14T16:00:00.000Z","updated":"2024-10-05T15:47:33.620Z","comments":true,"path":"2022/01/15/索引的创建与设计原则/","permalink":"https://yangh124.github.io/2022/01/15/%E7%B4%A2%E5%BC%95%E7%9A%84%E5%88%9B%E5%BB%BA%E4%B8%8E%E8%AE%BE%E8%AE%A1%E5%8E%9F%E5%88%99/","excerpt":"","text":"索引的声明与使用索引的分类 从功能逻辑上说，索引主要有四种，分别为普通索引、唯一索引、主键索引、全文索引。 按照物理实现方式，索引可以分为两种：聚簇索引和非聚簇索引。 按照作用字段个数进行划分，分成单列索引和联合索引。 MySQL8.0索引新特性支持降序索引降序索引以降序存储键值。虽然在语法上，从MySQL4版本开始就支持降序索引语法了。但实际上该DESC定义是被忽略的，直到MySQL8才开始真正支持降序索引（仅限于InnoDB存储引擎）。MySQL在8.0之前创建仍然是升序索引，使用时进行反向扫描，这大大降低了数据库的效率。 CREATE TABLE ts1(a int,b int,index idx_a_b(a ASC,b DESC)) 隐藏索引 ALTER TABLE tablename ALTER INDEX index_name INVISIBLE; #隐藏索引 ALTER TABLE tablename ALTER INDEX index_name VISIBLE; #显示索引隐藏显示索引可用与查看使用索引的效率提升 索引的设计原则数据准备创建表 123456789101112131415161718# 课程表CREATE TABLE `course` ( `id` int NOT NULL AUTO_INCREMENT, `course_id` int NOT NULL, `course_name` varchar(50) COLLATE utf8mb4_unicode_ci DEFAULT NULL, PRIMARY KEY (`id`)) ENGINE=InnoDB AUTO_INCREMENT=101 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;# 学生表CREATE TABLE `student_info` ( `id` int NOT NULL AUTO_INCREMENT, `student_id` int NOT NULL, `name` varchar(50) COLLATE utf8mb4_unicode_ci DEFAULT NULL, `course_id` int NOT NULL, `class_id` int DEFAULT NULL, `create_time` datetime DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP, PRIMARY KEY (`id`)) ENGINE=InnoDB AUTO_INCREMENT=1000001 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci; 生成随机数函数 12345678# 生成随机数函数CREATE DEFINER=`root`@`%` FUNCTION `rand_num`( from_num INT,to_num INT) RETURNS intBEGIN DECLARE i INT DEFAULT 0; SET i = FLOOR(from_num+RAND()*(to_num-from_num +1)); RETURN i;END 生成随机字符串函数 123456789101112## 生成随机字符串CREATE DEFINER=`root`@`%` FUNCTION `rand_string`(n INT) RETURNS varchar(255) CHARSET utf8mb4 COLLATE utf8mb4_unicode_ciBEGIN DECLARE chars_str varchar(100) DEFAULT &#x27;abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789&#x27;; DECLARE return_str varchar(255) DEFAULT &#x27;&#x27;; DECLARE i INT DEFAULT 0; WHILE i &lt; n DO SET return_str = concat(return_str,substring(chars_str , FLOOR(1 + RAND()*62 ),1)); SET i = i +1; END WHILE; RETURN return_str;END 生成课程表数据储存过程 1234567891011121314151617181920# 生成课程表数据储存过程CREATE DEFINER=`root`@`%` PROCEDURE `insert_course`( max_num INT )BEGIN DECLARE i INT DEFAULT 0; SET autocommit = 0; REPEAT SET i = i + 1; INSERT INTO course ( course_id, course_name ) VALUES ( rand_num ( 10000, 10100 ), rand_string ( 6 )); UNTIL i = max_num END REPEAT; COMMIT; END 生成学生表数据存储过程 123456789101112131415161718# 生成学生表存储过程CREATE DEFINER=`root`@`%` PROCEDURE `insert_stu`( max_num INT )BEGIN DECLARE i INT DEFAULT 0; SET autocommit = 0; REPEAT SET i = i + 1; INSERT INTO student_info ( course_id, class_id,student_id,name ) VALUES( rand_num(10000,10100),rand_num(10000,10200),rand_num(10000,20000),rand_string(6) ); UNTIL i = max_num END REPEAT; COMMIT; END 调用存储过程 12CALL insert_stu(1000000);CALL insert_course(100); 哪些情况适合创建索引 字段的数值有唯一性限制： 业务上具有唯一特性的字段，即使是组合字段，也必须创建组合索引（来源：Alibaba） 频繁作为where查询条件的字段 经常 group by 和 order by 的字段 update、delete的where条件 distinct字段需要创建索引 多表join连接操作时，创建索引的注意事项 连接表尽量不超过3张；对where条件创建索引，并且该字段在多张表中的类型必须一致（隐式转换（使用了函数，索引失效））； 使用列的类型小的创建索引 使用字符串前缀创建索引 区分度高（散列性高）的列适合创建索引 使用最频繁的列放到联合索引的左侧 在多个列都要创建索引时，联合索引优于单值索引 哪些情况不适合创建索引 在where中使用不到的字段不要创建索引 数据量小的表最好不要创建索引 有大量重复数据的列不要建立索引 避免对经常更新的表创建过多的索引 不建议用无序的值作为索引 删除不再使用的或者很少使用的索引 不要定义冗余或重复的索引 小结索引是一把双刃剑，可以提高查询效率，但是也会降低插入和更新的速度并占用磁盘空间。","categories":[{"name":"后端","slug":"后端","permalink":"https://yangh124.github.io/categories/%E5%90%8E%E7%AB%AF/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://yangh124.github.io/tags/MySQL/"}]},{"title":"InnoDB中的索引","slug":"InnoDB中的索引","date":"2022-01-11T16:00:00.000Z","updated":"2024-10-05T15:47:33.613Z","comments":true,"path":"2022/01/12/InnoDB中的索引/","permalink":"https://yangh124.github.io/2022/01/12/InnoDB%E4%B8%AD%E7%9A%84%E7%B4%A2%E5%BC%95/","excerpt":"","text":"InnoDB中的索引设计方案 0:普通的用户记录 1:目录项记录 2:最小记录 3:最大记录一般情况下，我们用到的B+树都不会超过4层。（减少磁盘I&#x2F;O）一个B+树的节点其实可以分成好多层，规定最下边的那层，也就是存放用户记录的那层为第0层，之后依次往上加。一个数据页可以存放16k的数据。假设所有存放用户记录的叶子节点代表的数据页可以存放100条用户记录，所有存放目录项记录的内节点代表的数据页可以存放1000条数据，那么： 如果B+树只有1层，也就是只有一个用于存放记录的节点，最多能存放100条记录。 如果B+树有2层，最多能存放1000 * 100 &#x3D; 10万条记录。 如果B+树有3层，最多能存放1000 * 1000 * 100 &#x3D; 1亿条记录 如果b+树有4层，做多能存放1000 * 1000 * 1000 * 100 &#x3D; 1000亿条记录 常见索引概念聚簇索引二级索引（辅助索引、非聚簇索引） 联合索引 InnoDB的B+树索引的注意事项根页面位置万年不变 每当某个表创建一个B+树索引（聚簇索引）的时候，都会为这个索引创建一个根节点页，最开始表中没有数据的时候，每个B+树索引对应的根节点中，既没有用户记录，也没有目录项记录。 随后向表中插入用户记录时，先把用户记录存储到这个根节点中。 当根节点中的可用空间用完时，继续插入记录，此时会将根节点中的所有记录复制到新分配的页，比如页a中，然后后对这个新页进行页分裂的操作，得到另一个新页，比如页b，这时新插入的记录根据键值（也就是索引值）的大小就会被分配到页a或者页b中，而根节点便升级为存储目录项记录的页这个过程特别注意的是： 一个B+树索引的根节点自诞生之日起，便不会再移动。这样只要我们对某个表建立一个索引，那么它的根节点的页号便会被记录到某个地方，然后凡事InnoDB存储引擎需要用到这个索引的时候，都会从哪个固定的地方取出根节点的页，从而来访问这个索引。 非叶子节点中的目录项记录的唯一性即非叶子节点的索引唯一（创建的索引默认后面会跟主键） 一个数据页最少存储2条数据MySQL数据结构选择的合理性Hash结构1. Hash索引仅能满足（=）（&lt;&gt;）和in查询。范围查询效率很差。 2. Hash索引数据存储是没有顺序的。 3. 对于联合索引的情况，Hash值是将联合索引合并后一起来计算的，无法对单独的一个键或者几个索引键进行查询。 4. 对于等值查询来说，通常Hash索引的效率很高，不过存在一种情况，就是索引列的重复值很多，效率就会降低。 另外，InnoDB本身不支持Hash索引，但是提供了自适应hash索引（adaptive_hash_index）。如果某个数据经常被访问，当满足一定条件的时候，就会将这个数据页的地址存放到Hash表中。这样下次查询的时候，就可以直接找到这个页面的所在位置。这样让B+树也具备了Hash索引的优点。 二叉搜索树二叉树特征： * 一个节点只能有两个子节点，也就是一个节点度不能超过2 * 左子节点 &lt; 本节点 &lt;&#x3D; 右子节点 在极端情况下会变成一个线性链表 AVL树 平衡二叉树特征： 它是一棵空树或它的左右两个子树的高度差的绝对值不超过1，并且左右两个子树都是一棵平衡二叉树。树的层数过多，访问一个节点相当于做一次磁盘IO B-TreeBalance Tree，也就是多路平衡查找树。 B树在插入和删除节点的时候如果导致树不平衡，就通过自动调节节点的位置保持树的自平衡。 关键字集合分布在整棵树中，即叶子节点和非叶子节点都存放数据。搜索有可能在非叶子节点结束 其搜索性能等价于在关键字全集内做一次二分查找。 B+TreeB+树也是一种多路搜索树，基于B-Tree做了改进。B+树适合文件索引系统。B+树和B树的差异： 有k个子节点就有k个关键字。也就是子节点数量&#x3D;关键字数量。而B树中，子节点数量&#x3D;关键字数量+1。 非叶子节点的关键字会同时存在在子节点中，并且是在子节点中所有关键字的最大（或最小）。 非叶子节点仅用于索引，不保存数据记录，跟记录相关的信息都存放在叶子节点中。而B树中，非叶子节点即保存索引也保存数据记录。 所有关键字都在叶子节点出现，叶子节点构成一个有序链表，而且叶子节点本身按照关键字的大小从小到大顺序链接。","categories":[{"name":"后端","slug":"后端","permalink":"https://yangh124.github.io/categories/%E5%90%8E%E7%AB%AF/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://yangh124.github.io/tags/MySQL/"}]},{"title":"Shell脚本","slug":"Shell脚本","date":"2021-12-03T16:00:00.000Z","updated":"2024-10-05T15:47:33.617Z","comments":true,"path":"2021/12/04/Shell脚本/","permalink":"https://yangh124.github.io/2021/12/04/Shell%E8%84%9A%E6%9C%AC/","excerpt":"","text":"HelloWorld第一个Shell脚本 12#!bin/bashecho &quot;Hello World!&quot; 输出结果: 1Hello World! Shell变量 定义变量时，变量名不加美元符号：your_name=&quot;yanghap&quot; 变量名和等号之间不能有空格，同时，变量名的命名须遵循如下规则： 命名只能使用英文字母，数字和下划线，首个字符不能以数字开头。 中间不能有空格，可以使用下划线 _。 不能使用标点符号。 不能使用bash里的关键字（可用help命令查看保留关键字）。 Shell传递参数我们可以在Shell脚本中传递参数，脚本传递参数的格式为：$n（n为一个数字）,1为第一个参数，2为第二个参数…$0代表执行时文件的文件路径 12345#!bin/bashecho &quot;执行的文件名：$0&quot;;echo &quot;第一个参数为：$1&quot;;echo &quot;第二个参数为：$2&quot;;echo &quot;第三个参数为：$3&quot;; sh /Users/yh/Downloads/test.sh 1 2 3输出结果: 1234执行的文件名：/Users/yh/Downloads/test.sh第一个参数为：1第二个参数为：2第三个参数为：3 几个特殊的参数处理 $# 传递到脚本的参数个数 $* 以一个单字符串显示所有向脚本传递的参数。如”$*”用「”」括起来的情况、以”$1 $2 … $n”的形式输出所有参数。 $$ 脚本运行的当前进程ID号 $! 后台运行的最后一个进程的ID号 $@ 与$*相同，但是使用时加引号，并在引号中返回每个参数。如”$@“用「”」括起来的情况、以”$1“ “$2“ … “$n“ 的形式输出所有参数。 $- 显示Shell使用的当前选项，与set命令功能相同。 $? 显示最后命令的退出状态。0表示没有错误，其他任何值表明有错误。 Shell数组 Shell数组用括号来表示，元素用空格分隔开array_name=(value1 value2 ... valuen) 读取数组：$&#123;array_name[index]&#125;实例:12345678#!/bin/bashmy_array=(A B &quot;C&quot; D)echo &quot;第一个元素为: $&#123;my_array[0]&#125;&quot;echo &quot;第二个元素为: $&#123;my_array[1]&#125;&quot;echo &quot;第三个元素为: $&#123;my_array[2]&#125;&quot;echo &quot;第四个元素为: $&#123;my_array[3]&#125;&quot; Shell运算符算术运算符+、-、*、&#x2F;、%、&#x3D;（赋值）、&#x3D;&#x3D;（相等）、!&#x3D;(不相等)注意：条件表达式要放在方括号内，并且要有空格，例如: [$a==$b] 是错误的，必须写成 [ $a == $b ]。 关系运算符关系运算符只支持数字，不支持字符串，除非字符串的值是数字。 -eq：检测两个数是否相等，相等返回 true。 -ne：!&#x3D; -gt：&gt; -lt：&lt; -ge：&gt;&#x3D; -le：&lt;&#x3D; 布尔运算符 !：非运算，表达式为 true 则返回 false，否则返回 true。 -o：或运算，有一个表达式为 true 则返回 true。（or） -a：与运算，两个表达式都为 true 才返回 true。（and） 逻辑运算符 &amp;&amp;：逻辑的 AND ||：逻辑的 OR 字符串运算符 =：检测两个字符串是否相等，相等返回 true。 !=：检测两个字符串是否不相等，不相等返回 true。 -z：检测字符串长度是否为0，为0返回 true。 -n：检测字符串长度是否不为 0，不为 0 返回 true。 $：检测字符串是否不为空，不为空返回 true。 其他文件测试运算符 Shell echo命令显示普通字符串echo &quot;It is a test&quot; 显示转义字符echo &quot;\\&quot;It is a test\\&quot;&quot;结果:&quot;It is a test&quot; 显示变量echo &quot;$1&quot; 显示换行12echo -e &quot;OK! \\n&quot; # -e 开启转义echo &quot;It is a test&quot; 结果: 123OK!It is a test 显示不换行123#!/bin/shecho -e &quot;OK! \\c&quot; # -e 开启转义 \\c 不换行echo &quot;It is a test&quot; 结果： 1OK! It is a test 显示结果定向至文件echo &quot;It is a test&quot; &gt; myfile 原样输出字符串，不进行转义或取变量(用单引号)echo &#39;$name\\&quot;&#39;结果:$name\\&quot; 显示命令执行结果，使用反引号1echo `date` Shell printf命令Shell test命令Shell中的 test 命令用于检查某个条件是否成立，它可以进行数值、字符和文件三个方面的测试。 数值测试实例： 12345678num1=100num2=100if test $[num1] -eq $[num2]then echo &#x27;两个数相等！&#x27;else echo &#x27;两个数不相等！&#x27;fi 字符串测试文件测试Shell流程控制if1234567if conditionthen command1 command2 ... commandN fi if else123456789if conditionthen command1 command2 ... commandNelse commandfi if else-if else123456789if condition1then command1elif condition2 then command2else commandNfi for循环1234567for var in item1 item2 ... itemNdo command1 command2 ... commandNdone 实例： 1234for loop in 1 2 3 4 5do echo &quot;The value is: $loop&quot;done Shell输入&#x2F;输出重定向 command &gt; file：将输出重定向到 file。 command &lt; file：将输入重定向到 file。 command &gt;&gt; file：将输出以追加的方式重定向到 file。 n &gt; file：将文件描述符为 n 的文件重定向到 file。 n &gt;&gt; file：将文件描述符为 n 的文件以追加的方式重定向到 file。 n &gt;&amp; m：将输出文件 m 和 n 合并。 n &lt;&amp; m：将输入文件 m 和 n 合并。 &lt;&lt; tag：将开始标记 tag 和结束标记 tag 之间的内容作为输入。注意： 文件描述符 0 通常是标准输入（STDIN），1 是标准输出（STDOUT），2 是标准错误输出（STDERR）。 如果希望屏蔽STDOUT和STDERR，可以这样写：command &gt; /dev/null 2&gt;&amp;1注意： 这里的 2 和 &gt; 之间不可以有空格，2&gt; 是一体的时候才表示错误输出。 Shell 文件包含和其他语言一样，Shell 也可以包含外部脚本。这样可以很方便的封装一些公用的代码作为一个独立的文件。语法格式如下： 12345. filename # 注意点号(.)和文件名中间有一空格或source filename","categories":[{"name":"脚本语言","slug":"脚本语言","permalink":"https://yangh124.github.io/categories/%E8%84%9A%E6%9C%AC%E8%AF%AD%E8%A8%80/"}],"tags":[]},{"title":"通过企业微信（微信插件）实现每日天气推送","slug":"通过企业微信（微信插件）实现每日天气推送","date":"2021-11-05T16:00:00.000Z","updated":"2024-10-05T15:47:33.620Z","comments":true,"path":"2021/11/06/通过企业微信（微信插件）实现每日天气推送/","permalink":"https://yangh124.github.io/2021/11/06/%E9%80%9A%E8%BF%87%E4%BC%81%E4%B8%9A%E5%BE%AE%E4%BF%A1%EF%BC%88%E5%BE%AE%E4%BF%A1%E6%8F%92%E4%BB%B6%EF%BC%89%E5%AE%9E%E7%8E%B0%E6%AF%8F%E6%97%A5%E5%A4%A9%E6%B0%94%E6%8E%A8%E9%80%81/","excerpt":"","text":"企业微信配置进入企业微信官网，点击立即注册 登录企业微信管理后台，找到应用管理，点击创建自建应用（例如这里我创建了一个名称为天气推送的应用） 点击打开创建的应用，获取开发相关的配置信息（AgentId,Secret） AgentId：这个应用的唯一标识 Secret：密钥（需要发送到企业微信app才能查看） 到此所有企业微信配置结束天气开放平台配置进入和风天气开放平台，点击注册 注册完成后建议申请认证个人开发者（支持更多接口） 登录控制台，点击应用管理，创建应用，获取到应用的KEY 项目开发前端项目：weather-push-admin后台项目：weather-push 定时任务配置配置执行执行初始化sql在后台项目resources&#x2F;sql&#x2F;init.sql 运行前后端项目 登录后台使用默认账号密码登录：admin 123456 配置地区即配置需要发送天气预报的地区注：这里的地区对应企业微信通讯录中的标签 地区管理 -&gt; 添加 地区绑定成员 成员管理 -&gt; 选择左边的地区 -&gt; 点击添加成员注：这里的成员对应企业微信通讯录中的成员 创建定时任务 最终效果","categories":[{"name":"项目","slug":"项目","permalink":"https://yangh124.github.io/categories/%E9%A1%B9%E7%9B%AE/"}],"tags":[{"name":"实战","slug":"实战","permalink":"https://yangh124.github.io/tags/%E5%AE%9E%E6%88%98/"}]},{"title":"SpringBoot+RabbitMQ实战","slug":"SpringBoot RabbitMQ实战","date":"2021-10-28T16:00:00.000Z","updated":"2024-10-05T15:47:33.618Z","comments":true,"path":"2021/10/29/SpringBoot RabbitMQ实战/","permalink":"https://yangh124.github.io/2021/10/29/SpringBoot%20RabbitMQ%E5%AE%9E%E6%88%98/","excerpt":"","text":"一、引入依赖本文使用的SpringBoot版本为2.5.4 1.pom文件12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.5.4&lt;/version&gt; &lt;relativePath /&gt; &lt;!-- lookup parent from repository --&gt; &lt;/parent&gt; &lt;groupId&gt;com.yh&lt;/groupId&gt; &lt;artifactId&gt;rabbitmq-demo&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;name&gt;rabbitmq-demo&lt;/name&gt; &lt;description&gt;rabbitmq-demo&lt;/description&gt; &lt;properties&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-amqp&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-configuration-processor&lt;/artifactId&gt; &lt;optional&gt;true&lt;/optional&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;optional&gt;true&lt;/optional&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.amqp&lt;/groupId&gt; &lt;artifactId&gt;spring-rabbit-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;!-- https://mvnrepository.com/artifact/com.alibaba/fastjson --&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;fastjson&lt;/artifactId&gt; &lt;version&gt;1.2.78&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;excludes&gt; &lt;exclude&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;/exclude&gt; &lt;/excludes&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; 2.application.yml文件123456spring: rabbitmq: host: 192.168.2.9 port: 5672 username: yh password: yh 二、RabbitMQ SpringBoot配置1.配置RabbitTemplate1234567891011121314151617181920212223242526272829303132333435363738394041package com.yh.rabbitmqdemo.config;import lombok.extern.slf4j.Slf4j;import org.springframework.amqp.rabbit.connection.ConnectionFactory;import org.springframework.amqp.rabbit.core.RabbitTemplate;import org.springframework.amqp.support.converter.Jackson2JsonMessageConverter;import org.springframework.amqp.support.converter.MessageConverter;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;import org.springframework.context.annotation.DependsOn;/** * rabbitmq 配置 * * @author : yh * @date : 2021/9/9 20:12 */@Slf4j@Configurationpublic class RabbitMqConfig &#123; @DependsOn(&quot;messageConverter&quot;) @Bean(&quot;rabbitTemplate&quot;) public RabbitTemplate rabbitTemplate(ConnectionFactory connectionFactory) &#123; RabbitTemplate rabbitTemplate = new RabbitTemplate(connectionFactory); rabbitTemplate.setMessageConverter(messageConverter()); return rabbitTemplate; &#125; /** * 配置消息转换器 */ @Bean public MessageConverter messageConverter() &#123; return new Jackson2JsonMessageConverter(); &#125;&#125; 2.定义交换机（Exchange）12345678910111213141516171819202122232425262728293031323334353637383940package com.yh.rabbitmqdemo.rabbitmq;import org.springframework.amqp.core.Exchange;import org.springframework.amqp.core.ExchangeBuilder;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;/** * 定义交换机 * 这里定义了所有类型的交换机（四种） * 方便学习测试 * &lt;p&gt; * durable 消息持久化 * * @author : yh * @date : 2021/9/9 20:38 */@Configurationpublic class ExchangeConfig &#123; @Bean(&quot;myDirectExchange&quot;) public Exchange myDirectExchange() &#123; return ExchangeBuilder.directExchange(&quot;my-direct-exchange&quot;).durable(true).build(); &#125; @Bean(&quot;myTopicExchange&quot;) public Exchange myTopicExchange() &#123; return ExchangeBuilder.topicExchange(&quot;my-topic-exchange&quot;).durable(true).build(); &#125; @Bean(&quot;myFanoutExchange&quot;) public Exchange myFanoutExchange() &#123; return ExchangeBuilder.fanoutExchange(&quot;my-fanout-exchange&quot;).durable(true).build(); &#125; @Bean(&quot;myHeadsExchange&quot;) public Exchange myHeadersExchange() &#123; return ExchangeBuilder.headersExchange(&quot;my-headers-exchange&quot;).durable(true).build(); &#125;&#125; 3.定义队列（Queue）","categories":[{"name":"后端","slug":"后端","permalink":"https://yangh124.github.io/categories/%E5%90%8E%E7%AB%AF/"}],"tags":[{"name":"实战","slug":"实战","permalink":"https://yangh124.github.io/tags/%E5%AE%9E%E6%88%98/"}]},{"title":"SpringBoot+Elasticsearch实战","slug":"SpringBoot ES实战","date":"2021-10-25T16:00:00.000Z","updated":"2024-10-05T15:47:33.617Z","comments":true,"path":"2021/10/26/SpringBoot ES实战/","permalink":"https://yangh124.github.io/2021/10/26/SpringBoot%20ES%E5%AE%9E%E6%88%98/","excerpt":"一、引入依赖本文使用的SpringBoot版本为2.5.4，elasticsearch版本为7.15.0","text":"一、引入依赖本文使用的SpringBoot版本为2.5.4，elasticsearch版本为7.15.0 1. pom文件如下1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.5.4&lt;/version&gt; &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt; &lt;/parent&gt; &lt;groupId&gt;com.yh&lt;/groupId&gt; &lt;artifactId&gt;es-demo&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;name&gt;es-demo&lt;/name&gt; &lt;description&gt;es-demo&lt;/description&gt; &lt;properties&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;elasticsearch.version&gt;7.15.0&lt;/elasticsearch.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-configuration-processor&lt;/artifactId&gt; &lt;optional&gt;true&lt;/optional&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;optional&gt;true&lt;/optional&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;!--使用的rest-high-level-client--&gt; &lt;dependency&gt; &lt;groupId&gt;org.elasticsearch.client&lt;/groupId&gt; &lt;artifactId&gt;elasticsearch-rest-high-level-client&lt;/artifactId&gt; &lt;version&gt;$&#123;elasticsearch.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.elasticsearch&lt;/groupId&gt; &lt;artifactId&gt;elasticsearch&lt;/artifactId&gt; &lt;version&gt;$&#123;elasticsearch.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;fastjson&lt;/artifactId&gt; &lt;version&gt;1.2.70&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;excludes&gt; &lt;exclude&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;/exclude&gt; &lt;/excludes&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; 2. application.yml配置如下123elasticsearch: host-arr: - 127.0.0.1:9200 二、ES SpringBoot配置1. 配置ES服务器地址123456789101112131415161718192021package com.yh.esdemo.config;import lombok.Getter;import lombok.Setter;import org.springframework.boot.context.properties.ConfigurationProperties;import org.springframework.stereotype.Component;/** * @author : yh * @date : 2021/8/30 20:35 */@Component@Getter@Setter@ConfigurationProperties(prefix = &quot;elasticsearch&quot;)public class EsHostConfig &#123; private String[] hostArr;&#125; 2. 配置RestHighLevelClient12345678910111213141516171819202122232425262728293031323334package com.yh.esdemo.config;import org.apache.http.HttpHost;import org.elasticsearch.client.RestClient;import org.elasticsearch.client.RestHighLevelClient;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;/** * 定义RestClient Bean * * @author : yh * @date : 2021/8/30 20:32 */@Configurationpublic class EsRestClientConfig &#123; @Autowired private EsHostConfig esHostConfig; @Bean public RestHighLevelClient restHighLevelClient() &#123; String[] hostArr = esHostConfig.getHostArr(); int size = hostArr.length; HttpHost[] httpHostArr = new HttpHost[size]; for (int i = 0; i &lt; size; i++) &#123; String[] split = hostArr[i].split(&quot;:&quot;); httpHostArr[i] = new HttpHost(split[0], Integer.parseInt(split[1]), &quot;http&quot;); &#125; return new RestHighLevelClient(RestClient.builder(httpHostArr)); &#125; &#125; 配置完成后只需在相关业务中注入RestHighLevelClient即可使用 三、ES基本使用索引操作1. 创建索引123456789101112131415161718192021222324252627282930313233343536@Testpublic void addIndex() throws Exception &#123; //获取 IndicesClient ，用于创建 Index IndicesClient indices = restHighLevelClient.indices(); //定义一个 CreateIndexRequest ，参数为索引的名称 CreateIndexRequest createIndexRequest = new CreateIndexRequest(&quot;yh_customer&quot;); //CreateIndexRequest配置相关参数 number_of_shards：分片数 number_of_replicas：每个分片的副本数 createIndexRequest.settings( Settings.builder() .put(&quot;index.number_of_shards&quot;, 1) .put(&quot;index.number_of_replicas&quot;, 0) ); Map&lt;String, Object&gt; name = new HashMap&lt;&gt;(); name.put(&quot;type&quot;, &quot;text&quot;); name.put(&quot;analyzer&quot;, &quot;ik_max_word&quot;);//指定分词器 name.put(&quot;search_analyzer&quot;, &quot;ik_smart&quot;); Map&lt;String, Object&gt; timestamp = new HashMap&lt;&gt;(); timestamp.put(&quot;type&quot;, &quot;date&quot;); timestamp.put(&quot;format&quot;, &quot;yyyy-MM-dd HH:mm:ss&quot;);//指定时间格式 Map&lt;String, Object&gt; uuid = new HashMap&lt;&gt;(); uuid.put(&quot;type&quot;, &quot;keyword&quot;); Map&lt;String, Object&gt; properties = new HashMap&lt;&gt;(); properties.put(&quot;name&quot;, name); properties.put(&quot;timestamp&quot;, timestamp); properties.put(&quot;uuid&quot;, uuid); Map&lt;String, Object&gt; mapping = new HashMap&lt;&gt;(); mapping.put(&quot;properties&quot;, properties); //创建mapping mapping可以理解为此索引中数据的数据结构 createIndexRequest.mapping(mapping); //创建索引 此方法同步返回结果 异步使用 createAsync CreateIndexResponse createIndexResponse = indices.create(createIndexRequest, RequestOptions.DEFAULT); //创建结果（true/false） System.out.println(createIndexResponse.isAcknowledged()); //输出创建成功的索引名称 System.out.println(createIndexResponse.index());&#125; 最终输出: 2. 删除索引1234567@Testpublic void delIndex() throws Exception &#123; IndicesClient indices = restHighLevelClient.indices(); DeleteIndexRequest deleteIndexRequest = new DeleteIndexRequest(&quot;yh_customer&quot;); AcknowledgedResponse deleteIndexResponse = indices.delete(deleteIndexRequest, RequestOptions.DEFAULT); System.out.println(deleteIndexResponse.isAcknowledged());&#125; 增删改查1. 添加数据（单个）12345678910111213@Testpublic void addDoc() throws Exception &#123; SimpleDateFormat simpleDateFormat = new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;); String timestamp = simpleDateFormat.format(new Date()); Map&lt;String, Object&gt; jsonMap = new HashMap&lt;&gt;(); jsonMap.put(&quot;uuid&quot;, &quot;534523dqw4qweq24324&quot;); jsonMap.put(&quot;name&quot;, &quot;张三&quot;); jsonMap.put(&quot;timestamp&quot;, timestamp); //创建 IndexRequest 指定id IndexRequest indexRequest = new IndexRequest(&quot;yh_customer&quot;).id(&quot;1&quot;).source(jsonMap); IndexResponse indexResponse = restHighLevelClient.index(indexRequest, RequestOptions.DEFAULT); System.out.println(indexResponse);&#125; 最终输出： 2. 批量添加数据1234567891011121314151617181920212223242526272829303132333435363738394041424344454647 @Test public void bulkTest() &#123; //批量添加使用 BulkRequest BulkRequest bulkRequest = new BulkRequest(); for (int i = 0; i &lt; 100; i++) &#123; Customer customer = new Customer(UUID.randomUUID().toString().replace(&quot;-&quot;, &quot;&quot;), i + &quot;号客户&quot;, LocalDateTime.now()); //String data = JSONObject.toJSONString(customer); String data = JSONObject.toJSONStringWithDateFormat(customer, &quot;yyyy-MM-dd HH:mm:ss&quot;); IndexRequest request = new IndexRequest(&quot;yh_customer&quot;).source(data, XContentType.JSON); bulkRequest.add(request); &#125; //同步// try &#123;// BulkResponse bulk = restHighLevelClient.bulk(bulkRequest, RequestOptions.DEFAULT);// boolean hasFailures = bulk.hasFailures();// if (hasFailures) &#123;// List&lt;BulkItemResponse&gt; collect = Arrays.stream(bulk.getItems()).filter(BulkItemResponse::isFailed).collect(Collectors.toList());// System.out.println(collect.size());// String msg = bulk.buildFailureMessage();// System.out.println(msg);// &#125;// &#125; catch (IOException e) &#123;// e.printStackTrace();// &#125; //异步 restHighLevelClient.bulkAsync(bulkRequest, RequestOptions.DEFAULT, new ActionListener&lt;BulkResponse&gt;() &#123; //成功回调 @Override public void onResponse(BulkResponse bulkItemResponses) &#123; System.out.println(&quot;success&quot;); &#125; //失败回调 @Override public void onFailure(Exception e) &#123; System.out.println(&quot;failure&quot;); e.printStackTrace(); &#125; &#125;); try &#123; TimeUnit.MINUTES.sleep(5); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; 最终输出： 3. 修改数据123456789@Testpublic void updateDoc() throws Exception &#123; UpdateRequest updateRequest = new UpdateRequest(&quot;yh_customer&quot;, &quot;1&quot;); Map&lt;String, Object&gt; jsonMap = new HashMap&lt;&gt;(); jsonMap.put(&quot;name&quot;, &quot;李四&quot;); updateRequest.doc(jsonMap); UpdateResponse update = restHighLevelClient.update(updateRequest, RequestOptions.DEFAULT); System.out.println(update);&#125; 最终输出: 4. 获取数据1234567@Testpublic void get() throws Exception &#123; GetRequest getRequest = new GetRequest(&quot;yh_customer&quot;); getRequest.id(&quot;1&quot;); GetResponse getResponse = restHighLevelClient.get(getRequest, RequestOptions.DEFAULT); System.out.println(getResponse);&#125; 最终输出： 5. 直接获取source123456@Testpublic void getSource() throws Exception &#123; GetSourceRequest getRequest = new GetSourceRequest(&quot;yh_customer&quot;, &quot;1&quot;); GetSourceResponse source = restHighLevelClient.getSource(getRequest, RequestOptions.DEFAULT); System.out.println(source);&#125; 最终输出： 判断是否存在1234567@Testpublic void exists() throws Exception &#123; GetRequest getRequest = new GetRequest(&quot;yh_customer&quot;); getRequest.id(&quot;1&quot;); boolean exists = restHighLevelClient.exists(getRequest, RequestOptions.DEFAULT); System.out.println(exists);&#125; 最终输出: 四、ES简单搜索根据条件搜索1234567891011121314151617@Testpublic void search() throws Exception &#123; SearchRequest searchRequest = new SearchRequest(&quot;yh_customer&quot;);//没有参数，查询所有索引 //构建搜索条件 SearchSourceBuilder searchSourceBuilder = new SearchSourceBuilder(); //根据name字段搜索 searchSourceBuilder.query(QueryBuilders.matchQuery(&quot;name&quot;, &quot;1&quot;)); searchRequest.source(searchSourceBuilder); SearchResponse search = restHighLevelClient.search(searchRequest, RequestOptions.DEFAULT); //返回命中数据 SearchHits hits = search.getHits(); for (SearchHit hit : hits.getHits()) &#123; String sourceAsString = hit.getSourceAsString(); Customer customer = JSONObject.parseObject(sourceAsString, Customer.class); System.out.println(customer); &#125;&#125; 最终输出: 查询所有123456789101112131415@Testpublic void simpleSearch() throws Exception &#123; //创建搜索request SearchRequest searchRequest = new SearchRequest(&quot;bank&quot;); SearchSourceBuilder searchSourceBuilder = new SearchSourceBuilder(); searchSourceBuilder.query(QueryBuilders.matchAllQuery());//检索所有字段 SearchRequest request = searchRequest.source(searchSourceBuilder); SearchResponse search = restHighLevelClient.search(request, RequestOptions.DEFAULT); SearchHits hits = search.getHits();//命中条数 System.out.println(&quot;总条数：&quot; + hits.getTotalHits().value); for (SearchHit hit : hits) &#123;//默认只检索出10条 Staff staff = JSONObject.parseObject(hit.getSourceAsString(), Staff.class); System.out.println(staff); &#125;&#125; 最终输出： 五、ES聚合搜索需要先导入官方提供的测试数据（使用kibana） 1234567891011121314151617181920212223242526272829303132 @Test public void aggSearch() throws Exception &#123; //创建搜索request SearchRequest searchRequest = new SearchRequest(&quot;bank&quot;); SearchSourceBuilder searchSourceBuilder = new SearchSourceBuilder(); //查询条件 searchSourceBuilder.query(QueryBuilders.matchAllQuery()); //统计各个年龄的平均工资 searchSourceBuilder.aggregation(AggregationBuilders.terms(&quot;ageAgg&quot;).field(&quot;age&quot;).subAggregation(AggregationBuilders.avg(&quot;balanceAvgAgg&quot;).field(&quot;balance&quot;))); //所有人平均工资 searchSourceBuilder.aggregation(AggregationBuilders.avg(&quot;allBalanceAvgAgg&quot;).field(&quot;balance&quot;)); //构建request SearchRequest request = searchRequest.source(searchSourceBuilder); //search SearchResponse searchResponse = restHighLevelClient.search(request, RequestOptions.DEFAULT); //Aggregations aggregations = searchResponse.getAggregations(); //Aggregation接口有很多实现类 Avg（平均值）// Avg avg = aggregations.get(&quot;balanceAgg&quot;);// System.out.println(avg.getValue()); Aggregations aggregations = searchResponse.getAggregations(); Terms ageAgg = aggregations.get(&quot;ageAgg&quot;); List&lt;? extends Terms.Bucket&gt; buckets = ageAgg.getBuckets(); for (Terms.Bucket bucket : buckets) &#123;//只查出了10条数据 System.out.println(&quot;年龄:&quot; + bucket.getKey()); System.out.println(&quot;人数:&quot; + bucket.getDocCount()); Avg avg = bucket.getAggregations().get(&quot;balanceAvgAgg&quot;); System.out.println(&quot;平均工资:&quot; + avg.getValue()); System.out.println(&quot;=================================&quot;); &#125; Avg avg = aggregations.get(&quot;allBalanceAvgAgg&quot;); System.out.println(&quot;所有人平均工资:&quot; + avg.getValue()); &#125; 最终输出： 1234567891011121314151617181920212223242526272829303132333435363738394041年龄:31人数:61平均工资:28312.918032786885=================================年龄:39人数:60平均工资:25269.583333333332=================================年龄:26人数:59平均工资:23194.813559322032=================================年龄:32人数:52平均工资:23951.346153846152=================================年龄:35人数:52平均工资:22136.69230769231=================================年龄:36人数:52平均工资:22174.71153846154=================================年龄:22人数:51平均工资:24731.07843137255=================================年龄:28人数:51平均工资:28273.882352941175=================================年龄:33人数:50平均工资:25093.94=================================年龄:34人数:49平均工资:26809.95918367347=================================所有人平均工资:25714.837 demo地址：https://github.com/yangh124/es-demo","categories":[{"name":"后端","slug":"后端","permalink":"https://yangh124.github.io/categories/%E5%90%8E%E7%AB%AF/"}],"tags":[{"name":"elastic search","slug":"elastic-search","permalink":"https://yangh124.github.io/tags/elastic-search/"},{"name":"实战","slug":"实战","permalink":"https://yangh124.github.io/tags/%E5%AE%9E%E6%88%98/"}]},{"title":"Netty","slug":"Netty","date":"2021-09-13T16:00:00.000Z","updated":"2024-10-05T15:47:33.616Z","comments":true,"path":"2021/09/14/Netty/","permalink":"https://yangh124.github.io/2021/09/14/Netty/","excerpt":"NettyNetty介绍应用场景Netty介绍 Netty是Jboss提供的一个开源框架，现为GitHub上的独立项目。 Netty是异步的、基于事件驱动的网络应用框架，用以快速开发高性能、高可靠性的网络IO程序。 Netty主要针对在TCP协议下，面向Clients端的高并发应用，或者Peer to Peer场景下的大量数据持续传输的应用。 Netty本质是NIO框架，适用于服务器通讯相关的多种应用场景。 要透彻理解Netty，需要先学习NIO，这样我们才能阅读Netty源码。","text":"NettyNetty介绍应用场景Netty介绍 Netty是Jboss提供的一个开源框架，现为GitHub上的独立项目。 Netty是异步的、基于事件驱动的网络应用框架，用以快速开发高性能、高可靠性的网络IO程序。 Netty主要针对在TCP协议下，面向Clients端的高并发应用，或者Peer to Peer场景下的大量数据持续传输的应用。 Netty本质是NIO框架，适用于服务器通讯相关的多种应用场景。 要透彻理解Netty，需要先学习NIO，这样我们才能阅读Netty源码。 Netty的应用场景 互联网行业 在分布式系统中，各个节点之间需要远程服务调用，高性能的RPC框架必不可少，Netty作为异步高性能通信框架，往往作为基础通信组件被这些RPC框架使用。 典型应用：阿里分布式服务框架Dubbo的RPC框架使用Dubbo协议进行节点间通信，Dubbo协议默认使用Netty作为基础通信组件，用与实现各进程节点之间的内部通信。 游戏行业 无论是手游服务端还是大型的网络游戏，Java语言得到了越来越广泛的应用。 Netty作为高性能的基础通信组件，提供了TCP&#x2F;UDP和HTTP协议栈，方便定制和开发私有协议，账号登录服务器。 地图服务器之间可以方便的通过Netty进行高性能的通信。 大数据领域 经典的Hadoop的高性能通信和序列化组件（AVRO 实现数据文件共享）的RPC框架，默认采用Netty进行跨界点通信。 它的Netty Service基于Netty框架二次封装实现的。 Java IO编程IO模型IO模型基本说明 IO模型简单的理解：就是用什么样的通道进行数据的发送和接收，很大程度上决定了程序通信的性能。 java共支持3种网络编程模型IO模式：NIO、BIO、AIO BIO：同步并阻塞（传统阻塞型），服务器实现模式为一个连接一个线程，即客户端有连接请求时服务器端就需要启动一个线程进行处理，如果这个连接不做任何事情会造成不必要的线程开销。 NIO：同步非阻塞，服务器实现模式为一个线程处理多个请求（连接），即客户端发送连接请求都会注册到多路复用器上， 多路复用器轮询到连接有IO请求就进行处理。 AIO（NIO.2）：异步非阻塞，AIO引入异步通道的概念，采用了Proactor模式，简化了程序编写，有效的请求才启动线程，它的特点是先由操作系统完成后才通知服务端程序启动线程取处理，一般适用于连接数多且连接时间较长的应用。 适用场景分析 BIO方式适用于连接数目较小且固定的架构，这种方式对服务器资源要求比较高，并发局限于应用中，JDK1.4以前的唯一选择，但程序简单易理解。 NIO方式适用于连接数目多且连接时间比较短（轻操作）的架构，比如聊天服务器，弹幕系统，服务间通讯等。编程比较复杂，JDK1.4开始支持。 AIO方式适用于连接数目多且连接比较长（重操作）的架构，比如相册服务器，充分调用OS参与并发操作，编程比较复杂，JDk7开始支持。 Java BIO Java BIO就是传统的java io编程，其相关的类和接口在java.io BIO（block IO）：同步阻塞，服务器实现模式为一个连接一个线程，即客户端有连接请求是服务端就需要启动一个线程进行处理，如果连接不做任何事情会造成不必要的线程开销，可以通过线程池机制改善（实现多个客户端连接服务器）。 BIO方式适用于连接数目比较小且固定的架构，这种方式对服务器资源的要求比较高，并发局限于应用中，JDK1.4以前唯一的选择程序简单易理解 对BIO流程的梳理 服务端启动一个ServerSocket 客户端启动Socket对服务器进行通信，默认情况下服务器端需要对每一个客户建立一个线程与之通讯 客户端发出请求后，先咨询服务器是否有线程响应，如果没有则会等待，或者拒绝 如果有响应，客户端线程会等待请求结束后，再继续执行 Java BIO应用实例 使用BIO模型编写一个服务端，监听6666端口，当有客户端连接时，就启动一个线程与之通信。 要求使用线程池机制改善，可以连接多个客户端。 服务端可以接收客户端发送的数据（telnet方式即可） demo见netty-study Java NIO Java NIO全称 java non-blocking IO，是指JDK提供的新API。从JDK1.4开始，Java提供了一系列改进的输入&#x2F;输出的新特性，被统称为NIO（New IO），是同步非阻塞的 NIO相关类都被放在java.nio包及子包下，并且对原java.nio包中的很多类进行改写。 NIO有三大核心部分：Channel（通道），Buffer（缓冲区），Selector（选择器）。 NIO是面向缓冲区，或者面向块编程的。数据读取到一个它稍后处理的缓冲区，需要时可在缓冲区中前后移动，这就增加了处理过程中的灵活性，使用它可以提供非阻塞的高伸缩性网络 Java NIO的非阻塞模式，使一个线程从某通道发送请求或者读取数据，但是它仅能得到目前可用的数据，如果目前没有可用的数据，就什么都不会获取，而不是保持线程阻塞，所以直至数据变得可以读之前，该线程可以继续做其他的事情。非阻塞写也是如此，一个线程请求写入一些数据到某通道，但不需要等待它完全写入，这个线程同时可以去做别的事情。 通俗理解：NIO是可以做到用一个线程来处理多个操作的。假设有10000个请求过来，根据实际情况，可以分配50或者100个线程来处理。不像之前的阻塞IO那样，非得分配10000个线程。 http2.0使用了多路复用的技术，做到同一个连接并发处理多个请求，而且并发请求的数量比http1.1大了好几个数量级。 NIO和BIO的比较 BIO以流的方式处理数据，而NIO以块的方式处理数据，块IO的效率比流IO高很多 BIO是阻塞的，NIO则是非阻塞的 BIO基于字节流和字符流进行操作，而NIO基于Channel（通道）和Buffer（缓冲区）进行操作，数据总是从通道读取到缓冲区中，或者从缓冲区写入到通道中。Selector（选择器）用于监听多个通道的事件（比如：连接请求，数据到达等），因此使用当个线程就可以监听多个客户端通道 NIO三大核心 每个Channel都会对应一个Buffer Selector会对应一个线程，一个线程对应多个Channel（连接） 该图反映了有三个Channel注册到该Selector上 程序切换到哪个Channel是由事件决定的，Event就是一个重要的概念 Selector会根据不同的事件，在各个通道上切换 Buffer就是一个内存块，底层是一个数组 数据的读取写入都是通过Buffer，这个和BIO有很大区别（BIO要么是输入流，要么是输出流，不能双向） Channel是双向的，可以反映操作系统底层的情况，比如Linux，底层的通道就是双向的 缓冲区（Buffer） 基本介绍： 缓冲区本质上是一个可以读写数据的内存块，可以理解为是一个容器对象（数组），该对象提供一组方法，可以轻松跟踪和记录缓存区的状态变换情况。Channel提供从文件、网络读取数据的渠道，但是读取或写入的数据都必须经由Buffer。如图： Buffer类定义了所有的缓冲区都具有4个属性来提供关于其所包含的数据元素的信息： Buffer类相关方法ByteBuffer： 对于Java中的基本数据类型（boolean）除外，都有一个Buffer类型与之相对应，最常用的自然是ByteBuffer类（二进制数据），该类的主要方法： 通道（Channel）基本介绍： NIO的通道类似于流，但是有些区别： 通道可以同时进行读写，而流只能读或者只能写 通道可以实现异步读写数据 通道可以从缓冲读数据，也可以写数据到缓冲 BIO的stream是单向的，例如FileInputStream对象只能进行读取数据的操作，而NIO中的通道（Channel）是双向的，可以读操作，也可以写操作。 Channel在NIO中是一个接口 常用的Channel类有：FileChannel、DatagramChannel、ServerSocketChannel和SocketChannel（ServerSocketChannel -&gt; Java ServerSocket；SocketChannel -&gt; Java Socket） FileChannel用于文件读写，DatagramChannel用于udp的数据读写，ServerSocketChannel和SocketChannel用于tcp数据读写 FileChannel类FileChannel主要用于对本地文件进行IO操作，常见的方法有 关于Buffer和Channel的注意事项和细节 Buffer支持类型化的put和get，put放入的是什么数据类型，get就应该使用相应的数据类型来取出，否则可能有BufferUnderflowException。 可以将一个普通Buffer转成只读Buffer。 NIO还提供了MappedByteBuffer，可以让文件直接在内存（堆外内存）中进行修改，而如何同步到文件由NIO来完成。 前面我们讲的读写操作，都是通过一个Buffer来完成的，NIO还支持通过多个Buffer（即Buffer数组）完成读写操作，即Scattering（分散）和Gathering（合并）。 选择器（Selector） Java的NIO，用非阻塞的IO方式。可以用一个线程，处理多个的客户端连接，就会使用到Selector（选择器）。 Selector能够检测多个注册的通道上是否有事件发生（注意：多个Channel以事件的方式可以注册到同一个Selector），如果有事件发生，便获取事件然后针对每个事件进行相应的处理。这样就可以只用一个单线程去管理多个通道，也就是管理多个连接和请求。 只有连接真正有读写事件发生时，才会进行读写，就大大地减少了系统开销，并且不必为每个连接都创建一个线程，不用去维护多个线程 避免了多线程之间的上下文切换导致的开销。特点再说明： Netty的IO线程NioEventLoop聚合了Selector（选择器，也叫多路复用器），可以同时并发处理成百上千个客户端连接。 当线程从某个客户端Socket通道进行读写数据时，若没有数据可用时，该线程可以进行其他任务。 线程通常将非阻塞IO的空闲时间用于在其他通道上执行IO操作，所以单独的线程可以管理多个输入和输出通道。 由于读写操作时非阻塞的，这就可以充分提升IO线程的运行效率，避免由于频繁的IO阻塞导致线程挂起。 一个IO线程可以并发的处理N个客户端连接和读写操作，这从根本上解决传统同步阻塞IO一连接一线程模型，架构的性能、弹性伸缩能力和可靠性都得到了极大的提升。 Selector相关方法说明： 1234selector.select();// 阻塞selector.select(1000);// 阻塞1秒，在1秒后返回selector.wakeup();// 唤醒selectorselector.selectNow();// 不阻塞，立即返回 NIO非阻塞网络编程原理分析图 说明： 1. 当客户端连接时，会通过ServerSocketChannel得到SocketChannel。 2. 将socketChannel注册到Selector上（register(Selector selector, int ops)），一个selector上可以注册多个SocketChannel。 3. 注册后返回一个selectionKey，会和该Selector关联（集合）。 4. Selector进行监听（select方法），返回有时间发生的通道个数,得到各个SelectionKey 5. 通过SelectionKey反向获取SocketChannel（channel方法） 6. 最后可以通过得到channel，完成业务处理 SelectionKeySelectionKey,表示Selector和网络通道的注册关系，共4种： * OP_ACCEPT：有新的网络连接可以accept，值为16 * OP_CONNECT：代表连接已经建立，值为8 * OP_READ：代表读操作，值为1 * OP_WRITE：代表写操作，值为4 ServerSocketChannel ServerSocketChannel在服务器端监听新的客户端Socket连接 相关方法 SocketChannel SocketChannel，网络IO通道，具体负责进行读写操作。NIO把缓冲区的数据写入通道，或者把通道里的数据读到缓冲区。 相关方法 NIO网络编程应用实例-群聊系统 编写一个NIO群聊系统，实现服务器和客户端之间的数据简单通讯（非阻塞） 实现多人群聊 服务端：可以监测用户上下线，并实现消息转发功能 客户端：通过Channel可以无阻塞发送消息给其它所有用户，同时可以接受其它用户发送的消息（由服务器转发得到） 目的：进一步理解NIO非阻塞网络编程 NIO于零拷贝 零拷贝（没有CPU拷贝）是网络编程的关键，很多性能优化都离不开。 在Java程序中，常用的零拷贝有mmap（内存映射）和sendFile。 传统IO4次拷贝，3次切换状态DMA：direct memory access（直接内存访问） mmap优化3次拷贝，3次切换状态 sendFile优化 Linux2.1版本提供了sendFile函数，其基本原理如下：数据根本不用经过用户态，直接从内核缓冲区进入到Socket Buffer，同时，由于和用户态完全无关，就减少了一次上下文切换。3次拷贝，2次切换状态 Linux2.4版本中，做了一些修改，避免从内核缓冲区拷贝到Socket Buffer的操作，直接拷贝到协议栈，从而又一次减少了数据拷贝。2次拷贝*，2次切换状态*这里其实是有一次cpu拷贝的，但是拷贝的信息很少，拷贝一些元数据（length，offset..）,消耗很低，可以忽略 总结 我们说零拷贝，是从操作系统的角度来说的。因为内核缓冲区之间，没有数据重复的（只有Kernel buffer中有一份数据）。 零拷贝不仅仅带来更少的数据复制，还能带来其他的性能优势，例如更少的上下文切换，更少的CPU缓存伪共享以及无CPU校验和计算。 Java AIO JDK7引入了Asynchronous IO，即AIO。在进行IO编程中，常用到两种模式：Reactor和Proactor。Java的NIO就是Reactor，当有事件触发时，服务端得到通知，进行相应的处理 AIO及NIO2.0，叫做异步不阻塞的IO。AIO引入异步通道的概念，采用了Proactor模式，简化了程序编写，有效的请求才启动线程，它的特点是先由操作系统完成后才通知服务端程序启动线程去处理，一般适用于连接数较多且连接事件较长的应用 目前AIO还没有广泛应用，Netty也是基于NIO的 Netty 原生NIO存在的问题 NIO的类库和API繁杂，使用麻烦：需要熟练掌握Selector、ServerSocketChannel、SocketChannel、ByteBuffer等。 需要具备其他额外的技能：要熟悉java多线程编程，因为NIO编程涉及到Reactor模式，你必须对多线程网络编程非常熟悉，才能编写出高质量的NIO程序。 开发工作量和难度非常大：例如客户端面临断连重连、网络闪断、半包读写、失败缓存、网络拥塞和异常流的处理等等。 JDK NIO的BUG：例如臭名昭著的Epoll Bug，它会导致Selector空轮询，最终导致CPU 100%。直到JDK1.7版本该问题依旧存在，没有被根本解决。 Netty高性能架构设计线程模型基本介绍 目前存在的线程模型有： 传统的阻塞IO服务模型 Reactor模式 根据Reactor的数量和处理资源池线程的数量不同，有3种典型的实现 单Reactor单线程 单Reactor多线程 主从Reactor多线程 Netty线程模式（Netty主要基于主从Reactor多线程模型做了一定的改进，其中主从Reactor多线程模型有多个Reactor） 说明： 1. Reactor模式，通过一个或多个输入同时传递给服务处理器的模式（基于事件驱动） 2. 服务端程序处理传入的多个请求，并将它们同步分派到相应的处理线程，因此Reactor模式也叫做Dispather模式 3. Reactor模式使用IO复用监听事件，收到事件后，分发给某个线程（进程）。这就是支持高并发的原因 Reactor模式核心组成 Reactor：Reactor在一个单独的线程中运行，负责监听和分发事件，分发给适当的处理程序来对IO事件做出反应。它就像公司的电话接线员，它接听来自客户的电话并将线路转移到适当的联系人。 Handlers：处理程序执行IO事件要完成的实际事件，类似于客户想要与之交谈的公司中的实际人员。Reactor通过调度适当的处理程序来响应IO事件，处理程序执行非阻塞操作。 单Reactor单线程 优点：模型简单，没有多线程、进程通信、竞争的问题，全部都在一个线程中完成 缺点：性能问题，只有一个线程，无法完全发挥多核CPU的性能。Handler在处理某个连接上的业务时，整个进程无法处理其他连接事件，很容易导致性能瓶颈；可靠性问题，线程意外终止，或者进入死循环，会导致整个系统通信模块不可用，不能接收和处理外部消息，造成节点故障。 使用场景：客户端数量有限，业务处理非常快速，比如Redis在业务处理的时间复杂度O(1)的情况 单Reactor多线程说明： Reactor对象通过select监控客户端请求事件，收到事件后，通过Dispatch进行分发 如果建立连接请求（accept），则由Accept通过accept处理连接请求，然后创建一个Handler对象处理完成连接后的各种事件 如果不是连接请求，则由Reactor分发调用连接对应的handler来处理 handler只负责响应事件，不做具体的业务处理，通过read读取数据后，会分发给后面的worker线程池的某个线程处理业务 worker线程池会分配独立线程完成真正的业务，并将处理结果返回给handler handler收到响应后，通过send方法将结果返回给client优点：可以比较充分利用多核cpu的处理能力缺点： 多线程数据共享和访问比较复杂，Reactor还是单线程的，在高并发下也会出现性能瓶颈 主从Reactor多线程说明： Reactor主线程MainReactor对象通过select监听连接事件，收到事件后，通过Acceptor处理连接事件 当Acceptor处理连接事件后，MainReactor将连接分配给SubReactor SubReactor将连接加入到连接队列，并创建handler进行各种事件的处理 当有新的事件发生时，SubReactor就会调用对应的handler进行处理 handler通过read读取数据，分发给后面的worker线程处理 worker线程池分配独立的worker线程进行处理，并返回结果 handler收到响应后，再通过send将结果返回client Reactor主线程可以对应多个Reactor子线程，一个MainReactor对应多个SubReactor优点：父线程与子线程的数据交互简单职责明确，父线程只需要接收新连接，子线程完成后续业务处理；父线程与子线程的数据交互简单，Reactor主线程只需要把新连接传给子线程，子线程无需返回数据缺点：编程复杂度较高结合实例：这种模型在许多项目中广泛使用，包括Nginx主从Reactor多线程模型，Memcached主从多线程，Netty主从多线程模型的支持 Netty模型工作原理示意图1 - 简单版 工作原理示意图2 - 进阶版 工作原理示意图 - 详细版 Netty 抽象出两组线程池：BossGroup 和 WorkerGroup，也可以叫做 BossNioEventLoopGroup 和WorkerNioEventLoopGroup。每个线程池中都有 NioEventLoop 线程。BossGroup 中的线程专门负责和客户端建立连接，WorkerGroup 中的线程专门负责处理连接上的读写。 BossGroup 和 WorkerGroup 的类型都是 NioEventLoopGroup。 NioEventLoopGroup 相当于一个事件循环组，这个组中含有多个事件循环，每个事件循环就是一个 NioEventLoop。 NioEventLoop 表示一个不断循环的执行事件处理的线程，每个 NioEventLoop 都包含一个 Selector，用于监听注册在其上的 Socket 网络连接（Channel）。 NioEventLoopGroup 可以含有多个线程，即可以含有多个 NioEventLoop。 每个 BossNioEventLoop 中循环执行以下三个步骤： select：轮询注册在其上的 ServerSocketChannel 的 accept 事件（OP_ACCEPT 事件） processSelectedKeys：处理 accept 事件，与客户端建立连接，生成一个 NioSocketChannel，并将其注册到某个 WorkerNioEventLoop 上的 Selector 上 runAllTasks：再去以此循环处理任务队列中的其他任务 每个 WorkerNioEventLoop 中循环执行以下三个步骤： select：轮训注册在其上的 NioSocketChannel 的 read&#x2F;write 事件（OP_READ&#x2F;OP_WRITE 事件） processSelectedKeys：在对应的 NioSocketChannel 上处理 read&#x2F;write 事件 runAllTasks：再去以此循环处理任务队列中的其他任务 在以上两个processSelectedKeys步骤中，会使用 Pipeline（管道），Pipeline 中引用了 Channel，即通过 Pipeline 可以获取到对应的 Channel，Pipeline 中维护了很多的处理器（拦截处理器、过滤处理器、自定义处理器等）。这里暂时不详细展开讲解 Pipeline。 任务队列中Task有3种典型的使用场景 用户自定义的普通任务 -&gt; TaskQueue 用户自定义定时任务 -&gt; ScheduleTaskQueue 非当前Reactor线程调用Channel的各种方法例如在推送系统的业务线程里面，根据用户的标识，找到对应的Channel引用，然后调用Write类方法向该用户推送消息，就会进入到这种场景。最终的Write会提交到任务队列中后被异步消费 异步模型 异步的概念和同步相对。当一个异步过程调用发出后，调用者不能立刻得到结果。实际处理这个调用的组件在完成后，通过状态、通知和回调来通知调用者。 Netty中的IO操作是异步的，包括Bind、Write、Connect等操作会简单返回一个ChannelFuture。 调用者不能立刻获得结果，而是通过Future-Listener机制，用户可以方便的主动获取或者通过通知机制获取IO结果。 Netty的异步模型是建立在Future和callback之上的。 Netty核心模块组件BootStrap、ServerBootStrap BootStrap意思是引导，一个Netty应用通常由一个BootStrap开始，主要作用是配置整个Netty程序，串联各个组件，Netty中BootStrap类是客户端程序的启动引导类，ServerBootStrap是服务端启动引导类 常见方法有： Future、ChannelFuture Netty中所有的IO操作都是异步的，不能立刻得知消息是否被正确处理。通过Future和ChannelFuture，它们可以注册一个监听，当操作执行成功或失败时监听会自动触发注册的监听事件 常见的方法： Channel Netty通信的组件，能够用于执行网络IO操作。 通过Channel可获取当前网络连接的通道的状态。 通过Channel可获得网络连接的配置参数（例如接收缓冲区大小） Channel提供异步的网络IO操作（如建立连接，读写，绑定端口）。异步调用意味着任何IO调用都将立即返回，并且不保证在调用结束时说请求的IO操作完成。 调用立即返回一个ChannelFuture实例，通过注册监听器到ChannelFuture上，可以IO操作成功、失败或取消时回调通知调用方 支持关联IO操作与对应的处理程序 不同协议、不同的阻塞类型都有不同的Channel类型与之对应，常用的Channel类型： Selector Netty基于Selector对象实现IO多路复用，通过Selector一个线程可以监听多个连接的Channel事件。 当向一个Selector中注册Channel后，Selector内部的机制就可以自动不断的查询（select）这些注册的Channel是否有已就绪的IO事件（例如可读，可写，网络连接完成等），这样程序久可以简单的使用一个线程高效的管理多个Channel ChannelHandler及其实现类 ChannelHandler是一个接口，处理IO事件或拦截IO操作，并将其转发到其ChannelPipeline（业务处理链）中的下一个处理程序。 ChannelHandler本身没有提供很多方法，因为这个接口有许多的方法需要实现，方便使用期间，可以继承它的子类 ChannelHandler及其实现类： ChannelInboundHandler用于处理入站IO事件。 ChannelOutboundHandler用于处理出站IO操作。 ChannelDuplexHandler既可以用于处理入站IO，也可以处理出站IO Pipeline和ChannelPipeline ChannelPipeline是一个Handler的集合，它负责处理和拦截inbound或者outbound的事件和操作，相当于一个贯穿Netty的链。 ChannelPipeline实现了一种高级形式的拦截过滤器模式，使用户可以完全控制事件的处理方式，以及CHannel中各个的ChannelHandler如何交互 在Netty中每个Channel都有且仅有一个ChannelPipeline与之对应，它们的组成关系如下 一个Channel包含了一个ChannelPipeline，而ChannelPipeline中又维护了一个有ChannelHandlerContext组成的双向链表，并且每个ChannelHandlerContext又关联着一个ChannelHandler 入站事件和出站事件在一个双向链表中，入站事件会从链表head往后传递到最后一个入站的handler，出站事件会从链表的tail往前传递到最前一个出站的handler，两种类型的handler互补干扰 ChannelHandlerContext 保存Channel相关的所有上下文信息，同时关联一个ChannelHandler对象 即ChannelHandlerContext中包含一个具体的事件处理器ChannelHandler，同时ChannelHandlerContext中也绑定了对应的pipeline和Channel信息，方便对ChannelHandler进行调用。 常用方法：123ChannelFuture close(); //关闭通道ChannelOutBoundInvoker flush(); //刷新ChannelFuture writeAndFlush(Object msg); //将数据写到ChannelPipeline中当前ChannelHandler的next ChannelOption Netty在创建Channel实例后，一般都需要设置ChannelOption参数。 ChannelOption参数如下： ChannelOption.SO_BACHLOG：对应TCP&#x2F;IP协议listen函数中的backlog参数，用来初始化服务器可连接队列的大小。服务端处理客户端连接请求是顺序处理的，所以同一时间只能处理一个客户端连接。多个客户端来的时候，服务端将不能处理的客户端连接请求放在队列中等待处理，backlog参数指定了队列的大小。 ChannelOption.SO_KEEPALIVE：一直保持连接活动状态。 EventLoopGroup和其实现类NioEventLoopGroup EventLoopGroup是一组EventLoop的抽象，Netty为了更好的利用多核CPU资源，一般会有多个EventLoop同时工作，每个EventLoop维护着一个Selector实例。 EventLoopGroup提供next接口，可以从组里面按照一定规则获取其中一个EventLoop来处理任务。在Netty服务端编程中，我们一般都需要提供两个EventLoopGroup，例如：bossEventLoopGroup，workerEventLoopGroup","categories":[{"name":"后端","slug":"后端","permalink":"https://yangh124.github.io/categories/%E5%90%8E%E7%AB%AF/"}],"tags":[{"name":"网络编程","slug":"网络编程","permalink":"https://yangh124.github.io/tags/%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/"}]},{"title":"ElasticSearch","slug":"ElasticSearch","date":"2021-08-30T16:00:00.000Z","updated":"2024-10-05T15:47:33.612Z","comments":true,"path":"2021/08/31/ElasticSearch/","permalink":"https://yangh124.github.io/2021/08/31/ElasticSearch/","excerpt":"简介Elasticsearch是一个基于Lucene的搜索服务器，他提供了一个分布式全文搜索引擎，基于restful web接口。 Elasticsearch是用Java语言开发的，基于Apache协议的开源项目，是目前最受欢迎的企业搜索引擎。Elasticsearch广泛运用于云计算中，能够达到实时搜索，具有稳定，可靠，快速的特点。","text":"简介Elasticsearch是一个基于Lucene的搜索服务器，他提供了一个分布式全文搜索引擎，基于restful web接口。 Elasticsearch是用Java语言开发的，基于Apache协议的开源项目，是目前最受欢迎的企业搜索引擎。Elasticsearch广泛运用于云计算中，能够达到实时搜索，具有稳定，可靠，快速的特点。 安装Elasticsearch，Kibana，IKAnalyzer中文分词器。版本要一致 相关概念 Never Realation（近实时）：Elasticsearch是一个近乎实时的搜索平台，这意味着从索引文档到可搜索文档之间只有轻微的延迟（通常是一秒钟）。 Cluster（集群）：集群是一个或多个节点的集合，它们一起保存整个数据，并提供所有节点的联合索引和搜索功能。每个集群都有自己的唯一集群名称，节点通过名称加入集群。 Node（节点）：节点是指属于集群的单个Elasticsearch实例，存储数据并参与集群的索引的搜索功能。可以将节点配置为按集群名称加入特定集群，默认情况下，每个节点都设置为加入一个名为elasticsearch的集群。 Index（索引）：索引是一些具有相似特征的文档集合，类似于Mysql中数据库的概念。 Type（类型）：类型是索引的逻辑类别分区，通常，为具有一组公共字段的文档类型，类似Mysql中表的概念。（在ES6及之后的版本，一个索引只能包含一个类型，ES7中标记为过时的，ES8中将移除Type）。 Document（文档）：文档是可被搜索的基本信息单位，以JSON格式表示，类似于Mysql中的行。 Shards（分片）：当索引存储大量数据时，可能会超出当个节点的硬件设置，为了解决这个问题，ES提供了将索引细分为分片的概念。分片机制赋予了索引水平扩容的能力、并允许跨分片分发和并行化操作，从而提高性能和吞吐量。 Replicas（副本）：在可能出现故障的网络的环境中，需要有一个故障切换机制，ES提供了将索引的分片复制为一个或多个副本的功能，副本在某些节点失效的情况下提供高可用性。 集群状态查看 查看集群健康状态1GET /_cat/health?v 查看节点状态1GET /_cat/nodes?v 查看所有索引信息1GET /_cat/indices?v 索引操作 创建索引并查看1PUT /customer GET /_cat/indices?v 删除索引1DELETE /customer 类型操作 查看文档的类型1GET /bank/_mapping 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091&#123; &quot;bank&quot;: &#123; &quot;mappings&quot;: &#123; &quot;account&quot;: &#123; &quot;properties&quot;: &#123; &quot;account_number&quot;: &#123; &quot;type&quot;: &quot;long&quot; &#125;, &quot;address&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;fields&quot;: &#123; &quot;keyword&quot;: &#123; &quot;type&quot;: &quot;keyword&quot;, &quot;ignore_above&quot;: 256 &#125; &#125; &#125;, &quot;age&quot;: &#123; &quot;type&quot;: &quot;long&quot; &#125;, &quot;balance&quot;: &#123; &quot;type&quot;: &quot;long&quot; &#125;, &quot;city&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;fields&quot;: &#123; &quot;keyword&quot;: &#123; &quot;type&quot;: &quot;keyword&quot;, &quot;ignore_above&quot;: 256 &#125; &#125; &#125;, &quot;email&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;fields&quot;: &#123; &quot;keyword&quot;: &#123; &quot;type&quot;: &quot;keyword&quot;, &quot;ignore_above&quot;: 256 &#125; &#125; &#125;, &quot;employer&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;fields&quot;: &#123; &quot;keyword&quot;: &#123; &quot;type&quot;: &quot;keyword&quot;, &quot;ignore_above&quot;: 256 &#125; &#125; &#125;, &quot;firstname&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;fields&quot;: &#123; &quot;keyword&quot;: &#123; &quot;type&quot;: &quot;keyword&quot;, &quot;ignore_above&quot;: 256 &#125; &#125; &#125;, &quot;gender&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;fields&quot;: &#123; &quot;keyword&quot;: &#123; &quot;type&quot;: &quot;keyword&quot;, &quot;ignore_above&quot;: 256 &#125; &#125; &#125;, &quot;lastname&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;fields&quot;: &#123; &quot;keyword&quot;: &#123; &quot;type&quot;: &quot;keyword&quot;, &quot;ignore_above&quot;: 256 &#125; &#125; &#125;, &quot;state&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;fields&quot;: &#123; &quot;keyword&quot;: &#123; &quot;type&quot;: &quot;keyword&quot;, &quot;ignore_above&quot;: 256 &#125; &#125; &#125; &#125; &#125; &#125; &#125;&#125; 文档操作 在索引中添加文档1234PUT /customer/doc/1GET&#123; &quot;name&quot;:&quot;yanghao&quot;&#125; 1234567891011121314&#123; &quot;_index&quot;: &quot;customer&quot;, &quot;_type&quot;: &quot;doc&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_version&quot;: 1, &quot;result&quot;: &quot;created&quot;, &quot;_shards&quot;: &#123; &quot;total&quot;: 2, &quot;successful&quot;: 1, &quot;failed&quot;: 0 &#125;, &quot;_seq_no&quot;: 3, &quot;_primary_term&quot;: 1&#125; 查看索引中的文档1GET /customer/doc/1 12345678910&#123; &quot;_index&quot;: &quot;customer&quot;, &quot;_type&quot;: &quot;doc&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_version&quot;: 2, &quot;found&quot;: true, &quot;_source&quot;: &#123; &quot;name&quot;: &quot;yanghao&quot; &#125;&#125; 修改索引中的文档1234POST /customer/doc/1/_update&#123; &quot;doc&quot;:&#123;&quot;name&quot;:&quot;yanghao dashuaige&quot;&#125;&#125; 1234567891011121314&#123; &quot;_index&quot;: &quot;customer&quot;, &quot;_type&quot;: &quot;doc&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_version&quot;: 2, &quot;result&quot;: &quot;updated&quot;, &quot;_shards&quot;: &#123; &quot;total&quot;: 2, &quot;successful&quot;: 1, &quot;failed&quot;: 0 &#125;, &quot;_seq_no&quot;: 4, &quot;_primary_term&quot;: 1&#125; 删除索引中的文档1DELETE /customer/doc/1 1234567891011121314&#123; &quot;_index&quot;: &quot;customer&quot;, &quot;_type&quot;: &quot;doc&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_version&quot;: 3, &quot;result&quot;: &quot;deleted&quot;, &quot;_shards&quot;: &#123; &quot;total&quot;: 2, &quot;successful&quot;: 1, &quot;failed&quot;: 0 &#125;, &quot;_seq_no&quot;: 2, &quot;_primary_term&quot;: 1&#125; 对索引中的文档执行批量操作12345POST /customer/doc/_bulk&#123;&quot;index&quot;:&#123;&quot;_id&quot;:&quot;1&quot;&#125;&#125;&#123;&quot;name&quot;:&quot;xiaohong&quot;&#125;&#123;&quot;index&quot;:&#123;&quot;_id&quot;:&quot;2&quot;&#125;&#125;&#123;&quot;name&quot;:&quot;xiaoming&quot;&#125; 12345678910111213141516171819202122232425262728293031323334353637383940&#123; &quot;took&quot;: 45, &quot;errors&quot;: false, &quot;items&quot;: [ &#123; &quot;index&quot;: &#123; &quot;_index&quot;: &quot;customer&quot;, &quot;_type&quot;: &quot;doc&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_version&quot;: 3, &quot;result&quot;: &quot;updated&quot;, &quot;_shards&quot;: &#123; &quot;total&quot;: 2, &quot;successful&quot;: 1, &quot;failed&quot;: 0 &#125;, &quot;_seq_no&quot;: 5, &quot;_primary_term&quot;: 1, &quot;status&quot;: 200 &#125; &#125;, &#123; &quot;index&quot;: &#123; &quot;_index&quot;: &quot;customer&quot;, &quot;_type&quot;: &quot;doc&quot;, &quot;_id&quot;: &quot;2&quot;, &quot;_version&quot;: 1, &quot;result&quot;: &quot;created&quot;, &quot;_shards&quot;: &#123; &quot;total&quot;: 2, &quot;successful&quot;: 1, &quot;failed&quot;: 0 &#125;, &quot;_seq_no&quot;: 0, &quot;_primary_term&quot;: 1, &quot;status&quot;: 201 &#125; &#125; ]&#125; 数据搜索查询表达式（Query DSL）是一种非常灵活又富有表现力的查询语言，ES使用它可以以简单的JSON接口来实现丰富的搜索功能。 数据准备 首先导入测试数据，数据结构如下&#x2F;12345678910111213&#123; &quot;account_number&quot;: 0, &quot;balance&quot;: 16623, &quot;firstname&quot;: &quot;Bradshaw&quot;, &quot;lastname&quot;: &quot;Mckenzie&quot;, &quot;age&quot;: 29, &quot;gender&quot;: &quot;F&quot;, &quot;address&quot;: &quot;244 Columbus Place&quot;, &quot;employer&quot;: &quot;Euron&quot;, &quot;email&quot;: &quot;bradshawmckenzie@euron.com&quot;, &quot;city&quot;: &quot;Hobucken&quot;, &quot;state&quot;: &quot;CO&quot;&#125; 然后使用批量操作来导入数据（使用Kibana的Dev Tools操作）1234567891011121314151617181920POST /bank/account/_bulk&#123; &quot;index&quot;: &#123; &quot;_id&quot;: &quot;1&quot; &#125;&#125;&#123; &quot;account_number&quot;: 1, &quot;balance&quot;: 39225, &quot;firstname&quot;: &quot;Amber&quot;, &quot;lastname&quot;: &quot;Duke&quot;, &quot;age&quot;: 32, &quot;gender&quot;: &quot;M&quot;, &quot;address&quot;: &quot;880 Holmes Lane&quot;, &quot;employer&quot;: &quot;Pyrami&quot;, &quot;email&quot;: &quot;amberduke@pyrami.com&quot;, &quot;city&quot;: &quot;Brogan&quot;, &quot;state&quot;: &quot;IL&quot;&#125;......省略若干条数据 导入完成查看索引信息，可以发现bank索引中已经创建了1000条文档 搜索入门 最简单的搜索GET /bank/_search &#123; &quot;query&quot;:&#123; &quot;match_all&quot;:&#123;&#125; &#125; &#125; 分页搜索123456GET /bank/_search&#123; &quot;query&quot;:&#123; &quot;match_all&quot;:&#123;&#125; &#125;, &quot;from&quot;:0, &quot;size&quot;:10&#125; 搜索排序，使用sort12345GET /bank/_search&#123; &quot;query&quot;:&#123; &quot;match_all&quot;:&#123;&#125; &#125;, &quot;sort&quot;:&#123;&quot;balance&quot;:&#123;&quot;order&quot;:&quot;desc&quot;&#125;&#125; #按照balance字段降序&#125; 搜索并返回指定字段内容，使用_source 表示12345GET /bank/_search&#123; &quot;query&quot;:&#123; &quot;match_all&quot;:&#123;&#125; &#125;, &quot;_source&quot;:[&quot;account_number&quot;,&quot;balance&quot;] #只返回account_number，balance字段&#125; 条件搜索 条件搜索，使用match表示匹配条件 12345678GET /bank/_search&#123; &quot;query&quot;:&#123; &quot;match&quot;:&#123; &quot;account_number&quot;:20 &#125; &#125;&#125; 文本类型字段的条件搜索，对比上一条可以发现，对于数字类型的字段进行的是精确匹配，而对于文本使用的是模糊匹配 123456789101112GET /bank/_search&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;address&quot;: &quot;mill&quot; &#125; &#125;, &quot;_source&quot;: [ &quot;address&quot;, &quot;account_number&quot; ]&#125; 短语匹配搜索，使用match_phrase 12345678GET /bank/_search&#123; &quot;query&quot;: &#123; &quot;match_phrase&quot;: &#123; &quot;address&quot;: &quot;mill lane&quot; &#125; &#125;&#125; 组合搜索 组合搜索，使用bool来进行组合，must表示必须同时满足 123456789101112GET /bank/_search&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;must&quot;: [ #同时包含mill lane &#123; &quot;match&quot;: &#123; &quot;address&quot;: &quot;mill&quot; &#125; &#125;, &#123; &quot;match&quot;: &#123; &quot;address&quot;: &quot;lane&quot; &#125; &#125; ] &#125; &#125;&#125; 组合搜索，使用should，表示只要满足其中一个 1234567891011GET /bank/_search&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;should&quot;: [ &#123; &quot;match&quot;: &#123; &quot;address&quot;: &quot;mill&quot; &#125; &#125;, &#123; &quot;match&quot;: &#123; &quot;address&quot;: &quot;lane&quot; &#125; &#125; ] &#125; &#125;&#125; 组合搜索，must_not表示必须同时不满足 1234567891011GET /bank/_search&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;must_not&quot;: [ &#123; &quot;match&quot;: &#123; &quot;address&quot;: &quot;mill&quot; &#125; &#125;, &#123; &quot;match&quot;: &#123; &quot;address&quot;: &quot;lane&quot; &#125; &#125; ] &#125; &#125;&#125; 组合搜索，组合must和must_not 12345678910111213GET /bank/_search&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;must&quot;: [ &#123; &quot;match&quot;: &#123; &quot;age&quot;: &quot;40&quot; &#125; &#125; ], &quot;must_not&quot;: [ &#123; &quot;match&quot;: &#123; &quot;state&quot;: &quot;ID&quot; &#125; &#125; ] &#125; &#125;&#125; 搜索聚合 对搜索结果进行聚合，使用aggs表示，类似于Mysql中的group by ，例如对state字段进行聚合，统计出不同state的文档数量。1234567891011GET /bank/_search&#123; &quot;size&quot;:0, &quot;aggs&quot;:&#123; &quot;group_by_state&quot;:&#123; &quot;terms&quot;:&#123; &quot;field&quot;:&quot;state.keyword&quot; &#125; &#125; &#125;&#125; 嵌套聚合，例如对state进行聚合，统计出不同state的文档数量，再统计出blance的平均值。12345678910111213141516GET /bank/_search&#123; &quot;size&quot;: 0, &quot;aggs&quot;: &#123; &quot;group_by_age&quot;: &#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;state.keyword&quot; &#125; &#125;, &quot;avg_blance&quot;: &#123; &quot;avg&quot;: &#123; &quot;field&quot;: &quot;age&quot; &#125; &#125; &#125;&#125; 对聚合结果进行排序，例如按balance的平均值降序排序123456789101112131415161718192021GET /bank/_search&#123; &quot;size&quot;: 0, &quot;aggs&quot;: &#123; &quot;group_by_state&quot;: &#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;state.keyword&quot;, &quot;order&quot;: &#123; &quot;average_balance&quot;: &quot;desc&quot; &#125; &#125;, &quot;aggs&quot;: &#123; &quot;average_balance&quot;: &#123; &quot;avg&quot;: &#123; &quot;field&quot;: &quot;balance&quot; &#125; &#125; &#125; &#125; &#125;&#125; 按字段的范围进行分段聚合，例如分段范围为age字段的[20,30] [30,40] [40,50]，之后按gender统计文档个数和balance的平均值。123456789101112131415161718192021222324252627282930313233343536373839GET /bank/_search&#123; &quot;size&quot;: 0, &quot;aggs&quot;: &#123; &quot;group_by_age&quot;: &#123; &quot;range&quot;: &#123; &quot;field&quot;: &quot;age&quot;, &quot;ranges&quot;: [ &#123; &quot;from&quot;: 20, &quot;to&quot;: 30 &#125;, &#123; &quot;from&quot;: 30, &quot;to&quot;: 40 &#125;, &#123; &quot;from&quot;: 40, &quot;to&quot;: 50 &#125; ] &#125;, &quot;aggs&quot;: &#123; &quot;group_by_gender&quot;: &#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;gender.keyword&quot; &#125;, &quot;aggs&quot;: &#123; &quot;average_balance&quot;: &#123; &quot;avg&quot;: &#123; &quot;field&quot;: &quot;balance&quot; &#125; &#125; &#125; &#125; &#125; &#125; &#125;&#125; Spring Boot整合ElasticSearch使用ElasticSearch-Rest-Client对elasticsearch进行操作 引入依赖： 12345&lt;dependency&gt; &lt;groupId&gt;org.elasticsearch.client&lt;/groupId&gt; &lt;artifactId&gt;elasticsearch-rest-high-level-client&lt;/artifactId&gt; &lt;version&gt;7.9.0&lt;/version&gt;&lt;/dependency&gt; 因为SpringBoot管理了elasticsearch的依赖版本，所以我们需要指定一下elasticsearch的版本与其一致： 123&lt;properties&gt; &lt;elasticsearch.version&gt;7.9.0&lt;/elasticsearch.version&gt;&lt;/properties&gt; 然后编写一个配置类，向容器中注册一个操作elasticsearch的组件： 1234567891011121314151617181920212223242526272829303132333435363738/** * @author : yh * @date : 2021/8/30 20:35 */@Component@Getter@Setter@ConfigurationProperties(prefix = &quot;elasticsearch&quot;)public class EsHostConfig &#123; private String[] hostArr;&#125;/** * 定义RestClient Bean * * @author : yh * @date : 2021/8/30 20:32 */@Configurationpublic class EsRestClientConfig &#123; @Autowired private EsHostConfig esHostConfig; @Bean public RestHighLevelClient restHighLevelClient() &#123; String[] hostArr = esHostConfig.getHostArr(); int size = hostArr.length; HttpHost[] httpHostArr = new HttpHost[size]; for (int i = 0; i &lt; size; i++) &#123; String[] split = hostArr[i].split(&quot;:&quot;); httpHostArr[i] = new HttpHost(split[0], Integer.parseInt(split[1]), &quot;http&quot;); &#125; return new RestHighLevelClient(RestClient.builder(httpHostArr)); &#125;&#125; 接下来我们就可以通过它操作 elasticsearch 了： 12345678910111213141516171819202122232425@Autowiredprivate RestHighLevelClient client;@Dataclass User &#123; private String name; private Integer age; private String gender;&#125;@Testpublic void index() throws IOException &#123; IndexRequest indexRequest = new IndexRequest(&quot;users&quot;); indexRequest.id(&quot;1&quot;); // indexRequest.source(&quot;name&quot;,&quot;zhangsan&quot;,&quot;age&quot;,20,&quot;gender&quot;,&quot;男&quot;); User user = new User(); user.setName(&quot;zhangsan&quot;); user.setAge(20); user.setGender(&quot;男&quot;); String json = JSON.toJSONString(user); indexRequest.source(json, XContentType.JSON); // 执行保存操作 IndexResponse index = client.index(indexRequest, RequestOptions.DEFAULT); // 响应数据 System.out.println(index);&#125; RestHighLevelClient提供了非常多的方式用于保存数据，但比较常用的是通过json数据直接保存，首先需要指定索引， IndexRequest indexRequest &#x3D; new IndexRequest(&quot;users&quot;); 指定了users索引，然后指定数据id，接着指定数据值，最后使用client执行保存操作，然后可以拿到响应数据。 elasticsearch的其它简单操作，诸如：更新、删除等，都只需要转换一下调用方法即可，如更新操作，就需要使用client调用update方法，接下来我们看看Java程序该如何实现较为复杂的检索操作。 比如现在想聚合出年龄的分布情况，并求出每个年龄分布人群的平均薪资，就应该这样进行编写： 1234567891011121314151617181920212223242526272829303132@Test public void aggSearch() throws Exception &#123; //创建搜索request SearchRequest searchRequest = new SearchRequest(&quot;bank&quot;); SearchSourceBuilder searchSourceBuilder = new SearchSourceBuilder(); //查询条件 searchSourceBuilder.query(QueryBuilders.matchAllQuery()); //统计各个年龄的平均工资 searchSourceBuilder.aggregation(AggregationBuilders.terms(&quot;ageAgg&quot;).field(&quot;age&quot;).subAggregation(AggregationBuilders.avg(&quot;balanceAvgAgg&quot;).field(&quot;balance&quot;))); //所有人平均工资 searchSourceBuilder.aggregation(AggregationBuilders.avg(&quot;allBalanceAvgAgg&quot;).field(&quot;balance&quot;)); //构建request SearchRequest request = searchRequest.source(searchSourceBuilder); //search SearchResponse searchResponse = restHighLevelClient.search(request, RequestOptions.DEFAULT); //Aggregations aggregations = searchResponse.getAggregations(); //Aggregation接口有很多实现类 Avg（平均值）// Avg avg = aggregations.get(&quot;balanceAgg&quot;);// System.out.println(avg.getValue()); Aggregations aggregations = searchResponse.getAggregations(); Terms ageAgg = aggregations.get(&quot;ageAgg&quot;); List&lt;? extends Terms.Bucket&gt; buckets = ageAgg.getBuckets(); for (Terms.Bucket bucket : buckets) &#123;//只查出了10条数据 System.out.println(&quot;年龄:&quot; + bucket.getKey()); System.out.println(&quot;人数:&quot; + bucket.getDocCount()); Avg avg = bucket.getAggregations().get(&quot;balanceAvgAgg&quot;); System.out.println(&quot;平均工资:&quot; + avg.getValue()); System.out.println(&quot;=================================&quot;); &#125; Avg avg = aggregations.get(&quot;allBalanceAvgAgg&quot;); System.out.println(&quot;所有人平均工资:&quot; + avg.getValue()); &#125; 最终输出结果123456789101112131415161718192021222324252627282930313233343536373839404142年龄:31人数:61平均工资:28312.918032786885=================================年龄:39人数:60平均工资:25269.583333333332=================================年龄:26人数:59平均工资:23194.813559322032=================================年龄:32人数:52平均工资:23951.346153846152=================================年龄:35人数:52平均工资:22136.69230769231=================================年龄:36人数:52平均工资:22174.71153846154=================================年龄:22人数:51平均工资:24731.07843137255=================================年龄:28人数:51平均工资:28273.882352941175=================================年龄:33人数:50平均工资:25093.94=================================年龄:34人数:49平均工资:26809.95918367347=================================所有人平均工资:25714.837","categories":[{"name":"后端","slug":"后端","permalink":"https://yangh124.github.io/categories/%E5%90%8E%E7%AB%AF/"}],"tags":[{"name":"elastic search","slug":"elastic-search","permalink":"https://yangh124.github.io/tags/elastic-search/"}]},{"title":"Dubbo","slug":"Dubbo","date":"2021-08-02T16:00:00.000Z","updated":"2024-10-05T15:47:33.612Z","comments":true,"path":"2021/08/03/Dubbo/","permalink":"https://yangh124.github.io/2021/08/03/Dubbo/","excerpt":"","text":"基础知识分布式基础理论什么是分布式系统《分布式系统原理和与范例》定义：分布式系统是若干个独立计算机的集合，这些计算机对于用户来说就像单个相关系统。分布式系统（distributed system）是建立在网络之上的软件系统。 随着互联网的发展，网络应用的规模不断扩大，常规的垂直架构已无法应对，分布式服务架构以及流动计算机架构势在必行，急需一个治理系统确保架构有条不紊的演进。 发展演变 RPC什么是RPCRPC（remote procedure call）是指远程过程调用，是一种进程间通信方式，它是一种技术的思想，而不是规范。它允许程序调用另一个地址空间（通常是共享网络的另一台机器上）的过程或函数，而不用显示的编码这个远程调用的细节。即程序员无论是调用本地的还是远程的函数，本质上编写的代码基本相同。 一次完整的RPC调用流程（同步调用，异步另说）如下： 1. 服务消费者（client）调用以本地调用方式调用服务； 2. client stub接收到调用后负责将方法、参数等组装成能够进行网络传输的消息体； 3. client stub找到消息服务地址，并将消息发送到服务端； 4. server stub找到消息后进行解码； 5. server stub根据解码结果调用本地服务； 6. 本地服务执行并将结果返回给server stub； 7. server stub将返回结果打包成消息并发送至消费方； 8. client stub接收消息，并进行解码； 9. 服务消费方得到最终结果。RPC框架的目的就是将2～8这些步骤都封装起来，这些细节对用户来说是不可见的。 netty通信原理Netty是一个异步事件驱动的网络应用程序框架。用于快速开发可维护的高性能协议服务器和客户端。它极大的简化了TCP和UDP套接字服务器等网络编程。 BIO NIO（Non-Blocking IO） Selector一般称为选择器，也可以翻译为多路复用器； Connect（连接就绪）、Accept（接受就绪）、Read（读就绪）、Write（写就绪） Netty基本原理Dubbo原理框架设计 启动解析、加载配置服务暴露服务引用服务调用","categories":[{"name":"后端","slug":"后端","permalink":"https://yangh124.github.io/categories/%E5%90%8E%E7%AB%AF/"}],"tags":[{"name":"微服务","slug":"微服务","permalink":"https://yangh124.github.io/tags/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"}]},{"title":"Docker常用命令","slug":"Docker常用命令","date":"2021-07-30T16:00:00.000Z","updated":"2024-10-05T15:47:33.611Z","comments":true,"path":"2021/07/31/Docker常用命令/","permalink":"https://yangh124.github.io/2021/07/31/Docker%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/","excerpt":"Docker简介Docker是一个开源的应用容器引擎，让开发者可以打包应用及依赖包到一个可移植的镜像中，然后发布到任何流行的Linux或Windows机器上。使用Docker可以更方便地打包、测试以及部署应用程序。","text":"Docker简介Docker是一个开源的应用容器引擎，让开发者可以打包应用及依赖包到一个可移植的镜像中，然后发布到任何流行的Linux或Windows机器上。使用Docker可以更方便地打包、测试以及部署应用程序。 Docker环境安装（Linux）1. 安装yum-utils；yum install -y yum-utils device-mapper-persistent-data lvm2 2. 为yum源添加docker仓库位置；yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo 3. 安装dokcer服务；yum install docker-ce 4. 启动docker服务；systemctl start docker Docker常用镜像命令搜索镜像docker search xxx 下载镜像dockr pull xxx 查看镜像版本由于docker search命令只能查找出是否有该镜像，不能找到该镜像支持的版本，所以我们需要通过Docker Hub来搜索支持的版本。 进入Docker Hub的官网，地址：[https://hub.docker.com](https://hub.docker.com/) 列出镜像docker images 删除镜像1. 指定名称删除镜像docker rmi java:8.0 2. 指定名称强制删除镜像docker rmi -f java:8.0 3. 删除所有没有引用的镜像docker rmi `docker images | grep none | awk &#39;&#123;print $3&#125;&#39;` 4. 强制删除所有镜像docker rmi -f $(docker images) 构建镜像-t 表示指定镜像仓库名称/镜像名称:镜像标签 .表示使用当前目录下的Dockerfile文件 docker build -t yh124/test-porject:1.0-SNAPSHOT . Docker容器常用命令新建并启动容器docker run -p 80:80 --name nginx \\ -e TZ=&quot;Asia/Shanghai&quot; \\ -v /mydata/nginx/html:/usr/share/nginx/html \\ -d nginx:1.17.0 1. -p：将宿主机和容器端口进行映射，格式为：宿主机端口:容器端口。 2. --name：自定义容器名称，之后可以通过容器名称进行操作。 3. -e：设置容器的环境变量，这里设置的时区。 4. -v：将宿主机的文件挂载到容器中，格式为：宿主机文件目录:容器文件目录。 5. -d：表示容器以后台方式运行 列出容器1. 列出运行中的所有容器docker ps 2. 列出所有容器docker ps -a 停止容器docker stop \\$containerName(or \\$containerID) 强制停止容器docker kill xxx 启动容器docker start xxx 进入容器1. 先查出容器piddocker inspect --format &quot;&#123;&#123;.State.Pid&#125;&#125;&quot; $ContainerName 2. 根据容器pid进入容器nsenter --target &quot;$pid&quot; --mount --uts --ipc --net --pid 删除容器1. 删除指定容器docker rm $ContainerName 2. 按名称通配符删除容器，比如删除以名称test-开头的容器docker rm &#39;docker ps -a | grep test-* | awk &#123;print $1&#125;&#39; 3. 强制删除所有容器docker rm -f $(docker ps -a -q) 查看容器日志1. 查看容器产生的全部日志docker logs $ContainerName 2. 动态查看日志docker logs -f -t --tail=100 $ContainerName 查看容器的IP地址docker inspect --format &#39;&#123;&#123;.NetworkSettings.IPAddress&#125;&#125;&#39; $ContainerName 修改容器的启动方式1. 将容器启动方式改为always docker container update --restart=always $ContainerName 2. 关闭自动重启 docker container update --restart=no $ContainerName 同步宿主机时间到容器docker cp /etc/localtime $ContainerName:/etc 指定容器时区docker run -p 80:80 --name nginx \\ -e TZ=&quot;Asia/Shanghai&quot; \\ -d nginx:1.17.0 查看容器资源占用状况1. 查看指定容器资源占用状况，比如cpu，网络，内存，IO状态docker stats $ContainerName 2. 查看所有容器医院占用情况docker stats -a 查看磁盘使用情况docker system df 执行容器内部命令（进入容器）docker exec -it $ContainerName /bin/bash 指定账号进入容器内部使用root账号进入容器内部 docker exec -it --user root $ContainerName /bin/bash 查看所有网络docker network ls 创建外部网络docker network create -d bridge my-bridge-network 指定容器网络docker run -p 80:80 --name nginx \\ --network my-bridge-network \\ -d nginx:1.17.0 修改镜像的存放位置1. 查看镜像的存放位置docker info | grep &quot;Docker Root Dir&quot; 2. 关闭docker 服务systemctl stop docker 3. 先将原镜像目录移动到目标目录mv /var/lib/docker /mydata/docker 4. 建立软连接ln -s /mydata/docker /var/lib/docker Docker容器清理1. 查看Docker占用磁盘空间docker system df 2. 删除所有关闭的容器docker ps -a | grep Exit | cut -d &#39; &#39; -f 1 | xargs docker rm 3. 删除所有dangling镜像（没有Tag的镜像）docker rmi $(docker images | grep &quot;^&lt;none&gt;&quot; | awk &quot;&#123;print $3&#125;&quot;) 4. 删除所有dangling 数据卷（即无用的volume）docker volume rm $(docker volume ls -qf dangling=true) Docker Compose常用命令构建、创建、启动相关容器docker-compose up -d #指定yaml文件 -f 指定文件(file) -d 后台运行 docker-compose -f docker-compose.yml up -d 停止所有相关容器docker-compose stop 列出所有容器信息docker-compose ps","categories":[{"name":"运维","slug":"运维","permalink":"https://yangh124.github.io/categories/%E8%BF%90%E7%BB%B4/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"https://yangh124.github.io/tags/Docker/"}]},{"title":"MySQL","slug":"MySQL","date":"2021-07-30T16:00:00.000Z","updated":"2024-10-05T15:47:33.615Z","comments":true,"path":"2021/07/31/MySQL/","permalink":"https://yangh124.github.io/2021/07/31/MySQL/","excerpt":"事务概念一个数据库事务通常包含对数据库进行读或写的一个操作序列。它的存在包含有以下两个目的： 1. 为数据库操作提供了一个从失败中恢复到正常状态的方法，同时提供了数据库即使在异常状态下仍能保持一致性的方法。 2. 当多个应用程序在并发访问数据库时，可以在这些应用程序之间提供一个隔离方法，以防止彼此的操作互相干扰。","text":"事务概念一个数据库事务通常包含对数据库进行读或写的一个操作序列。它的存在包含有以下两个目的： 1. 为数据库操作提供了一个从失败中恢复到正常状态的方法，同时提供了数据库即使在异常状态下仍能保持一致性的方法。 2. 当多个应用程序在并发访问数据库时，可以在这些应用程序之间提供一个隔离方法，以防止彼此的操作互相干扰。 特性 原子性（Atomicity）：事务作为一个整体被执行，包含在其中的对数据库的操作要么全部被执行，要么都不执行。 一致性（Consistency）：事务应确保数据库的状态从一个一致状态转变为另一个一致状态。一致状态的含义是数据库中的数据应满足完整性约束。 隔离性（Isolation）：多个事务并发执行时，一个事务的执行不应影响其他事务的执行。 持久性（Durability）：一个事务一旦提交，他对数据库的修改应该永久保存在数据库中。 数据库的读现象浅析&quot;读现象&quot;是多个事务并发执行时，在读取数据方面可能碰到的状况。 脏读脏读又称无效数据的读出，是指在数据库访问中，事务T1将某一值修改，然后事务T2读取该值，此后T1因为某种原因撤销对该值的修改，这就导致了T2所读取到的数据是无效的。 事务一读取到事务二未提交数据。 不可重复读在一个事务内，多次读同一个数据。在这个事务还没有结束时，另一个事务也访问该同一数据。那么，在第一个事务的两次读数据之间。由于第二个事务的修改，那么第一个事务读到的数据可能不一样，这样就发生了在一个事务内两次读到的数据是不一样的，因此称为不可重复读，即原始读取不可重复。 在事务二提交之前，事务一不可重复读。 幻读第一个事务对一个表中的数据进行了修改，比如这种修改涉及到表中的”全部数据行”。同时，第二个事务也修改这个表中的数据，这种修改是向表中插入”一行新数据”。那么，以后就会发生操作第一个事务的用户发现表中还有没有修改的数据行，就好象发生了幻觉一样.一般解决幻读的方法是增加范围锁RangeS，锁定检锁范围为只读，这样就避免了幻读。 返回数据变多，幻读。 “幻读(phantom read)”是不可重复读(Non-repeatablereads)的一种特殊场景：当事务没有获取范围锁的情况下执行SELECT ... WHERE操作可能会发生”幻影读(phantom read)”。 Mysql中的行级锁、表级锁、页级锁行级锁行级锁是Mysql中锁粒度最细的锁，表示只针对当前操作的行为加锁。行级锁能大大减少数据库操作的冲突。其加锁粒度最小，但加锁的开销也最大。行级锁分为共享锁和排他锁。 特点：开销大，加锁慢；会出现死锁；锁粒度最小，发生锁冲突的概率最低，并发度也最高。 Innodb默认使用行级锁 读锁（read lock）：也叫共享锁（shared lock），允许一个事务去读一行，阻止其他事务获得相同数据集的排他锁 写锁（write lock）：也叫排他锁（exclusive lock），允许获得排他锁的事务更新数据，阻止其他事务取得相同数据集的共享锁和排他锁 行锁的实现算法 Record Lock 锁 单个行记录上的锁RecordLock总是会去锁住索引记录，如果InnoDB存储引擎表建立的时候没有设置任何一个索引，这时InnoDB存储引擎会使用隐式的主键来进行锁定 Gap Lock 锁 当我们用范围条件而不是相等条件检索数据，并请求共享或排他锁时，InnoDB会给符合条件的已有数据记录的索引加锁，对于键值在条件范围内但并不存在的记录。 优点：解决了事务并发的幻读问题不足：因为query执行过程中通过范围查找的话，他会锁定争个范围内所有的索引键值，即使这个键值并不存在。 间隙锁有一个致命的弱点，就是当锁定一个范围键值之后，即使某些不存在的键值也会被无辜的锁定，而造成锁定的时候无法插入锁定键值范围内任何数据。在某些场景下这可能会对性能造成很大的危害。 Next-key Lock 锁 同时锁住数据+间隙锁(1+2),在Repeatable Read隔离级别下，Next-key Lock 算法是默认的行记录锁定算法。 表级锁表级锁是MySQL中锁定粒度最大的一种锁，表示对当前操作的整张表加锁，它实现简单，资源消耗较少，被大部分MySQL引擎支持。最常使用的MYISAM与INNODB都支持表级锁定。表级锁定分为表共享读锁（共享锁）与表独占写锁（排他锁）。 特点：开销小，加锁快；不会出现死锁；锁定粒度大，发出锁冲突的概率最高，并发度最低。 MyISAM默认使用表级锁 读锁（read lock），也叫共享锁（shared lock）针对同一份数据，多个读操作可以同时进行而不会互相影响（select） 写锁（write lock），也叫排他锁（exclusive lock）当前操作没完成之前，会阻塞其它读和写操作（update、insert、delete） 意向共享锁（IS）：一个事务给一个数据行加共享锁时，必须先获得表的IS锁 意向排它锁（IX）：一个事务给一个数据行加排他锁时，必须先获得该表的IX锁 读锁会阻塞写，但不会阻塞读。而写锁则会把读和写都阻塞。 表锁分析：查看哪些表被锁了：show open tables; 分析表锁定：show status like &#39;table%&#39;; 页锁开销和加锁时间介于行锁和表锁之间，会出现死锁，锁定粒度介于行锁和表锁之间，并发度一般。 Innodb中的行锁与表锁InnoDB行锁是通过给索引上的索引项加锁来实现的，所以只有通过索引条件检索数据，InnoDB才使用行级锁，否则，InnoDB将使用表锁！(索引失效，行锁变表锁) 当两个事务同时执行，一个锁住了主键索引在等待其他相关索引，一个锁定了非主键索引，在等待主键索引。这样就会发生死锁。 发生死锁后，InnoDB一般都可以检测到，并使一个事务释放锁回退，另一个获取锁完成事务。 有多种方法可以避免死锁，这里只介绍常见的三种 1. 如果不同程序会并发存取多个表，尽量约定以相同的顺序访问表，可以大大降低死锁机会。 2. 在同一个事务中，尽可能做到一次锁定所需要的所有资源，减少死锁产生概率； 3. 对于非常容易产生死锁的业务部分，可以尝试使用升级锁定颗粒度，通过表级锁定来减少死锁产生的概率； MySQL中的共享锁与排他锁共享锁(Share Lock)共享锁又称为读锁，是读取操作创建的锁。其他用户可以并发读取数据，但任何事物都不能对数据进行修改，直到已释放所有共享锁。 显式加锁 锁行：SELECT ... LOCK IN SHARE MODE; 锁表：LOCK TABLE XXX READ;(解锁UNLOCK TABLES) 排他锁（Exclusive Lock）排他锁又称写锁，如果事务T对数据A加上排他锁后，则其他事务不能再对A加任任何类型的读写。获准排他锁的事务既能读数据，又能修改数据。 显式加锁 锁行：SELECT ... FOR UPDATE; 锁表：LOCK TABLE XXX WRITE;(解锁UNLOCK TABLES) InnoDB的间隙锁： 当我们用范围条件而不是相等条件检索数据，并请求共享或排他锁时，InnoDB会给符合条件的已有数据记录的索引项加锁；对于键值在条件范围内但并不存在的记录，叫做”间隙（GAP)”，InnoDB也会对这个”间隙”加锁，这种锁机制就是所谓的间隙锁（Next-Key锁）。 深入理解乐观锁与悲观锁乐观并发控制(乐观锁)和悲观并发控制（悲观锁）是并发控制主要采用的技术手段。 悲观锁通过开启排他锁的方式实现了悲观锁 乐观锁实现数据版本有两种方式，第一种是使用版本号，第二种是使用时间戳。 深入分析事务的隔离级别未提交读(Read uncommitted)读未提交是最低级的隔离级别。在这种事物隔离级别下，一个事务可以读到另一个事务未提交数据。 未提交读会导致脏读未提交读的数据库锁情况（实现原理） 事务在读数据的时候并未对数据加锁。事务在修改数据的时候只对数据增加行级共享锁。 提交读(Read committed)在一个事务修改数据过程中，如果事务还没提交，其他事务不能读该数据。 提交读不能解决不可重复读的读现象（重复读数据会变化） 提交读的数据库锁情况 事务对当前被读取的数据加行级共享锁（当读到时才加锁），一旦读完该行，立即释放该行级共享锁；事务在更新某数据的瞬间（就是发生更新的瞬间），必须先对其加行级排他锁，直到事务结束才释放。 可重复读(Repeatable reads)这种隔离级别就叫可重复读（mysql默认隔离级别） 可重复读不能解决幻读 MySQL如何解决幻读？SERIALIZABLE 串行化 MVCC + Next-Key Lock 可重复读的数据库锁情况事务在读取某数据的瞬间（就是开始读取的瞬间），必须先对其加行级共享锁，直到事务结束才释放；事务在更新某数据的瞬间（就是发生更新的瞬间），必须先对其加行级排他锁，直到事务结束才释放。 可序列化(Serializable)可序列化(Serializable)是最高的隔离级别，前面提到的所有的隔离级别都无法解决的幻读，在可序列化的隔离级别中可以解决。 可序列化的数据库锁情况事务在读取数据时，必须先对其加 表级共享锁，直到事务结束才释放； 事务在更新数据时，必须先对其加 表级排他锁，直到事务结束才释放。 MySQL性能优化 单条SQL运行慢创建并正确使用索引数据拆分 垂直拆分：常用字段、不常用字段拆分 水平拆分：一张表的数据拆分成多张表存放 部分SQL运行慢慢查询分析：开启慢查询日志，分析日志进行优化 查询是否开启慢查询日志 SHOW VARIABLES LIKE &quot;%slow_query_log%&quot;; 开启慢查询日志(重启失效) SET GLOBAL slow_query_log&#x3D;1; 查询慢查询时间阈值 show VARIABLES LIKE &quot;%long_query_time%&quot;; 设置慢查询时间阈值(重启失效，需要重新开启一个会话才生效) SET GLOBAL long_query_time&#x3D;3; 慢查询次数 show GLOBAL STATUS LIKE &#39;%Slow_queries%&#39;; 使用mysqldumpslow做日志分析： s：是表示按照何种顺序排序 c：访问次数 l：锁定时间 r：返回记录 t：查询时间 al：平均锁定时间 ar：平均返回记录数 at：平均查询时间 t：即为返回前面多少条数据 g：后边搭配正则表达式，大小写不敏感 得到返回记录集最多的10个SQL mysqldumpslow -s r -t 10 &#x2F;var&#x2F;lib&#x2F;mysql&#x2F;xxx-slow.log 得到访问次数最多的10个SQL mysqldumpslow -s c -t 10 &#x2F;var&#x2F;lib&#x2F;mysql&#x2F;xxx-slow.log 得到按照时间排序的前10条里面含有左连接的查询语句 mysqldumpslow -s t -t 10 -g &quot;left join&quot; &#x2F;var&#x2F;lib&#x2F;mysql&#x2F;xxx-slow.log 另外建议在使用这些命令时结合 | more使用，避免有太多数据 mysqldumpslow -s r -t 10 &#x2F;var&#x2F;lib&#x2F;mysql&#x2F;xxx-slow.log | more 使用show profile分析SQL： 查询是否开启 SHOW VARIABLES LIKE &#39;profiling&#39;; #开启 SET profiling&#x3D;on; 查询最近执行sql语句(查询) show PROFILES; #查询某一条sql执行过程 show PROFILE cpu,block io for query 26; 全局日志查询：开启全局日志 SET GLOBAL general_log=1; 记录在表中 SET GLOBAL log_output=\\&#39;TABLE\\&#39;; 查询日志 SELECT \\* FROM mysql.general_log; 整个SQL运行慢读写分离： 应用层解决方案：通过应用层对数据源做路由来实现读写分离。 中间件解决方案：通过 MySQL 的中间件做主从集群。 EXPLAIN命令： id：相同按顺序执行，不同id越大优先级越高，越先执行 select_type: SIMPLE：简单的select查询，查询中不包含子查询或者union PRIMARY：查询中若包含任何复杂的子部分，最外层查询则被标记为PRIMARY SUBQUERY：在select或where列表中包含了子查询 DERIVED：在from列表中包含子查询被标记为derived（衍生），MySQL会递归执行这些子查询，把结果放在临时表中 UNION：使用了UNION，第二个select则被标记为UNION，若UNION包含在FROM子查询中，外层SELECT将被标记为：DERIVED UNION RESULT：使用UNION获取结果的查询 table：表名 partitions：表分区 type：表示查询类型，从最好到最差：system&gt;const&gt;eq_ref&gt;ref&gt;range&gt;index&gt;ALL system：表示只有一行记录（等于系统表），这是const类型的特例，平时不会出现，这个可以忽略不计 const：表示通过索引一次就找到了，const出现在primary key或者unique索引，因为只匹配一条数据，所以很快，如将主键置于WHERE条件中查询，MYSQL就能将改查询转换为一个常量 eq_ref：唯一性索引扫描，常见于主键或唯一索引扫描 ref：非唯一性索引扫描，返回匹配某个单独值的所有行 range：检索指定范围的行，使用一个索引来选择行。key列显示使用了哪个索引，一般出现在where中使用between,&lt;,&gt;,in等范围查询 index：全索引扫描，index和all的区别为index类型只遍历索引树。这通常比all快，因为索引文件通常比数据文件小。 all：全表扫描，性能最差 possible_key：可能使用的索引 key：实际使用的索引，查询中若使用了覆盖索引，则该索引仅出现在key列表中。（覆盖索引：查询的是索引字段） key_len：表示索引中使用的字节数，长度越短越好 ref：显示索引的哪一列被使用了，列与索引的比较 rows：根据表统计信息及索引选用情况，大致估算出找到所需的记录所需要读取的行数 Extra：额外信息 Using filesort：说明mysql会对数据使用一个外部的索引排序，而不是按照表内的索引顺序进行读取。MYSQL中无法利用索引完成的排序操作称为”文件排序”。 **Using temporary：使用了临时表保存中间结果，MySQL在对查询结果排序时使用临时表。常见于排序order by和分组查询group by** * **Using Index：表示相应的select操作中使用了覆盖索引（Covering Index），避免访问了表的数据行，效率不错,如果同时出现了using where，表明索引被用来执行索引键值查找。如果没有同时出现using where，表明索引用来读取数据而非执行查找动作。** * **Using where：使用where子句** * **Using join buffer： 使用了连接缓存** * **Using impossible where：表示where子句总是false，查询不到任何数据** * **select tables optimized away：在没有group by子句的情况下基于索引优化MIN/MAX；对于MYISAM储存引擎优化COUNT(\\*)操作，不必等到执行阶段再计算，查询执行计划生成的阶段即完成优化。** * **distinct：优化distinct操作，在找到第一匹配的元组后即停止查找同样值的动作** 高频面试题什么是索引索引是一种数据结构，可以帮助我们快速进行数据查找。 索引是个什么样的数据结构索引的数据结构和具有储存引擎的实现有关，在MySQL中使用较多的索引有Hash索引，B+树索引等，而我们经常使用的InnoDB存储引擎的默认索引实现为：B+树索引。 Hash索引和B+树索引有什么区别或者说优劣hash索引底层就是hash表，进行查找时，调用一次hash函数就可以获取到相应的键值，之后进行回表查询获得实际数据； B+的底层现实是多路平衡查找树，对于每一次查询都是从根节点出发，查找叶子节点方可以获取的所查键值，然后根据查询判断是否需要回表查询数据。 不同：hash索引进行等值查询(where xx&#x3D;xx)更快（一般情况下），但是却无法进行范围查询(模糊查询...)。 因为在hash索引中经过hash函数建立索引之后，索引的顺序与原顺序无法保持一致，不能支持范围查询，而B+树的所有节点皆遵循（左节点小于父节点，右节点大于父节点，多叉树也类似），天然支持范围查询。 hash索引不支持使用索引排序，原理同上。 hash索引不支持模糊查询以及多列索引的最左前缀原则，原理也是因为hash函数的不可预测AAAA和AAAAB的索引没有相关性。 hash索引任何时候都避免不了回表查询数据，而B+树索引在符合某些条件下(聚簇索引，覆盖索引等)的时候可以只通过索引完成查询。 hash索引在等值查询上较快，但是不稳定，性能不可预测，当某个键值存在大量重复的时候，发生hash碰撞，此时效率可能极差，而B+树的查询比较稳定，对于所有查询都是从根节点到叶子节点，而树的高度较低。 因此，在大多数情况下，直接使用B+树索引可以获得稳定且较好的查询速度，而不需要使用hash索引。 什么是聚簇索引在B+树的索引中，叶子节点可能存储了当前的key值，也可能存储了当前的key值以及整行数据，这就是聚簇索引和非聚簇索引。 在InnoDB中，只有主键索引是聚簇索引，如果没有主键，则挑选一个唯一键建立聚簇索引，如果没有唯一键，则隐式的生成一个键来建立聚簇索引。 当查询使用聚簇索引时，在对应的叶子节点，可以获取整行数据，因此不用再次进行回表查询。 非聚簇索引一定要回表查询？不一定，这涉及到查询语句所要求的字段是否全部命中了索引，那么就不必再进行回表查询。 select age from employee where age &lt;20;在age字段建立索引，查询age，无需回表查询，在索引的叶子节点上，已经包含了age信息。 在建立索引的时候，都需要考虑哪些因素字段使用的频率。经常作为条件进行查询的字段比较合适，如果需要建立联合索引的话，还需要考虑联合索引的顺序。 联合索引是什么？为什么需要注意联合索引中的顺序？mysql可以使用多个字段同时建立一个索引，叫做联合索引，在联合索引中，如果想要命中索引，需要按照建立索引时的顺序挨个使用，否则无法命中索引。 怎么知道索引是否被使用？或者说怎么才能知道sql执行慢的原因？explain查看执行计划。 什么时候索引会失效 使用不等号查询 列参与了数学运算或者函数 like &#39;%xxx&#39; 当mysql分析全表扫描比使用索引快的时候 当使⽤联合索引,前⾯⼀个条件为范围查询,后⾯的即使符合最左前缀原则,也⽆法使⽤索引 索引有什么劣势 实际上索引也是张表，该表保存了主键和索引字段，并指向实体表记录，所以索引也是要占用空间的。 虽然索引大大的提高查询速度，如对表进行INSERT、UPDATE和DELETE。因为更新表时，MYSQL不仅要保存数据，还要保存一下索引文件每次更新添加了索引类的字段，都会调整因为更新所带来的键值变化后的索引信息。 表结构设计尽量设置一个主键 主键使用自增ID 字段不要定义为null 存储密码使用char而不是使用varchar 存储引擎相关MySQL支持哪些存储引擎InnoDB、MyISAM等等。 InnoDB和MyISAM的区别 InnoDB支持事务MyISAM不支持。 InnoDB支持行级锁，而MyISAM只支持表级锁 InnoDB支持MVCC，MyISAM不支持 InnoDB支持外键，MyISAM不支持 InnoDB不支持全文索引，而MyISAM支持 零散问题MySQL中varchar和char的区别char是一个定长字段，假如申请了char(10)的空间，那么无论实际存储多少内容，该字段都占用10个字符； 而varchar是变长的，也就是说申请的只是最大长度，占用空间为实际字符长度+1，最后一个字符存储使用了多长空间 在检索效率上，char&gt;varchar，所以如果确定某个字段是定长的，可以使用char，否则尽量使用varchar。例如MD5密码，则应该使用char。 varchar(10)和int(10)的区别varchar的10代表了申请的空间⻓度,也是可以存储的数据的最⼤⻓度,⽽int的10只是代表了展⽰的⻓度,不⾜10位以0填充。 也就是说,int(1)和int(10)所能存储的数字⼤⼩以及占⽤的空间都是相同的,只是在展⽰时按照⻓度展⽰。 MySQL的binlog有几种录入格式？1）statement： 2）row： 3）mixed： 超大分页怎么处理？select * from table where age &gt; 20 limit 1000000,10 select * from table where id in (select id from table where age &gt; 20limit 1000000,10) 同时如果ID连续的好,我们还可以 select * from table where id &gt; 1000000limit 10 数据库三大范式 第一范式：每个列都不可拆分。 第二范式：非主键列完全依赖于主键，而不能是依赖于主键的一部分。 第三范式：非主键只依赖于主键，不依赖于其他非主键。 在设计数据库结构式，要尽量遵守三范式，如果不遵守，必须有足够的理由。 MySQL读写分离如何实现MySQL的读写分离一个主库，多个从库。在主库写，然后主库会自动把数据同步到从库。 MySQL主从复制的原理主库将变更写入binlog日志，然后从库连接主库后，从库有一个IO线程，将主库的binlog日志拷贝到自己本地，写入一个relay中继日志中。接着从库中有一个SQL线程会从中继日志中读取binlog，然后执行binlog日志中的内容，也就是在自己本地执行一遍SQL， 这样就可以保证主从一致。 从库执行SQL是串行的，高并发下会有延迟问题MySQL主从同步延时问题 分库，将一个主库拆分为多个从库。 打开MySQL支持的并行复制。 重写代码，不要在写入之后马上查询。 如果必须要写入之后马上查询，对这个查询设置直连数据库。 MySQL优化【优化总结口诀】 全值匹配我最爱，最左前缀要遵守； 带头大哥不能死，中间兄弟不能断； 索引列上少计算，范围之后全失效； Like百分写最右，覆盖索引不写星； 不等空值还有or，索引失效要少用； VARCHAR引号不可丢，SQL高级也不难！ EXPLAIN SQL语句中IN包含的值不应过多 SELECT语句务必指明字段名称 当只需要一条数据的时候，使用limit 1 如果排序字段没有用到索引，就尽量少排序 如果限制条件中其他字段没有索引，尽量少用or 尽量用union all代替union 区分in和exists、not in和not exists(小表驱动大表) B为小表：select * from 表A where id in (select id from 表B) A为小表：select * from 表A where exists(select * from 表B where 表B.id&#x3D;表A.id) 使用合理的分页方式以提高分页效率 select id,name from product limit 866613, 20; 优化方法：可以将上一页排序字段最大值作为下一页的起点。 select id,name from product where id&gt; 866612 limit 20; 分段查询 分段查询，最终合并输出。 避免在where子句中进行null值判断 不建议使用%前缀模糊查询 例如 LIKE &#39;%name%&#39; 解决办法： 1. 使用全文索引: 创建全文索引SQL：ALTER TABLE \\`dynamic_201606\\` ADD FULLTEXT INDEX \\`idx_user_name\\` (\\`user_name\\`); 使用全文索引SQL：select id,fnum,fdst from dynamic_201606 where match(user_name) against(\\&#39;zhangsan\\&#39; in boolean mode); 2. 使用覆盖索引：查询的字段建立索引 避免在where子句中对字段进行表达式操作 避免进行隐式转换（字符串不加单引号导致索引失效） SELECT * FROM table WHERE type&#x3D;1;（type为varchar） 对于联合索引，要遵守最左前缀原则 举例来说联合索引含有字段id,name,school，可以直接用id字段，也可以id,name这样的顺序，name,school无法使用这个索引,id,school只能使用id索引。（必须是连续的，中间不能断，WHERE子句中MYSQL优化器会优化顺序）。 注意范围查询语句 对于联合索引，如果存在范围查询，比如between、&gt;、&lt;等条件，会造成后面索引字段失效。 关于join优化 1）MySQL没有full join,可以使用union来实现。 2）尽量使用inner join，因为inner join会自动选择小表作为驱动表。 3）合理运用索引 4）利用小表去驱动大表。 5）巧用STRAIGHT_JOIN：只能在inner join下使用,STRAIGHT_JOIN左边表为驱动表。 ORDER BY优化：（MYSQL支持两种排序方式，filesort和Index） ORDER BY子句，尽量使用INDEX（索引）方式排序，避免使用filesort方式排序 尽可能在索引列上完成排序操作，遵循索引的最左前缀原则 ORDER BY子句的列满足索引最左前缀原则 WHERE子句列和ORDER BY子句列满足索引最左前缀原则 无法使用INDEX，FileSort有两种算法： 双路排序 单路排序 GROUP BY优化：（和order by几乎相同） group by的实质是先排序后分组，遵循索引最左前缀原则 当无法使用索引列，增大max_length_for_sort_data参数的设置+增大sort_buffer_size参数设置 where优先级高于having，能在where限定的条件就不要用having了 MySQL日志MySQL逻辑架构 MySQL逻辑架构MySQL的逻辑架构大致可分为三层： 第一层：处理客户端连接，授权认证，安全校验。 第二层：服务端server层，负责对SQL解释、分析、优化、执行操作引擎等。 第三层：存储引擎，负责MySQL中数据的存储和提取。 MySQL日志redo log（重做日志）redo log属于MySQL存储引擎innoDB的事务日志。 MySQL的数据是存放在磁盘中的，每次读写数据都需要做磁盘IO操作，如果并发场景下性能就会很差。为此MySQL提供一个优化手段，引入缓存buffer pool。这个缓存中包含了磁盘中部分数据页（page）的映射，以此来缓解数据库的磁盘压力。 当从数据库读数据时，首先从缓存中读取，如果缓存中没有，则从磁盘读取后放入缓存；当向数据库写数据时，先向缓存写入，此时缓存中的数据页变更，这个数据页称为脏页，buff pool中修改完数据后会按照设定的更新策略，定期刷到磁盘中，这个过程称为刷脏页。 MySQL宕机如果刷脏页还未完成，可MySQL由于某些原因宕机重启，此时buffer pool中修改的数据还没有来得及刷到磁盘中，就会导致数据丢失，无法保证事务的持久性。 为了解决这个问题引入了redo log，redo Log如其名侧重于重做！它记录的是数据库中每个页的修改，而不是某一行或某几行修改成怎样，可以用来恢复提交后的物理数据页，且只能恢复到最后一次提交的位置。 redo log用到了WAL（Write-Ahead Logging）技术，这个技术的核心就在于修改记录前，一定要先写日志，并保证日志先落盘，才能算事务提交完成。 有了redo log再修改数据时，InnoDB引擎会把更新记录先写在redo log中，在修改Buffer Pool中的数据，当提交事务时，调用fsync把redo log刷入磁盘。至于缓存中更新的数据文件何时刷入磁盘，则由后台线程异步处理。 注意：此时redo log的事务状态为prepare，还未真正提交成功。要等到bin log日志写入磁盘完成才变更为commit，事务才算真正提交完成。 这样一来即使刷脏页之前MySQL意外宕机也没关系，只要在重启时解析redo log中更改记录进行重放，重新刷盘即可。 大小固定 redo log采用固定大小，循环写入的格式，当redo log写满之后，重新从头开始如此循环写，形成一个环状。 如此设计的原因： 因为redo log记录的是数据页上的修改，如果Buffer Pool中数据页已经刷磁盘了，那么这些日志记录就失效了，新日志会将这些失效的记录进行覆盖擦除。 图中的write pos表示redo log当前记录的日志序列号LSN(log sequence number)，写入还未刷盘，循环往后递增；check point表示redo log中修改记录已刷入磁盘后的LSN，循环往后递增，这个LSN之前的数据已经全落盘。 write pos到check point之间的部分是redo log空余的部分（绿色），用来记录新的日志；check point到write pos之间是redo log已经记录的数据页修改数据，此时数据页还未刷回磁盘的部分。当write pos追上check point时，会先推动check point向前移动，空出位置（刷盘）再记录新的日志。 注意：redo log日志满了，在擦除之前，需要确保这些要被擦除记录对应在内存中的数据页都已经刷到磁盘中了。擦除旧记录腾出新空间这段期间，是不能再接收新的更新请求的，此刻MySQL的性能会下降。所以在并发量大的情况下，合理调整redo log的文件大小非常重要。 crash-safe因为redo log的存在使得Innodb引擎具有了crash-safe的能力，即MySQL宕机重启，系统会自动去检查redo log，将修改还未写入磁盘的数据从redo log恢复到MySQL中。 MySQL启动时，不管上次是正常关闭还是异常关闭，总是会进行恢复操作。会先检查数据页中的LSN，如果这个LSN 小于 redo log 中的LSN，即write pos位置，说明在redo log上记录着数据页上尚未完成的操作，接着就会从最近的一个check point出发，开始同步数据。 简单理解，比如：redo log的LSN是500，数据页的LSN是300，表明重启前有部分数据未完全刷入到磁盘中，那么系统则将redo log中LSN序号300到500的记录进行重放刷盘。 undo log（回滚日志） undo log undo log也是属于MySQL存储引擎InnoDB的事务日志。 undo log属于逻辑日志，如其名主要起到回滚的作用，它是保证事务原子性的关键。记录的是数据修改前的状态，在数据修改的流程中，同时会记录一条与当前操作相反的逻辑日志到undo log中。 同一个事物内的一条记录被多次修改，不会把每次数据修改前的状态都写入undo log；undo log只负责记录事务开始前要修改数据的原始版本，当我们再次对这行数据进行修改，所产生的修改记录会写入到redo log，undo log负责完成回滚，redo log负责完成前滚。 回滚 未提交的事务，即事务未执行commit。但该事务内修改的脏页中，可能有一部分脏块已经刷盘。如果此时数据库实例宕机重启，就需要用回滚来将先前那部分已经刷盘的脏块从磁盘上撤销。 前滚 未完全提交的事务，即事务已经执行commit，但该事务内修改的脏页中只有一部分数据被刷盘，另外一部分还在buffer pool缓存上，如果此时数据库实例宕机重启，就需要用前滚来完成未完全提交的事务。将先前那部分由于宕机在内存上的未来得及刷盘数据，从redo log中恢复出来并刷入磁盘。 数据库实例恢复时，先做前滚，后做回滚。 bin log（归档日志）bin log是一种数据库Server层（和什么引擎无关），以二进制形式存储在磁盘中的逻辑日志。bin log记录了数据库所有DDL和DML操作（不包含 SELECT 和 SHOW等命令，因为这类操作对数据本身并没有修改）。 默认情况下，二进制日志功能是关闭的。可以通过以下命令查看二进制日志是否开启： SHOW VARIABLES LIKE &#39;log_bin&#39;; bin log也被叫做归档日志，因为它不会像redo log那样循环写擦除之前的记录，而是会一直记录日志。一个bin log日志文件默认最大容量1G（也可以通过max_binlog_size参数修改），单个日志超过最大值，则会新创建一个文件继续写。 show binary logs; bin log日志的内容格式其实就是执行SQL命令的反向逻辑，这点和undo log有点类似。一般来说开启bin log都会给日志文件设置过期时间（expire_logs_days参数，默认永久保存），要不然日志的体量会非常庞大。 show variables like &#39;expire_logs_days&#39;; bin log主要应用于MySQL主从模式（master-slave）中，主从节点间的数据同步；以及基于时间点的数据还原。 bin log和redo log的区别 层次不同：redo log 是InnoDB存储引擎实现的，bin log是MySQL的服务器层实现的，但MySQL数据库中的任何存储引擎对于数据库的更改都会产生bin log。 作用不同：redo log 用于碰撞恢复（crash recovery），保证MySQL宕机也不会影响持久性；bin log用于时间点恢复（point-in-time recovery），保证服务器可以基于时间点恢复数据和主从复制。 内容不同：redo log 是物理日志，内容基于磁盘的页Page；bin log的内容是二进制，可以根据binlog_format参数自行设置。 写入方式不同：redo log 采用循环写的方式记录；binlog 通过追加的方式记录，当文件大小大于给定值后，后续的日志会记录到新的文件上。 刷盘时机不同：bin log在事务提交时写入；redo log 在事务开始时即开始写入。 bin log 与 redo log功能并不冲突而是起到相辅相成的作用，需要二者同时记录，才能保证当数据库发生宕机重启时，数据不会丢失。 relay log（中继日志） relay log日志文件具有与bin log日志文件相同的格式，从上边MySQL主从复制的流程可以看出，relay log起到一个中转的作用，slave先从主库master读取二进制日志数据，写入从库本地，后续再异步由SQL线程读取解析relay log为对应的SQL命令执行。 slow query log慢查询日志（slow query log）: 用来记录在 MySQL中执行时间超过指定时间的查询语句，在 SQL优化过程中会经常使用到。通过慢查询日志，我们可以查找出哪些查询语句的执行效率低，耗时严重。 出于性能方面的考虑，一般只有在排查慢SQL、调试参数时才会开启，默认情况下，慢查询日志功能是关闭的。可以通过以下命令查看是否开启慢查询日志： SHOW VARIABLES LIKE &#39;slow_query%&#39;; general query log一般查询日志（general query log）：用来记录用户的所有操作，包括客户端何时连接了服务器、客户端发送的所有SQL以及其他事件，比如MySQL服务启动和关闭等等。MySQL服务器会按照它接收到语句的先后顺序写入日志文件。 由于一般查询日志记录的内容过于详细，开启后 Log文件的体量会非常庞大，所以出于对性能的考虑，默认情况下，该日志功能是关闭的，通常会在排查故障需获得详细日志的时候才会临时开启。 show variables like &#39;general_log&#39;; error log错误日志（error log）: 应该是 MySQL 中最好理解的一种日志，主要记录 MySQL服务器每次启动和停止的时间以及诊断和出错信息。 默认情况下，该日志功能是开启的，通过如下命令查找错误日志文件的存放路径。 SHOW VARIABLES LIKE &#39;log_error&#39;; 注意：错误日志中记录的可并非全是错误信息，像 MySQL 如何启动 InnoDB的表空间文件、如何初始化自己的存储引擎，初始化 buffer pool等等，这些也记录在错误日志文件中。","categories":[{"name":"后端","slug":"后端","permalink":"https://yangh124.github.io/categories/%E5%90%8E%E7%AB%AF/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://yangh124.github.io/tags/MySQL/"}]},{"title":"RabbitMQ","slug":"RabbitMQ","date":"2021-07-30T16:00:00.000Z","updated":"2024-10-05T15:47:33.616Z","comments":true,"path":"2021/07/31/RabbitMQ/","permalink":"https://yangh124.github.io/2021/07/31/RabbitMQ/","excerpt":"简介RabbitMQ是最受欢迎的开源消息中间件之一，在全球范围内被广泛应用。RabbitMQ是轻量级且易于部署的，能支持多种消息协议。RabbitMQ可以部署在分布式系统中，以满足大规模、高可用的要求。","text":"简介RabbitMQ是最受欢迎的开源消息中间件之一，在全球范围内被广泛应用。RabbitMQ是轻量级且易于部署的，能支持多种消息协议。RabbitMQ可以部署在分布式系统中，以满足大规模、高可用的要求。 安装及配置Linux安装：下载docker镜像 docker pull rabbitmq:3.7.15 使用docker命令启动服务 docker run -d -p 5672:5672 -p 15672:15672 --name rabbitmq rabbitmq:3.7.15 进入容器并开启管理功能 docker exec -it rabbitmq /bin/bash rabbitmq-plugins enable rabbitmq_management 开启防火墙便于外网访问 访问及配置 访问RabbitMQ管理页面地址，查看是否安装成功（Linux下使用服务器IP访问即可）：http://localhost:15672 输入账号密码并登录，这里使用默认账号密码登录：guest guest 创建帐号并设置其角色为管理员：yh yh 创建一个新的虚拟host为：&#x2F;yh 点击yh用户，给yh用户配置该虚拟host的权限； 至此，RabbitMQ的配置完成。 RabbitMQ简介RabbitMQ是一个由erlang开发的AMQP(Advanced Message Queue Protocol)的开源实现。 核心概念Message 消息，消息是不具名的，它由消息头和消息体组成。消息体是不透明的，而消息头则由一系列的可选属性组成，这些属性包括routing-key（路由键）、priority（相对于其他消息的优先权）、delivery-mode（指出该消息可能需要持久性储存）等。 Publish 消息的生产者，也是一个向交换器发布消息的客户端应用程序。 Exchange 交换器，用来接收生产者发送的消息并将这些消息路由给服务器中的队列。Exchange有4种类型：direct（默认），fanout，topic和headers，不同类型的Exchange转发消息的策略有所区别 Queue 消息队列，用来保存消息直到发送给消费者，它是消息的容器，也是消息的终点。一个消息可投入一个或多个队列。消息一直在队列里面，等待消费者连接到这个队列将其取走。 Binding 绑定，用于消息队列和交换器之间的关联。一个绑定就是基于路由键将交换器和消息队列连接起来的路由规则，所以可以将交换器理解为一个由绑定构成的路由表。Exchange和Queue的绑定可以是多对多的关系。 Connection 网络连接，比如一个TCP连接。 Channel 信道，多路复用连接中的一条独立的双向数据流通道。信道是建立在真实的TCP连接内的虚拟连接，AMQP命令都是通过信道发出去的，不管是发布消息、订阅队列还是接收消息，这些都是通过信道完成的。因为对于操作系统来说建立和销毁TCP都是非常昂贵的开销，所以引入信道的概念，以复用一条TCP连接。 Consumer 消息的消费者，表示一个从消息队列中取得消息的客户端应用程序。 Virtual Host 虚拟主机，表示一批交换器、消息队列和相关对象。虚拟主机是共享相同身份认证和加密环境的独立服务器域。每个vhost本质上就是一个mini版的RabbitMQ服务器，拥有自己的队列、交换器、绑定和权限机制。vhost是AMQP概念的基础，必须在连接时指定，默认的vhost是&#x2F;。 Broker 表示消息队列服务器实体 5种消息模式简单模式 简单模式是最简单的消息模式，它包含一个生产者、一个消费者和一个队列。生产者向队列里发送消息，消费者从队列中获取消息并消费。 工作模式 工作模式是指向多个互相竞争的消费者发送消息的模式，它包含一个生产者、两个消费者和一个队列。两个消费者同时绑定到一个队列上去，当消费者获取消息处理耗时任务时，空闲的消费者从队列中获取并消费消息。 发布&#x2F;订阅模式（fanout） 发布&#x2F;订阅模式是指同时向多个消费者发送消息的模式（类似广播的形式），它包含一个生产者、两个消费者、两个队列和一个交换机。两个消费者同时绑定到不同的队列上去，两个队列绑定到交换机上去，生产者通过发送消息到交换机，所有消费者接收并消费消息。 路由模式（direct） 路由模式是可以根据路由键选择性给多个消费者发送消息的模式，它包含一个生产者、两个消费者、两个队列和一个交换机。两个消费者同时绑定到不同的队列上去，两个队列通过路由键绑定到交换机上去，生产者发送消息到交换机，交换机通过路由键转发到不同队列，队列绑定的消费者接收并消费消息。 通配符模式（topic） 通配符模式是可以根据路由键匹配规则选择性给多个消费者发送消息的模式，它包含一个生产者、两个消费者、两个队列和一个交换机。两个消费者同时绑定到不同的队列上去，两个队列通过路由键匹配规则绑定到交换机上去，生产者发送消息到交换机，交换机通过路由键匹配规则转发到不同队列，队列绑定的消费者接收并消费消息。 header模式（header）header模式与routing不同的地方在于，header模式取消routingkey，使用header中的key&#x2F;value（键值对）匹配队列。 RPC模式（direct） 1、客户端即是生产者就是消费者，向RPC请求队列发送RPC调用消息，同时监听RPC响应队列。 2、服务端监听RPC请求队列的消息，收到消息后执行服务端的方法，得到方法返回的结果。 3、服务端将RPC方法 的结果发送到RPC响应队列。 4、客户端（RPC调用方）监听RPC响应队列，接收到RPC调用结果。 消息确认机制-可靠抵达 保证消息不丢失，可靠抵达，可以使用事务消息，性能下降250倍，为此引入确认机制 publish cornfirmCallback确认模式 publish returnCallback未投递到queue退回模式 consumer ack机制 如何保证消息可靠性-消息丢失1.消息丢失 消息发送出去，由于网络原因未到达服务器 做好容错方法，发送消息可能会网络失败，失败后要去重试机制，可记录到数据库，定期重发 做好日志记录，每个消息状态是否被服务器收到都应该记录 做好定期重发，如果消息发送成功，定期去数据库扫描未成功发送的消息进行重发 消息抵达Broker，Broker要将消息持久化才算成功。此时如果宕机，消息持久化未完成 publisher也必须加入确认回调机制，确认成功的消息，修改数据库消息状态 自动ack的状态下。消费者收到消息，但没来得及处理消息，宕机 开启手动ack，消费成功才移除，消费失败就noack并重新入队 2.消息重复 消息消费成功，但是手动ack时宕机，导致ack失败，消息被发送给其他消费者 消息消费失败，由于重试机制，自动又将消息发出去 保证消费业务接口幂等性，处理过了就不处理了 rabbitmq的每一个消息都有redelivered字段，可以获取是否被重新投递过来的，而不是第一次投递 3.消息积压 消费者宕机 消费者消费能力不足 发送者发送流量过大 上线更多消费者 先将消息记录到数据库，再慢慢处理 RabbitMQ延时队列（实现定时任务） 场景：比如未付款订单，超时一定时间后，系统自动取消订单并释放库存。 常用解决方案：spring Schedule定时任务轮询数据库 缺点： 消耗系统内存，增加数据库压力，存在时间误差 解决：RabbitMQ的消息ttl和死信Exchange结合 消息的TTL（Time To Live） 消息TTL就是消息的存活时间 RabbitMQ可以对队列和消息分别设置TTL 对队列设置就是队列没有消费者连着的保留时间，也可以对每个单独的消息做单独的设置。超过这个时间，我们认为消息死了，称之为死信。 如果队列设置了。消息也设置了，那么会取小的。所以一个消息如果被路由到不同的队列，这个消息的死亡的时间可能不一样（不同队列设置）。这里单讲单个消息的TTL因为它才是实现延迟任务的关键。可以通过设置消息expiration字段或者x-message-ttl属性来设置时间，两者是一样的效果。 Dead Letter Exchanges(DLX) 一个消息在满足如下条件下，会进入死信路由，记住这里是路由不是队列，一个路由可以应对很多队列。（什么是死信） 一个消息被消费者拒收了，并且reject方法的参数是requeue是false。也就是说不会被再次放在队列里，被其他消费者使用。（basicNack&#x2F;basicReject）requeue&#x3D;false 上面的消息ttl到了，消息过期 队列的长度限制满了。排在前面的消息会被丢弃或扔到死信路由 Dead Letter Exchange其实就是一个普通的exchange，和创建其他exchange没两样。只是在某一个设置DeadLetter Exchange的队列中有消息过期了，会自动触发消息的转发，发送到Dead Letter Exchange中去。 我们既可以控制消息在一段时间后变成死信，又可以控制变成死信的消息被路由到某一个指定的exchange，结合二者，就可以实现一个延迟队列。","categories":[{"name":"后端","slug":"后端","permalink":"https://yangh124.github.io/categories/%E5%90%8E%E7%AB%AF/"}],"tags":[{"name":"消息队列","slug":"消息队列","permalink":"https://yangh124.github.io/tags/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/"}]},{"title":"JVM","slug":"JVM","date":"2021-04-25T16:00:00.000Z","updated":"2024-10-05T15:47:33.614Z","comments":true,"path":"2021/04/26/JVM/","permalink":"https://yangh124.github.io/2021/04/26/JVM/","excerpt":"Java内存模型（JMM）主内存（main memory）、本地内存（local memory）（抽象概念） 本地内存涵盖了缓存，写缓冲区，寄存器以及其他的硬件和编译器优化。 Java编译器输入的指令流基本是一种基于栈的指令架构，另外一种指令架构师基于寄存器的指令架构。","text":"Java内存模型（JMM）主内存（main memory）、本地内存（local memory）（抽象概念） 本地内存涵盖了缓存，写缓冲区，寄存器以及其他的硬件和编译器优化。 Java编译器输入的指令流基本是一种基于栈的指令架构，另外一种指令架构师基于寄存器的指令架构。 执行引擎(Execution Engine)类加载子系统（Class Loader） class。文件，网络 加载、验证、准备、解析、初始化。然后是使用和卸载了 通过全限定名来加载生成class对象到内存中，然后进行验证这个class文件，包括文件格式校验、元数据验证，字节码校验等。准备是对这个对象分配内存。解析是将符号引用转化为直接引用（指针引用），初始化就是开始执行构造器的代码 双亲委派模型机制 启动类加载器(Bootstrap ClassLoader)用来加载java核心类库，无法被java程序直接引用。 扩展类加载器(Extensions ClassLoader):它用来加载 Java 的扩展库。Java 虚拟机的实现会提供一个扩展库目录。该类加载器在此目录里面查找并加载 Java 类。 应用类加载器（Application ClassLoader）：它根据 Java 应用的类路径（CLASSPATH）来加载 Java 类。一般来说，Java 应用的类都是由它来完成加载的。 用户自定义类加载器，通过继承 java.lang.ClassLoader类的方式实现。什么情况下需要自定义类加载器： 隔离加载类 修改类加载方式 扩展加载源 防止源码泄漏 运行时数据区(Runtime Data Area) 内存 什么是运行时数据区 什么是虚拟机栈 栈：数据结构，先进后出，First In Last Out,用来运行java方法的。线程私有。启用一个线程就有一个虚拟机栈。 调用一个方法就压入一帧。栈内存参数-Xss 1m(默认1M) 局部变量表（八大基本数据类型+对象引用） 定义为一个数字数组，主要用于储存方法参数和定义在方法体内的局部变量。 不存在数据线程安全问题。 局部变量表所需容量大小是在编译期确定下来的。 局部变量表中的变量也是重要的垃圾回收根节点（GCRoots可达性分析算法），只要被局部变量表中直接或间接引用的对象都不会被回收。关于Slot的理解：&emsp;1.最基本的储存单元Slot(变量槽)。&emsp;2.32位以内的类型只占一个Slot(包括returnAddress类型)，64位的类型(long和double)占用两个Slot。&emsp;3.构造方法和实例方法（非static方法）的局部变量表0位Slot会存放this。 操作栈（对局部变量数据进行运算操作）&emsp;操作栈：在方法的执行过程中，根据字节码指令，往栈中写入数据或提取数据，即入栈（push）&#x2F;出栈（pop）；主要用于保存计算过程的中间结果，同时作为计算过程中变量临时的存储空间；操作栈是JVM执行引擎的一个工作区； 动态链接（将常量池中的符号引用在运行期转化为直接引用&#x2F;指向运行时常量池的方法引用）&emsp;动态链接的作用就是为了将这些符号引用转换为调用方法的直接引用 返回地址（方法执行完返回地址）存放调用该方法的PC寄存器的值。异常退出：异常表 一些附加信息 本地方法栈（native方法[C,C++实现]） 主要为Native方法服务 程序计数器 内存空间小，字节码解释器工作时通过改变这个计数值可以选取下一条需要执行的字节码指令，分支、循环、跳转、异常处理和线程恢复等功能都需要依赖这个计数器完成。该内存区域是唯一一个java虚拟机规范没有规定任何OOM情况的区域。 线程切换 记录线程执行的当前的地址和行号 堆（-Xms 30m -Xmx 30m -XX:+PrintGCDetails） 设置堆空间大小的参数123456789101112131415161718 -Xms 用来设置堆空间（年轻代+老年代）的初始内存大小 -X 是jvm的运行参数 ms 是memory start -Xmx 用来设置堆空间（年轻代+老年代）的最大内存大小 2. 默认堆空间的大小 初始内存大小：物理电脑内存大小 / 64 最大内存大小：物理电脑内存大小 / 4 3. 手动设置：-Xms600m -Xmx600m 开发中建议将初始堆内存和最大的堆内存设置成相同的值。 4. 查看设置的参数：方式一： jps / jstat -gc 进程id 方式二：-XX:+PrintGCDetails-XX:+开启 -关闭-XX:NewRatio ： 设置新生代与老年代的比例。默认值是2.-XX:SurvivorRatio ：设置新生代中Eden区与Survivor区的比例。默认值是8-XX:-UseAdaptiveSizePolicy ：关闭自适应的内存分配策略 （暂时用不到）-Xmn:设置新生代的空间的大小。 （一般不设置） 初始化的对象，成员变量（那种非static变量），所有的对象实例和数组都要在堆中分配 堆里面的分区：Eden，Survivor(from+to)，老年代。各自的特点：堆里面分为新生代和老年代（java8取消永久代，采用Matespace(元空间)），新生代包含Eden和Survivor区，Survivor分为from+to区。 内存回收时，如果用的是复制算法，从from复制到to，当经过一次或者多次GC之后，存活下来的对象会被移入老年区。 当JVM内存不够用的时候，会触发FullGC，清理JVM老年代。 当新生区满了之后会触发YGC，先把存活的对象放到其中一个Survivor区，然后进行垃圾清理。因为如果仅仅清理需要删除的对象，这样会导致内存碎片，因此一般会把Eden进行完全的清理，然后整理内存。那么下次GC 的时候，就会使用下一个Survive，这样循环使用。 如果有特别大的对象，新生代放不下，就会使用老年代的担保，直接放到老年代里面。因为JVM 认为，一般大对象的存活时间一般比较久远。对象的分配原则：1.对象优先分配在Eden区2.长期存活的对象进入老年代。空间不足，进行GC。对象年龄+1，年龄&#x3D;15进入老年代&emsp;&emsp;动态对象年龄判断：如果survivor区中相同年龄的所有对象大小的总和大于survivor空间的一半，年龄大于或等于该年龄的对象直接进入老年代，无需等到MaxTenuringThreshole（15）中要求的年龄。3.大对象直接进入老年代&#x3D;&#x3D;TLAB&#x3D;&#x3D;(Thread Local Allocation Buffer) 内存不足、内存溢出、内存泄漏 Minor GC、Major GC、Full GC 部分收集：不是完整收集整个java堆的垃圾收集。 新生代收集（MinorGC&#x2F;YoungGC）:只是新生代的垃圾收集 老年代收集（MajorGC&#x2F;OldGC）：只是老年代的垃圾收集 &emsp;&emsp;目前，只有CMS GC会有单独收集老年代的行为。 &emsp;&emsp;&#x3D;&#x3D;注意，很多时候MajorGC会和FullGC混淆使用，需要具体分辨是老年代回收还是整堆回收。&#x3D;&#x3D; 混合收集（MixedGC）：收集整个新生代以及部分老年代的垃圾收集。 &emsp;&emsp;目前，只有G1 GC会有这种行为。 整堆收集（Full GC）：收集整个Java堆和方法区的垃圾收集。 堆空间常用参数总结测试堆空间常用的jvm参数：1234567891011121314* -XX:+PrintFlagsInitial : 查看所有的参数的默认初始值* -XX:+PrintFlagsFinal ：查看所有的参数的最终值（可能会存在修改，不再是初始值）* 具体查看某个参数的指令： jps：查看当前运行中的进程* jinfo -flag SurvivorRatio 进程id** -Xms：初始堆空间内存 （默认为物理内存的1/64）* -Xmx：最大堆空间内存（默认为物理内存的1/4）* -Xmn：设置新生代的大小。(初始值及最大值)* -XX:NewRatio：配置新生代与老年代在堆结构的占比* -XX:SurvivorRatio：设置新生代中Eden和S0/S1空间的比例* -XX:MaxTenuringThreshold：设置新生代垃圾的最大年龄* -XX:+PrintGCDetails：输出详细的GC处理日志* 打印gc简要信息：① -XX:+PrintGC ② -verbose:gc* -XX:HandlePromotionFailure：是否设置空间分配担保 堆是分配对象存储的唯一选择吗（逃逸分析技术-XX:+DoEscapeAnalysis默认开启）没有发生逃逸的对象，则可以分配到栈上，随着方法执行结束，栈空间就被移除。 逃逸分析：代码优化 栈上分配。 同步省略(锁消除)。 分离对象或标量替换:在JIT阶段，如果经过逃逸分析，发现一个对象不会被外界访问的话，那么经过JIT优化，就会把这个对象拆分成若干个其中包含的若干个成员变量来代替。这个过程就是标量替换。（-XX:+EliminateAllocations默认开启） 方法区(1.8元空间) 1.7及以前称为永久代，在该区内很少发生垃圾回收，但是并不代表不发生GC，在这里进行的GC主要对方法区里的常量池和对类型的卸载。 方法区主要用来储存已被虚拟机加载的类的信息、常量、静态变量和即时编译器编译后的代码等数据。 虚拟机启动过程中，会将各个Class文件中的常量池载入到运行时常量池中。 所以， Class常量池只是一个媒介场所。在JVM真的运行时，需要把常量池中的常量加载到内存中，进入到运行时常量池。 字符串常量池可以理解为运行时常量池分出来的部分。加载时，对于class的静态常量池，如果字符串会被装到字符串常量池中。 该区域被线程共享。 方法区里有一个运行时常量池，用于存放静态编译产生的字面量和符号引用。该常量池具有动态性，也就是说常量不一定是编译时确定的，运行时生成的常量也存放在这个常量池。 方法区的垃圾收集主要回收两部分内容：常量池中废弃的常量和不再使用的类型。永久代为什么要被元空间替代？ 为永久代设置空间大小是很难确定的。 对永久代进行调优是很困难的。 常量池表（Constant Pool Table）是Class文件中的一部分，用于存放编译期生成的字面量和符号引用，这部分内容将在类加载后存放到方法区的运行时常量池中。运行时常量池相对于Class文件常量池的另一个重要特征是：&#x3D;&#x3D;具有动态性。&#x3D;&#x3D; 对象的实例化 直接内存垃圾回收系统 新生代内存不够用时候发生MGC也叫YGC，JVM内存不够的时候发生FGC 引用计数法 可达性分析算法 GCroots对象 虚拟机栈中引用的对象 方法区类静态属性引用的对象 方法区常量池引用的对象 本地方法栈JNI引用的对象当一个对象不可达GC Root时，这个对象并 不会立马被回收，而是出于一个死缓的阶段，若要被真正的回收需要经历两次标记.如果对象在可达性分析中没有与GC Root的引用链，那么此时就会被第一次标记并且进行一次筛选，筛选的条件是是否有必要执行finalize()方法。当对象没有覆盖finalize()方法或者已被虚拟机调用过，那么就认为是没必要的。如果该对象有必要执行finalize()方法，那么这个对象将会放在一个称为F-Queue的对队列中，虚拟机会触发一个Finalize()线程去执行，此线程是低优先级的，并且虚拟机不会承诺一直等待它运行完，这是因为如果finalize()执行缓慢或者发生了死锁，那么就会造成F-Queue队列一直等待，造成了内存回收系统的崩溃。GC对处于F-Queue中的对象进行第二次被标记，这时，该对象将被移除”即将回收”集合，等待回收。 SafePoint 必须要等到Java线程都进入到safepoint的时候VMThread才能开始执行GC safepoint指的特定位置主要有: 循环的末尾（防止大循环的时候一直不进入safepoint，而其他线程在等它进入safepoint） 方法返回前 调用方法的call之后 抛出异常的位置 GC算法 标记清除：先标记，标记完毕再清除，效率不高，会产生碎片 复制算法：分为8:1的Eden区和survivor区，YGC 标记整理：标记完毕之后，让所有存活的对象向一端移动，清理另一端 分代收集：现在的虚拟机垃圾收集大多采用这种方式，它根据对象的生存周期，将堆分为新生代和老年代。在新生代中，由于对象生存期短，每次回收都会有大量对象死去，那么这时就采用复制算法。老年代里的对象存活率较高，没有额外的空间进行分配担保，所以可以使用标记-整理 或者 标记-清除。 GC收集器 串型收集器：串行收集器使用一个单独的线程进行收集，GC时服务有停顿时间 并行收集器：次要回收中使用多线程执行 CMS:基于“标记-清除”算法实现的，经过多次标记才会被清除 G1:从整体来看是基于“标记—整理”算法实现的收集器，从局部（两个Region之间）上来看是基于“复制”算法实现的 几种常见的内存调试工具：jmap,jstack,jconsole,jhat jstack可以看当前栈的情况，jmap查看内存，jhat 进行dump堆的信息 mat（eclipse的也要了解一下）","categories":[{"name":"后端","slug":"后端","permalink":"https://yangh124.github.io/categories/%E5%90%8E%E7%AB%AF/"}],"tags":[{"name":"JVM","slug":"JVM","permalink":"https://yangh124.github.io/tags/JVM/"}]},{"title":"数据结构和算法","slug":"数据结构和算法","date":"2021-04-23T16:00:00.000Z","updated":"2024-10-05T15:47:33.619Z","comments":true,"path":"2021/04/24/数据结构和算法/","permalink":"https://yangh124.github.io/2021/04/24/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E5%92%8C%E7%AE%97%E6%B3%95/","excerpt":"线性结构和非线性结构数据结构包括：线性结构和非线性结构","text":"线性结构和非线性结构数据结构包括：线性结构和非线性结构 线性结构 线性结构作为最常用的数据结构，其特点是数据元素之间存在一对一的线性关系 线性结构有两种不同的存储结构，即顺序存储结构和链式存储结构。 顺序存储结构的线性表称为顺序表，顺序表中的存储元素是连续的。 链式存储结构的线性表称为链表，链表中的存储结构不一定是连续的，元素结点中存放数据元素以及相邻元素的地址信息。 线性结构常见的有：数组、队列、链表和栈。 非线性结构非线性结构包括：二维数组，多维数组，广义表，树结构，图结构 代码实现: https://github.com/yangh124/DataStructures 线性结构稀疏数组和队列稀疏数组当一个数组大部分元素为0，或者为同一个值的数值时，可以使用稀疏数组来保存该数组。 稀疏数组的处理方法是： 第一行记录数组一共几行几列，有多少个不同的值 把具有不同值的元素的行列及值记录在一个小规模数组中，从而缩小程序的规模 二维数组–&gt;稀疏数组思路 遍历原始数组，得到数组中有效数据的个数sum 根据sum就可以创建稀疏数组 sparseArr &#x3D; int[sum+1] [3] 将原始的二维数组中的有效数据存入到稀疏数组 稀疏数组–&gt;原始的二维数组的思路 先读取稀疏数组的第一行，根据第一行的数据，创建原始二维数组，比如上面的 chessArr2 &#x3D; int [11] [11] 在读取稀疏数组后几行的数据，并赋给原始二维数组即可 队列 队列是一个有序列表，可以用数组或是链表来实现 遵循先入先出的原则，即：先存入队列的数据，要先取出。后存入的队列要后取出 数组模拟队列 队列本身是有序列表，若使用数组的结构来存储队列的数据，则队列数组声明如下图，其中maxSize是该队列的最大容量。 因为队列的输出、输入是分别从前后端来处理的，因此需要两个变量front和rear分别记录队列前后端的下标，front会随着数据输出而改变，而rear则是随着数据输入而改变，如图所示： 问题分析及优化： 目前数组使用一次就不能用，没有达到复用的效果 将这个数组使用算法，改进为一个环形数组（取模 %） 思路如下： front变量的含义做一个调整：front就指向队列的第一个元素，也就是说arr[front]就是队列的第一个元素 front的初始值&#x3D;0 rear变量的含义做一个调整：rear指向队列的最后一个元素的后一个位置。因为希望空出一个空间作为约定 rear的初始值&#x3D;0 当队列满时，条件为(rear+1)%maxSize&#x3D;&#x3D;front [满] 当队列为空时，条件为 rear&#x3D;front 队列的有效数据的个数 (rear+maxSize-front)%maxSize &#x2F;&#x2F;rear&#x3D;1 front&#x3D;0 这就是一个环形队列 链表（Linked List）链表是有序的列表 单向链表 链表是以结点的方式来存储的，是链式存储 每个结点包含data域，next域（指向下一个结点） 如图：发现链表的各个结点不一定是连续存储的 双向链表 环形链表：约瑟夫环(Josephus)，如图 栈（Stack） 栈是一种先入后出的(FILO)的有序列表 栈是限制线性表中元素的插入和删除只能在线性表的同一端进行的一种特殊线性表。允许插入和删除的一端，为变化的一端，称为栈顶，另一端为固定的一端，称为栈底。 根据栈的定义可知，最先放入栈中的元素在栈底，最后放入的元素在栈顶，而删除元素正好相反，最后放入的元素先删除，最先放入的元素最后删除。 出栈（pop）、入栈（push）的概念，如图所示 栈的应用场景 子程序的调用：在跳往子程序前，会先将下个指令的地址存到堆栈中，直到子程序执行完成再将地址取出，已回到原来的程序中。 处理递归调用：和子程序的调用类似，只是除了存储下一个指令的地址外，也将参数、局部变量等数据存入堆栈中。 表达式的转换（中缀表达式转后缀表达式）与求值（实际解决）。 二叉树的遍历。 图形的深度优先搜索算法（depth-first）使用栈完成计算一个表达式的结果：3 + 2 * 6 - 2&#x3D;? 实现思路： 通过一个index值（索引），来遍历我们的表达式 如果我们发现是一个数字，就直接入数栈 如果发现是一个运算符： 如果当前符号栈为空，就直接入符号栈 如果符号栈中已存在符号，就和栈顶的符号进行比较，如果当前的操作符的优先级小于或等于栈顶的运算符，就需要从数栈中pop出俩数，从符号栈中pop出一个运算符，进行运算，将结果push入数栈，然后将当前运算符push入运算符栈；如果当前的操作符的优先级大于栈顶的运算符，就直接将运算符push入运算符栈。 当表达式扫描完后，就顺序的从数栈和运算符栈中pop出数据进行运算并将计算结果push入数栈，最终在数栈中的值就是计算的结果 前缀、中缀、后缀表达式（逆波兰表达式） 前缀表达式 1. 前缀表达式又称波兰式，前缀表达式的运算符位于操作数之前 2. 前缀表达式的计算机求值 (3+4)*5-6 前缀表达式就是 - * + 3 4 5 6 从右至左扫描表达式，遇到数字时，将数字压入栈，遇到运算符时，弹出栈顶的两个数，用运算符对它们进行相应的计算（栈顶元素和次栈顶元素），并将结果入栈；重复上述过程直到表达式最左端，最后运算得出的值即为表达式的结果 例如：(3+4)*5-6 的对应的前缀表达式是 - * + 3 4 5 6，计算的过程步骤如下： 1. 从右至左扫描，将6、5、4、3压入栈 2. 遇到+运算符，因此弹出3和4，计算得7，再将7入栈 3. 接下来是`*`运算，因此弹出7和5，计算的35，再将35入栈 4. 最后为-运算符，计算35-6，得29，由此得到最终结果 中缀表达式 中缀表达式就是常见的运算表达式，如(3+4)`*`5-6 中缀表达式的求值是我们人最熟悉的，但对计算机来说却不好操作，因此，在计算结果时，往往会将中缀表达式转成其它表达式操作（一般转成后缀表达式） 后缀表达式 1. 后缀表达式又称为逆波兰表达式，与前缀表达式相似，只是运算符位于操作数之后 2. 举例说明：(3+4)*5-6对应的后缀表达式就是 3 4 + 5 `*` 6 - 后缀表达式的计算机求值 从左至右扫描表达式，遇到数字时，将数字压入堆栈，遇到运算符时，弹出栈顶的两个数，运算符对它们做相应的计算（次顶元素和栈顶元素），并将结果入栈。重复上述过程直到表达式最右端，最后运算得出的值即为表达式的结果 例如：(3+4)*5-1. 例如：(3+4)*5-6对应的后缀表达式就是3 4 + 5 * 6 -，针对后缀表达式的步骤如下： 1. 从左至右扫描，将3和4压入栈 2. 遇到+运算符，因此弹出4和3（4为栈顶元素，3为次顶元素），计算出3+4的值，得7，再将7入栈 3. 将5入栈 4. 接下来是*运算符，因此弹出5和7，计算出7*5=35，将35入栈 5. 将6入栈 6. 最后是-运算符，计算出35-6的值，即29，由此得出最终结果 中缀表达式转后缀表达式1 + ( ( 2+3 ) * 4) - 5 思路： 初始化两个栈：运算符栈s1和存储中间结果结果的栈s2 从左至右扫描中缀表达式 遇到操作数时，将其压入s2 遇到运算符时，比较其与s1栈顶运算符优先级 如果s1为空，或栈顶运算符为”(“，则直接将此运算符入栈 否则，若优先级比栈顶运算符高，也将运算符压入s1 否则，将s1栈顶的运算符弹出并压入到s2中，再次转到(d-i)与s1中新的栈顶运算符相比较（相当于调换俩运算符位置） 遇到括号时： 如果是左括号”(“，则直接压入s1 如果是右括号”)”，则依次弹出s1栈顶的运算符，并压入s2，直到遇到左括号为止，此时将这一对括号丢弃 重复步骤b-e，直到表达式的最右边 将s1中剩余的运算符依次弹出并压入s2 依次弹出s2中的元素并输出，结果的逆序即为中缀表达式的后缀表达式 递归 递归的应用场景看个实际应用场景，迷宫问题（回溯），递归（Recursion） 递归能解决什么问题 各种数学问题如：8皇后问题(在一个8x8的国际象棋上摆8个皇后，使它们不能相互攻击，即：任意两个皇后不能处于同一行、同一列或同一斜线，求多少种摆法（92种）)，汉诺塔，阶乘问题，迷宫问题，球和篮子的问题（Google编程大赛） 各种算法中也会使用到递归，比如快排，归并排序，二分查找，分治算法等 将用栈解决的问题-&gt;递归代码比较简洁 递归的概念 简单的说：递归就是方法自己调用自己，每次调用时传入不同的变量，递归有助于编程者解决复杂的问题，同时可以让代码变得简洁。 递归需要遵守的规则 执行一个方法时，就创建一个新的受保护的独立空间（栈空间） 方法的局部变量是独立的，不会相互影响，比如n变量 如果方法中使用的是引用类型变量（比如数组），就会共享该引用类型的数据 递归必须向退出递归的条件逼近，否则就是无限递归，出现StackOverflowError 当一个方法执行完毕，或者遇到return，就会返回，遵守谁调用，就将结果返回给谁，同时当方法执行完毕或者返回时，该方法也就执行完毕 递归-八皇后问题 八皇后问题算法思路分析 1. 第一个皇后先放第一行第一列。 2. 第二个皇后放在第二行第一列、然后判断是否ok，如果不ok，继续放在第二列、第三列…，依次把所有列都放完，找到一个合适的位置。 3. 继续第三个皇后，还是第一列、第二列、第三列…，直到第8个皇后也能放在一个不冲突的位置。 4. 当放完所有皇后时，得到一个正确解，在栈回退到上一个栈时，就会开始回溯，即将第一个皇后放到第一行第一列的所有正确解全部得到。 5. 然后把第一个皇后放在第一行第二列，重复1、2、3、4步骤 说明： * 理论上应该创建一个二维数组来表示棋盘，但是实际上可以通过算法，用一个一维数组即可解决问题，arr[8] &#x3D; {0,4,7,5,2,6,1,3}。 * 对应arr下标表示第几行和即第几个皇后，arr[i] &#x3D; val，val表示第i+1个皇后，放在第i+1行的第val+1列 排序算法介绍： 排序也称排序算法，排序是将一组数据，按指定的顺序进行排列的过程。 排序的分类： 内部排序：将需要处理的所有数据都加载到内存中进行排序。 外部排序：数据量过大时，无法全部加载到内存中，需要借助于外部存储进行排序。 常见排序算法： 内部排序（使用内存） 插入排序 直接插入排序 希尔排序 选择排序 简单选择排序 堆排序 交换排序 冒泡排序 快速排序 归并排序 基数排序 外部排序（使用内存和外存结合） 算法的时间复杂度时间频度：一个算法花费的时间与算法中语句的执行次数成正比，那个算法中语句执行次数多，它话费时间就多。一个算法中的语句执行次数称为语句频度或者时间频度。记为T(n)。 忽略常数项 忽略低次项 忽略系数 时间复杂度： 一般情况下，算法中的基本操作语句的重复执行次数是问题规模n的某个函数，用T(n)表示，若有某个辅助函数f(n)，使得当n趋近于无穷大时，T(n)&#x2F;f(n)的极限值为不等于零的常数，则称f(n)是T(n)的同数量级函数。记作T(n)&#x3D;O(f(n))，称O(f(n))为算法的渐进时间复杂度，简称时间复杂度 T(n)不同，但是时间复杂度可能相同。如T(n)&#x3D;n²+7n+6与T(n)&#x3D;3n²+2n+2它们的T(n)不同，但是时间复杂度相同，都为O(n²) 计算时间复杂度的方法： 用常数1代替运行时间中的所有加法常数 T(n)&#x3D;3n²+2n+2 &#x3D;&gt; T(n)&#x3D;3n²+2n+1 函数只保留最高阶项 T(n)&#x3D;3n²+2n+1 &#x3D;&gt; T(n)&#x3D;3n² 去除最高阶项的系数 T(n)&#x3D;3n² &#x3D;&gt; T(n)&#x3D;n² &#x3D;&gt; O(n²) 常见的时间复杂度： 常数阶O(1) 对数阶O(log n) 线性阶O(n) 线性对数阶O(n log n) 平方阶O(n²) 立方阶O(n³) k次方阶O(n^k) 指数阶O(2^n) 说明： 常见的算法时间复杂度由小到大依次为：O(1)&lt;O(log n)&lt;O(n)&lt;O(n log n)&lt;O(n²)&lt;O(n³)&lt;O(n^k)&lt;(2^n)，随着问题规模n不断增大，时间复杂度不断增大，算法的执行效率不断降低 冒泡排序冒泡排序（Bubble Sorting）的基本思想是：通过对待排序序列从前向后（从下标较小的元素开始），依次比较相邻元素的值，若发现逆序则交换位置，使值较大的元素逐渐从前移向后部，就想水底的气泡一样逐渐上冒。 因为排序的过程中，各元素不断接近自己的位置，如果一趟比较下来没有进行过交换，就说明序列有序，因此要在排序过程中设置一个标志flag判断元素是否进行过交换。从而减少不必要的比较。 原始数组：[9，3，-1，10，20]第一趟排序 3，9，-1，10，20 &#x2F;&#x2F;如果相邻的元素逆序就交换 3，-1，9，10，20 3，-1，9，10，20 3，-1，9，10，20第二趟排序 -1，3，9，10，20 -1，3，9，10，20 -1，3，9，10，20第三趟排序 -1，3，9，10，20 -1，3，9，10，20第四趟排序 -1，3，9，10，20 小结： 一共进行数组大小减一次的循环 每一趟排序的次数在主键减小 如果发现在某一趟排序中，没有发生一次交换，可以提前结束，排序完成 选择排序选择排序（Select Sorting）的思想：第一次从array[0]~array[n-1]中选取最小值，与array[0]交换，第二次从array[1]~array[n-1]中选取最小值，与array[1]交换，第三次从array[2]~array[n-1]中选取最小值，与array[2]交换，......，第n-1次从array[n-2]~array[n-1]中选取最小值，与array[n-2]交换，总共通过n-1次，最终得到一个有序序列。 原始数组：[101，34，119，1]第一轮排序：1，34，119，101第二轮排序：1，34，119，101第三轮排序：1，34，101，119 说明： 一共进行数组大小减一次的循环 每一轮排序，又是一个循环，循环规则（见代码） 先假定当前数为最小数 然后依次和后面的数进行比较，如果发现有比当前数更小的数，就重新确定最小数，并得到下标 直到遍历结束，就得到本轮的最小数和下标 交换（见代码） 插入排序插入排序属于内部排序法，是对于欲排序的元素以插入的方式找寻该元素的适当位置，以达到排序的目的。 插入排序（Insertion Sorting）的基本思想：把n个待排序的元素看成为一个有序表和一个无序表，开始时有序表只包含一个元素，无序表包含n-1个元素，排序过程中每次从无序表中取出第一个元素，把它的排序码依次与有序表元素的排序码进行比较，将它插入到有序表中的适当位置，使之成为新的有序表。 希尔排序介绍： 希尔排序是希尔（Donald Shell）于1959年提出的一种排序算法。希尔排序也是一种插入排序，它是简单的插入排序经过改进之后的一个更高效的版本，也称为缩小增量排序基本思想： 希尔排序是把记录按下标的一定增量分组，对每组使用直接插入排序算法的排序；随着增量逐渐减少，每组包含的关键词越来越多，当增量减至1时，整个文件恰被分成一组，算法便中止经过上面的”宏观调控“，整个数组已经基本有序。此时，仅仅需要对以上数组简单微调，即可排序完成。 快速排序快速排序（quick sort）是对冒泡排序的一种改进。 基本思想是： 通过一趟排序将要排序的的数据分割成独立的两部分，其中一部分的所有数据都比另外一部分的所有数据都小，然后再按此方法对两部分数据分别进行快速排序，整个排序过程可以递归进行，以此达到整个数据变成有序序列 归并排序介绍： 归并排序(merge-sort)是利用归并的思想实现的排序方法，该算法采用经典的分治(divide-and-conquer)策略(分治法将问题分(divide)成一些小的问题然后递归求解，而治(conquer)的阶段则将分的阶段得到的各答案“修补”在一起，即分而治之)。 基数排序基数排序（桶排序）介绍： 基数排序（radix sort）属于“分配式排序”（distribution sort），又称为“桶子法”（bucket sort）或bin sort，顾名思义，它是通过键值的各个位的值，将要排序的元素分配至某些“桶”中，达到排序的作用。 基数排序法是属于稳定性的排序，基数排序法是效率高的稳定性排序法。 基数排序是桶排序的扩展。 基数排序是1887年赫尔曼.何乐礼发明的。他是这样实现的：将整数按位数切割成不同数字，然后按每个位数分别比较。 基本思想： 将所有待比较数值统一为同样的数位长度，数位较短的数前面补零。然后，从低位开始，依次进行排序。这样从最低位排序一直到最高位排序完成以后，数列就变成一个有序序列。 基数排序的说明 基数排序是对传统排序的扩展，速度很快 基数排序是经典的空间换时间的方式，占用内存很大，当对海量数据进行排序时，容易造成OOM 基数排序是稳定的。（注：假定在待排序的记录序列中，存在多个具有相同关键字的记录，若经过排序，这些记录的相对序列保持不变，则称这种排序算法是稳定的，否则称为不稳定） 常用排序算法总结与对比 查找算法查找算法介绍： 在Java中，我们常用的查找有四种： 1. 顺序（线性）查找 2. 二分查找&#x2F;折半查找 3. 插值查找 4. 斐波那契查找 线性查找可以是无序数列for循环查找….. 二分查找必须是有序数列 {1,8,89,1000,1234}二分查找的思路分析 首先确定该数组的中间下标 mid&#x3D;(left+right)&#x2F;2 然后让需要查找的数findVal和arr[mid]比较 findVal&gt;arr[mid]，说明要查找的数在mid的右边，因此需要递归向右查找 findVal&lt;arr[mid]，说明要查找的数在mid的左边，因此需要递归向左查找 findVal&#x3D;&#x3D;arr[mid]说明找到，返回 什么时候结束退出递归呢？ 找到结束递归 递归完整个数组，仍然没有找到，也需要退出递归，当left&gt;right退出 插值查找介绍： 插值查找算法类似于二分查找，不同的是插值查找每次从自适应mid处开始查找。 将折半查找中的求mid索引的公式进行修改，low表示左边下标left，high表示右边下标right，a为有序数组，key为要查找的元素。 此时mid就称为插值索引。 插值查找算法的举例说明： 注意： 对于数据量较大，关键字分布不均匀的查找表来说，采用插值查找速度较快。 关键字分布不均匀的情况下，该方法不一定比二分查找要好 斐波那契（黄金分割法）查找算法原理： 斐波那契查找原理与前两种相似，仅仅改变了中间结点(mid )的位置，mid不再是中间或者插值得到，而是位于黄金分割点附近，即mid&#x3D;low+F(k-1)-1（F代表斐波那契数列），如下图所示：对F(k-1)-1的理解 有斐波那契数列F[k]&#x3D;F[k-1]+F[K-2]的性质，可以得到 F[k]-1&#x3D;(F[k-1]-1)+(F(k-2)-1)+1 该式说明：只要顺序表的长度为F[k]-1，则可以将该表分成长度为F[k-1]-1和F[k-2]-1的两段，即如上图所示。从而得出mid&#x3D;low+F(k-1)-1 类似的，每一个字段也可以用相同的方式分割。 但顺序表的长度n不一定刚好等于F[k]-1，所以需要将原来的顺序表长度n增加至F[k]-1。这里k值只要使得F[k]-1恰好大于或等于n即可，有以下代码得到，顺序表长度增加后，新增的位置（从n-1到F[K]-1位置），都赋为n位置即可。 12while(n &gt; fib(k)-1) k++; 哈希表介绍： 散列表（Hash table，也叫哈希表），是根据关键码值（key value）而直接进行访问的数据结构。也就是说，它通过把关键码值映射到表中的一个位置来访问记录，以加快查找的速度。这个映射函数叫做散列函数，存放记录的数组叫做散列表。 树结构为什么需要树这种数据结构？ 数组存储方式分析 优点：通过下标方式访问元素，速度快。对于有序数组，还可以使用二分查找提高检索效率。 缺点：如果要检索某个具体值，或者插入新值会整体移动，效率较低 链式存储方式分析 优点：在一定程度上对数组方式有优化（比如：插入一个数值结点，只需要将插入结点，链接到链表中即可，删除效率也很好）。 缺点：在进行检索时，效率仍然较低（需要从头结点开始遍历） 树存储方式分析 能够提高数据存储，读取效率，比如利用二叉排序树（binary sort tree），既可以保证数据的检索速度，同时也可以保证数据的插入，删除，修改的数据。 二叉树 二叉树的概念 树有很多种，每个结点最多只能有两个子结点的一种形式称为二叉树。 二叉树的子结点分为左结点和右结点。 如果该二叉树的所有叶子结点都在最后一层，并且结点总数&#x3D;2^n-1，n为层数，则我们称为满二叉树。 如果该二叉树的所有叶子结点都在最后一层或者倒数第二层，而且最后一层的叶子结点在左边连续，倒数第二层的结点在右边连续，我们称为完全二叉树。 二叉树遍历使用前序，中序和后序进行遍历。 前序遍历：先输出父结点，再遍历左子树和右子树。 中序遍历：先遍历左子树，再输出父结点，再遍历右子树。 后序遍历：先遍历左子树，再遍历右子树，最后输出父结点。 小结：看输出父结点的顺序，就确定是前序，中序还是后序了。 遍历步骤： 创建一颗二叉树 前序遍历 先输出当前结点（root结点） 如果左子结点不为空，则递归继续前序遍历 如果右子结点不为空，则递归继续前序遍历 中序遍历 如果当前结点的左子结点不为空，则递归中序遍历 输出当前结点 如果当前结点右子结点不为空，则递归中序遍历 后序遍历 如果当前结点的左子结点不为空，则递归后序遍历 如果当前结点右子结点不为空，则递归后序遍历 输出当前结点 二叉树删除结点规定： 如果删除的结点是叶子结点，则删除该结点 如果删除的结点是非叶子结点，则删除该子树 思路: 因为二叉树是单向的，所以我们是判断当前结点的子结点是否需要删除，而不是直接判断当前结点是否需要删除。 如果当前结点的左子结点不为空，并且左子结点为要删除的结点，就将this.left&#x3D;null;并且返回（结束递归）。 如果当前结点的右子结点不为空，并且右子结点为要删除的结点，就将this.right&#x3D;null;并且返回（结束递归）。 如果第2步和第3步没有删除结点，那么就需要向左子树进行递归删除。 如果第4步没有删除结点，则应该向右子树进行递归删除。 考虑树是空树，如果只有一个root结点，则等价将二叉树置空 顺序存储二叉树顺序储存二叉树概念 基本说明： 从数据储存来看，数组存储方式和树的存储方式可以相互转换，即数组可以转换成树，树也可以转换成数组，如图所示。要求： 图中的二叉树的结点，要求以数组的方式来存放 arr&#x3D;[1,2,3,4,5,6,7] 要求遍历数组时，仍然可以以前序遍历、中序遍历和后序遍历的方式完成结点遍历 顺序存储二叉树的特点： 1. 顺序二叉树通常只考虑完全二叉树 2. 第n个元素的左子结点为2*n+1 3. 第n个元素的右子结点为2*n+2 4. 第n个元素的父结点为(n-1)/2 5. n:表示二叉树的第几个元素（按0开始编号），如上图所示 线索化二叉树中序遍历结果应该为{8,3,10,1,14,6}线索化二叉树基本介绍： 1. n个结点的二叉链表中含有n+1(公式:2n-(n-1)=n+1)个空指针域。利用二叉链表中的空指针域，存放指向该结点在某种遍历次序下的前驱和后继结点的指针（这种附加指针称为“线索”）。 2. 这种加上了线索的二叉链表称为线索链表，相应的二叉树称为线索二叉树（Threaded BinaryTree）。根据线索性质的不同，线索二叉树可分为前序线索二叉树、中序线索二叉树和后序线索二叉树三种。 3. 一个结点的前一个结点，称为前驱结点。 4. 一个结点的后一个结点，称为后继结点。 说明：当线索化二叉树后，结点的属性left和right，有如下情况： 1. left指向的是左子树，也可能是指向的前驱结点。比如1结点left指向的左子树，而10结点的left指向的是前驱结点。 2. right指向的是右子树，也可能是指向后继结点，比如1结点的right指向的是右子树，而10结点的right指向的是后继结点。 树结构的实际应用堆排序堆排序基本介绍 堆排序是利用堆这种数据结构而设计的一种排序算法，堆排序是一种选择排序，它的最好最坏平均时间复杂度都为O(nlogn)，它是不稳定排序。 堆是具有以下性质的完全二叉树：每个结点的值都大于或等于其左右孩子结点的值，称为大顶堆，注意：没有要求结点的左右孩子结点的值的大小关系。 每个结点的 值都小于或等于其左右孩子结点的值，称为小顶堆。 大顶堆举例说明 小顶堆举例说明 一般升序采用大顶堆，降序采用小顶堆 堆排序基本思想 1. 将待排序数列构造成一个大顶堆。 2. 此时，真个序列的最大值就是堆顶的根结点。 3. 将其与末尾元素进行交换，此时末尾就是最大值。 4. 然后将剩余n-1个元素重新构造成一个堆，这样会得到n个元素的次小值。如此反复执行，便能得到一个有序序列了。 步骤： 步骤一：构造初始堆。将给定的无需序列构造成一个大定堆（一般升序用大顶堆。降序用小顶堆）。 原始数组：[4,6,8,5,9] 假设给定无序序列结构如下 此时我们从最后一个非叶子结点开始（第一个非叶子结点arr.length&#x2F;2-1&#x3D;5&#x2F;2-1&#x3D;1，也就是6），从左至右，从上至下进行调整。 找到下一个非叶子结点4，由于4，9，8中9最大，4和9交换 这时，交换导致了子树4，5，6结构混乱，继续调整，4，5，6中6最大，交换4和6. 此时，我们就将一个无序序列构造成一个大顶堆。 步骤二：将堆顶元素与末尾元素进行交换，使末尾元素最大。然后继续调整堆，再将堆顶元素与末尾元素交换，得到第二大元素。如此反复进行交换、重建、交换。 将堆顶元素9和末尾元素4进行交换 重新调整结构，使其满足堆定义 再将堆顶元素8与末尾元素5进行交换，得到第二大元素8 后续过程，继续进行调整，交换，反复进行，最终使整个序列有序 总结： 将无需序列构建成一个堆，根据升序或者降序选择大顶堆或小顶堆。 将堆顶元素与末尾元素交换，将最大（小）元素“沉”到数组末端。 重新调整结构，使其满足堆定义，然后继续交换堆顶元素与当前末尾元素，反复执行调整交换，直到整个序列有序。 赫夫曼树基本介绍 给定n个权值作为n个叶子结点，构造一棵二叉树，若该树的带权路径长度（WPL）达到最小，称这样的二叉树为最优二叉树，也称为赫夫曼树（Huffman Tree）,还有的书翻译为霍夫曼树。 赫夫曼树是带权路径长度最短的树，权值较大的结点离根较近。结点 赫夫曼树几个重要概念和举例说明 路径和路径长度：在一棵树中，从一个结点往下可以达到的孩子或孙子结点之间的通路，称为路径。通路中分支的数目称为路径长度。若规定根结点的层数为1，则从根结点到第L层结点的路径长度为L-1。 结点的权及带权路径长度：若将树中结点赋予一个有着某种意义的数值，则这个数值称为该结点的权。结点的带权路径长度为：从根结点到该结点之间的路径长度为该结点的权的乘积。 树的带权路径长度：树的带权路径长度规定为所有叶子结点的带权路径长度之和。记为WPL（weighted path length），权值越大的结点离根结点越近的二叉树才是最优二叉树。 WPL最小的就是赫夫曼树。","categories":[{"name":"基础","slug":"基础","permalink":"https://yangh124.github.io/categories/%E5%9F%BA%E7%A1%80/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://yangh124.github.io/tags/Java/"},{"name":"算法","slug":"算法","permalink":"https://yangh124.github.io/tags/%E7%AE%97%E6%B3%95/"}]}],"categories":[{"name":"其他","slug":"其他","permalink":"https://yangh124.github.io/categories/%E5%85%B6%E4%BB%96/"},{"name":"后端","slug":"后端","permalink":"https://yangh124.github.io/categories/%E5%90%8E%E7%AB%AF/"},{"name":"运维","slug":"运维","permalink":"https://yangh124.github.io/categories/%E8%BF%90%E7%BB%B4/"},{"name":"脚本语言","slug":"脚本语言","permalink":"https://yangh124.github.io/categories/%E8%84%9A%E6%9C%AC%E8%AF%AD%E8%A8%80/"},{"name":"项目","slug":"项目","permalink":"https://yangh124.github.io/categories/%E9%A1%B9%E7%9B%AE/"},{"name":"基础","slug":"基础","permalink":"https://yangh124.github.io/categories/%E5%9F%BA%E7%A1%80/"}],"tags":[{"name":"Camunda","slug":"Camunda","permalink":"https://yangh124.github.io/tags/Camunda/"},{"name":"k8s","slug":"k8s","permalink":"https://yangh124.github.io/tags/k8s/"},{"name":"MySQL","slug":"MySQL","permalink":"https://yangh124.github.io/tags/MySQL/"},{"name":"分布式","slug":"分布式","permalink":"https://yangh124.github.io/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"},{"name":"Docker","slug":"Docker","permalink":"https://yangh124.github.io/tags/Docker/"},{"name":"实战","slug":"实战","permalink":"https://yangh124.github.io/tags/%E5%AE%9E%E6%88%98/"},{"name":"elastic search","slug":"elastic-search","permalink":"https://yangh124.github.io/tags/elastic-search/"},{"name":"网络编程","slug":"网络编程","permalink":"https://yangh124.github.io/tags/%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/"},{"name":"微服务","slug":"微服务","permalink":"https://yangh124.github.io/tags/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"},{"name":"消息队列","slug":"消息队列","permalink":"https://yangh124.github.io/tags/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/"},{"name":"JVM","slug":"JVM","permalink":"https://yangh124.github.io/tags/JVM/"},{"name":"Java","slug":"Java","permalink":"https://yangh124.github.io/tags/Java/"},{"name":"算法","slug":"算法","permalink":"https://yangh124.github.io/tags/%E7%AE%97%E6%B3%95/"}]}